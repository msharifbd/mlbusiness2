{
  "hash": "4633c2c059f191f1f601cef2626637b7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"<center> Chapter # 04 <br> Logistic Regression\"\nformat: \n  html: \n    toc: true\n    #toc-title: Table of Contents \n    toc-depth: 5\n    number-sections: true\n    mainfont: emoji\n---\n\n\n\n\n## Introduction \n\n\n## Loading Python Package \n\n::: {#7ae1cfd7 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Scikit-learn version that is used for this code file is 1.6.0\n```\n:::\n:::\n\n\n## Loading Dataset \n\n::: {#4ad9e7b5 .cell execution_count=2}\n``` {.python .cell-code}\n# Importing Training Dataset \ntrain_df = pd.read_csv(\"DATA/train.csv\")\n# Importing Testing Dataset\ntest_df = pd.read_csv(\"DATA/test.csv\")\n```\n:::\n\n\n### Metadata of the Dataset \n\n::: {#2e29a1d2 .cell execution_count=3}\n``` {.python .cell-code}\nprint(\"The total number of rows and columns in the dataset is {} and {} respectively.\".format(train_df.shape[0],train_df.shape[1]))\nprint (\"\\nThe names and the types of the variables of the dataset:\")\ntrain_df.info()\ntrain_df.head()\nprint(\"\\nThe types of the variables in the dataset are:\")\ntrain_df.dtypes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe total number of rows and columns in the dataset is 891 and 12 respectively.\n\nThe names and the types of the variables of the dataset:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\nThe types of the variables in the dataset are:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n```\n:::\n:::\n\n\n::: {#183362aa .cell execution_count=4}\n``` {.python .cell-code}\nfor x in train_df.columns:\n  print  (x)\ntrain_df['Pclass'].value_counts(sort = True, ascending = True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nPclass\n2    184\n1    216\n3    491\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#97a78b7d .cell execution_count=5}\n``` {.python .cell-code}\ntrain_df['Sex'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nSex\nmale      577\nfemale    314\nName: count, dtype: int64\n```\n:::\n:::\n\n\n## Data Quality & Missing Value Assessment\n\n::: {#dc560a10 .cell execution_count=6}\n``` {.python .cell-code}\n# Missing value in Training Dataset \ntrain_df.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n```\n:::\n:::\n\n\n::: {#9544b963 .cell execution_count=7}\n``` {.python .cell-code}\n# Missing value in Testing Dataset \ntest_df.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n```\n:::\n:::\n\n\n### Age - Missing Value\n\n::: {#d88dc192 .cell execution_count=8}\n``` {.python .cell-code}\nprint ('The percent of missing \"Age\" record in the training dataset is %0.2f%%' %(train_df['Age'].isnull().sum()/train_df.shape[0]*100))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe percent of missing \"Age\" record in the training dataset is 19.87%\n```\n:::\n:::\n\n\n::: {#84b7f457 .cell execution_count=9}\n``` {.python .cell-code}\nprint ('The percent of missing \"Age\" record in testing dataset is %0.3f%%' %(test_df['Age'].isnull().sum()/test_df.shape[0]*100))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe percent of missing \"Age\" record in testing dataset is 20.574%\n```\n:::\n:::\n\n\n::: {#dfe5c575 .cell execution_count=10}\n``` {.python .cell-code}\nplt.figure(figsize = (10,8))\ntrain_df['Age'].hist(bins = 15, density = True, color = 'teal', alpha = 0.60)\ntrain_df['Age'].plot(kind = 'density', color = 'teal', alpha = 0.60)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-11-output-1.png){width=833 height=639}\n:::\n:::\n\n\nSince the variable `Age` is a little bit right skewed, using the mean to replace the missing observations might bias our results. Therefore, it is recommended that median be used to replace the missing observations.\n\n::: {#c5c741ce .cell execution_count=11}\n``` {.python .cell-code}\nprint ('The mean of \"Age\" variable is %0.3f.' %(train_df['Age'].mean(skipna = True)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean of \"Age\" variable is 29.699.\n```\n:::\n:::\n\n\n::: {#7d0f247f .cell execution_count=12}\n``` {.python .cell-code}\nprint ('The median of \"Age\" variable is %0.2f.' %(train_df['Age'].median(skipna = True)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe median of \"Age\" variable is 28.00.\n```\n:::\n:::\n\n\n### Cabin - Missing Value\n\n::: {#6fae57ab .cell execution_count=13}\n``` {.python .cell-code}\ntrain_df['Cabin'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nCabin\nG6             4\nC23 C25 C27    4\nB96 B98        4\nF2             3\nD              3\n              ..\nE17            1\nA24            1\nC50            1\nB42            1\nC148           1\nName: count, Length: 147, dtype: int64\n```\n:::\n:::\n\n\n::: {#9a34465d .cell execution_count=14}\n``` {.python .cell-code}\nprint ('The percent of cabin variable missing value is %0.2f%%.' %(train_df['Cabin'].isnull().sum()/train_df.shape[0]*100))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe percent of cabin variable missing value is 77.10%.\n```\n:::\n:::\n\n\n77% observations of the `Cabin` variable is missing. Therefore, it is better to prune the variable from the dataset. Moreover, the drop of the `Cabin` variable is justified because it has correlation with two other variables - `Fare` and `Pclass`.\n\n### Embarked - Missing Value\n\n::: {#bf9522f7 .cell execution_count=15}\n``` {.python .cell-code}\ntrain_df['Embarked'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#b1bafb39 .cell execution_count=16}\n``` {.python .cell-code}\n# Percent of missing 'Embarked' Variable\nprint(\n    \"The percent of missing 'Embarked' records is %.2f%%\" %\n    (train_df['Embarked'].isnull().sum()/train_df.shape[0]*100)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe percent of missing 'Embarked' records is 0.22%\n```\n:::\n:::\n\n\nSince there are only 0.22% missing observation for `Embarked`, we can impute the missing values with the port where most people embarked.\n\n::: {#66e7ad15 .cell execution_count=17}\n``` {.python .cell-code}\nprint('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\nprint(train_df['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train_df, palette='Set2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBoarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-18-output-2.png){width=597 height=435}\n:::\n:::\n\n\n::: {#5ac313da .cell execution_count=18}\n``` {.python .cell-code}\nprint('The most common boarding port of embarkation is %s.' %train_df['Embarked'].value_counts().idxmax())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe most common boarding port of embarkation is S.\n```\n:::\n:::\n\n\n## Final Adjustment to the Datasets (Training & Testing)\n\nBased on the assessment of the missing values in the dataset, We will make the following changes to the data:\n\n-   The missing value of `Age` variable will be imputed with 28 (median value `Age`)\n-   The missing value of `Embarked` variable will be imputed with `S` (the most common boarding point)\n-   There are many missing values for the variable `Cabin`; therefore, the variable will be dropped. Moreover, the drop will not affect the model as the variable is associated with two other variables - `Pclass` and `Fare`.\n\n::: {#b827b7ff .cell execution_count=19}\n``` {.python .cell-code}\ntrain_data = train_df.copy()\ntrain_data['Age'].fillna(train_data['Age'].median(skipna = True), inplace = True)\ntrain_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace = True)\ntrain_data.drop(['Cabin'], axis = 1, inplace = True)\ntrain_data.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nEmbarked       0\ndtype: int64\n```\n:::\n:::\n\n\n::: {#c739b00c .cell execution_count=20}\n``` {.python .cell-code}\ntrain_data.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>886</th>\n      <td>887</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Montvila, Rev. Juozas</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>211536</td>\n      <td>13.00</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>888</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Graham, Miss. Margaret Edith</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>112053</td>\n      <td>30.00</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>889</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n      <td>female</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>W./C. 6607</td>\n      <td>23.45</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>890</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Behr, Mr. Karl Howell</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>111369</td>\n      <td>30.00</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>891</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Dooley, Mr. Patrick</td>\n      <td>male</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>370376</td>\n      <td>7.75</td>\n      <td>Q</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#6477713d .cell execution_count=21}\n``` {.python .cell-code}\nplt.figure(figsize=(15,8))\n# Data with missing observations\nax = train_df['Age'].hist(bins = 15, density = True, stacked = True, color = 'teal', alpha = 0.6)\ntrain_df['Age'].plot(kind = 'density', color = 'teal')\n# Data without missing observations\nax = train_data['Age'].hist(bins = 15, density = True, stacked = True, color = 'orange', alpha = 0.6)\ntrain_data['Age'].plot(kind = 'density', color = 'orange')\nplt.xlim(-10,85)\nax.legend([\"Raw Age\", \"Adjusted Age\"])\nax.set(xlabel = 'Age')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-22-output-1.png){width=1196 height=657}\n:::\n:::\n\n\n### Additional Variables\n\nThe variable `SibSp` means whether the passenger has sibling or spouse aboard and the variable `Parch` means whether the passenger has parents or children aboard. For the sake of simplicity and to account for **multicollinearity**, these two variables will be combined into a categorical variable: whether or not the individual was traveling alone.\n\n::: {#4cfd404f .cell execution_count=22}\n``` {.python .cell-code}\n# Creating categorical variable for Traveling alone\ntrain_data['TravelAlone'] = np.where((train_data['SibSp']+train_data['Parch']) > 0, 0, 1)\ntrain_data.drop('SibSp', axis = 1, inplace = True)\ntrain_data.drop('Parch', axis = 1, inplace = True)\n```\n:::\n\n\nFor variables `Pclass`, `Sex`, and `Embarked`, categorical variables will be created\n\n::: {#a4dd2cb5 .cell execution_count=23}\n``` {.python .cell-code}\ntraining = pd.get_dummies(train_data, columns= [\"Pclass\", \"Embarked\", \"Sex\"])\ntraining.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Name         891 non-null    object \n 3   Age          891 non-null    float64\n 4   Ticket       891 non-null    object \n 5   Fare         891 non-null    float64\n 6   TravelAlone  891 non-null    int64  \n 7   Pclass_1     891 non-null    bool   \n 8   Pclass_2     891 non-null    bool   \n 9   Pclass_3     891 non-null    bool   \n 10  Embarked_C   891 non-null    bool   \n 11  Embarked_Q   891 non-null    bool   \n 12  Embarked_S   891 non-null    bool   \n 13  Sex_female   891 non-null    bool   \n 14  Sex_male     891 non-null    bool   \ndtypes: bool(8), float64(2), int64(3), object(2)\nmemory usage: 55.8+ KB\n```\n:::\n:::\n\n\n::: {#a2ad5be9 .cell execution_count=24}\n``` {.python .cell-code}\ntraining[['Pclass_1','Pclass_2','Pclass_3', 'Embarked_C','Embarked_Q','Embarked_S', 'Sex_female', 'Sex_male']].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Sex_female</th>\n      <th>Sex_male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#3f85c257 .cell execution_count=25}\n``` {.python .cell-code}\ntraining.drop(['Sex_female', 'PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\ntraining.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>TravelAlone</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Sex_male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>886</th>\n      <td>0</td>\n      <td>27.0</td>\n      <td>13.00</td>\n      <td>1</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>1</td>\n      <td>19.0</td>\n      <td>30.00</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>0</td>\n      <td>28.0</td>\n      <td>23.45</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>1</td>\n      <td>26.0</td>\n      <td>30.00</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>0</td>\n      <td>32.0</td>\n      <td>7.75</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#8fe7326b .cell execution_count=26}\n``` {.python .cell-code}\ntraining.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Survived     891 non-null    int64  \n 1   Age          891 non-null    float64\n 2   Fare         891 non-null    float64\n 3   TravelAlone  891 non-null    int64  \n 4   Pclass_1     891 non-null    bool   \n 5   Pclass_2     891 non-null    bool   \n 6   Pclass_3     891 non-null    bool   \n 7   Embarked_C   891 non-null    bool   \n 8   Embarked_Q   891 non-null    bool   \n 9   Embarked_S   891 non-null    bool   \n 10  Sex_male     891 non-null    bool   \ndtypes: bool(7), float64(2), int64(2)\nmemory usage: 34.1 KB\n```\n:::\n:::\n\n\n::: {#911ca577 .cell execution_count=27}\n``` {.python .cell-code}\nfinal_train = training\nfinal_train.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>TravelAlone</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Sex_male</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>886</th>\n      <td>0</td>\n      <td>27.0</td>\n      <td>13.00</td>\n      <td>1</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>1</td>\n      <td>19.0</td>\n      <td>30.00</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>0</td>\n      <td>28.0</td>\n      <td>23.45</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>1</td>\n      <td>26.0</td>\n      <td>30.00</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>0</td>\n      <td>32.0</td>\n      <td>7.75</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#00971736 .cell execution_count=28}\n``` {.python .cell-code}\ntest_df.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n```\n:::\n:::\n\n\nNow we apply the same changes in the testing dataset.\n\n-   We will apply to same imputation for `Age` in the Test data as we did for my Training data (if missing, `Age` = 28).\n-   We will also remove the `Cabin` variable from the test data, as we've decided not to include it in my analysis.\n-   There were no missing values in the `Embarked` port variable.\n-   We will add the dummy variables to finalize the test set.\n-   Finally, We will impute the 1 missing value for `Fare` with the median, 14.45.\n\n::: {#ceeb5a3f .cell execution_count=29}\n``` {.python .cell-code}\nprint('The median value of \"Fare\" variable in testing dataset is %0.3f.' %(train_df['Fare'].median(skipna = True)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe median value of \"Fare\" variable in testing dataset is 14.454.\n```\n:::\n:::\n\n\n::: {#f36f23b1 .cell execution_count=30}\n``` {.python .cell-code}\ntest_data = test_df.copy()\ntest_data['Age'].fillna(test_data['Age'].median(skipna = True), inplace = True)\ntest_data['Fare'].fillna(test_data['Fare'].median(skipna = True), inplace = True)\ntest_data.drop(['Cabin'], axis  = 1, inplace = True)\n```\n:::\n\n\n### Creating New Variables\n\n::: {#baa4a7bd .cell execution_count=31}\n``` {.python .cell-code}\n# Creating new variable - TravelAlone\ntest_data['TravelAlone']= np.where(test_data['SibSp']+test_data['Parch']>0,0,1)\ntest_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\ntest_data.sample(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>TravelAlone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>111</th>\n      <td>1003</td>\n      <td>3</td>\n      <td>Shine, Miss. Ellen Natalia</td>\n      <td>female</td>\n      <td>27.0</td>\n      <td>330968</td>\n      <td>7.7792</td>\n      <td>Q</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>257</th>\n      <td>1149</td>\n      <td>3</td>\n      <td>Niklasson, Mr. Samuel</td>\n      <td>male</td>\n      <td>28.0</td>\n      <td>363611</td>\n      <td>8.0500</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>995</td>\n      <td>3</td>\n      <td>Johansson Palmquist, Mr. Oskar Leander</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>347070</td>\n      <td>7.7750</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>916</td>\n      <td>1</td>\n      <td>Ryerson, Mrs. Arthur Larned (Emily Maria Borie)</td>\n      <td>female</td>\n      <td>48.0</td>\n      <td>PC 17608</td>\n      <td>262.3750</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>930</td>\n      <td>3</td>\n      <td>Sap, Mr. Julius</td>\n      <td>male</td>\n      <td>25.0</td>\n      <td>345768</td>\n      <td>9.5000</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n#### Creating the Dummies for Categorical Variables\n\n::: {#cd9c2355 .cell execution_count=32}\n``` {.python .cell-code}\ntesting = pd.get_dummies(test_data, columns = ['Pclass', 'Sex', 'Embarked'])\ntesting.drop(['Sex_female','PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)\ntesting.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>TravelAlone</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Sex_male</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>413</th>\n      <td>27.0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>39.0</td>\n      <td>108.9000</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>415</th>\n      <td>38.5</td>\n      <td>7.2500</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>416</th>\n      <td>27.0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>417</th>\n      <td>27.0</td>\n      <td>22.3583</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#46d7faf8 .cell execution_count=33}\n``` {.python .cell-code}\nfinal_test = testing\nfinal_test.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>TravelAlone</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Sex_male</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34.5</td>\n      <td>7.8292</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>47.0</td>\n      <td>7.0000</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>62.0</td>\n      <td>9.6875</td>\n      <td>1</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27.0</td>\n      <td>8.6625</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22.0</td>\n      <td>12.2875</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Exploratory Data Analysis (EDA)\n\n### Exploration of `Age` Variable\n\n::: {#03bc8832 .cell execution_count=34}\n``` {.python .cell-code}\nfinal_train.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Survived     891 non-null    int64  \n 1   Age          891 non-null    float64\n 2   Fare         891 non-null    float64\n 3   TravelAlone  891 non-null    int64  \n 4   Pclass_1     891 non-null    bool   \n 5   Pclass_2     891 non-null    bool   \n 6   Pclass_3     891 non-null    bool   \n 7   Embarked_C   891 non-null    bool   \n 8   Embarked_Q   891 non-null    bool   \n 9   Embarked_S   891 non-null    bool   \n 10  Sex_male     891 non-null    bool   \ndtypes: bool(7), float64(2), int64(2)\nmemory usage: 34.1 KB\n```\n:::\n:::\n\n\n::: {#f369a0af .cell execution_count=35}\n``` {.python .cell-code}\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Age'][final_train.Survived == 1], shade=True, color = 'darkturquoise')\nsns.kdeplot(final_train['Age'][final_train.Survived == 0], shade=True, color = 'lightcoral')\nax.legend(['Survived', 'Died']) # or you can use plt.legend(['Survived', 'Died])\nplt.title('Density Plot for Surviving Population and Deceased Population')\nplt.xlabel('Age') # or you can use ax.set(xlabel = 'Age')\nplt.xlim(-10,85)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-36-output-1.png){width=1196 height=677}\n:::\n:::\n\n\nThe age distribution for survivors and deceased is actually very similar. One notable difference is that, of the survivors, a larger proportion were children. The passengers evidently made an attempt to save children by giving them a place on the life rafts.\n\n::: {#2bffd378 .cell execution_count=36}\n``` {.python .cell-code}\navg_age = final_train.groupby(['Survived']) ['Age'].mean()\navg_age.to_frame().reset_index()\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>30.028233</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>28.291433</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#3ef62e9d .cell execution_count=37}\n``` {.python .cell-code}\nsns.boxplot(data = final_train, x  = 'Survived', y = 'Age', palette='Set2')\nplt.title(\"Comparison of Age of Passengers Conditioned on Survived\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-38-output-1.png){width=589 height=455}\n:::\n:::\n\n\n::: {#42a937d1 .cell execution_count=38}\n``` {.python .cell-code}\n# Creating a Dummy Variable IsMinor\nfinal_train['IsMinor'] = np.where(final_train['Age'] <= 16, 1, 0)\nfinal_test['IsMinor'] = np.where(final_test['Age'] <= 16, 1, 0)\n```\n:::\n\n\n### Exploration of `Fare` Variable\n\n::: {#5cbb6056 .cell execution_count=39}\n``` {.python .cell-code}\ntrain_df.groupby(['Survived']) ['Fare'].mean().to_frame().reset_index()\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>22.117887</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>48.395408</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#dddb5ec9 .cell execution_count=40}\n``` {.python .cell-code}\nsns.boxplot(data = final_train, x = 'Survived', y = 'Fare', palette='Set2')\nplt.ylim(0, 100)\nplt.title('Comparison of Fare of Passengers Conditioned on Survived')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-41-output-1.png){width=597 height=455}\n:::\n:::\n\n\n::: {#07f494ea .cell execution_count=41}\n``` {.python .cell-code}\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Fare'][final_train.Survived == 1],shade=True, color='darkturquoise')\nsns.kdeplot(final_train['Fare'][final_train.Survived==0], shade=True, color='lightcoral')\nax.legend(['Survived', 'Died'])\nax.set(xlabel= 'Fare')\nplt.xlim(-20,200)\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-42-output-1.png){width=1217 height=677}\n:::\n:::\n\n\nAs the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class, which we'll look at next.\n\n::: {#1974c5c9 .cell execution_count=42}\n``` {.python .cell-code}\n# Pair Plot of two continuous variables (Age and Fare)\nplt.figure(figsize=(15,8))\nsns.pairplot(data=train_data, hue='Survived', vars= ['Age', 'Fare'])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 1440x768 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-43-output-2.png){width=552 height=473}\n:::\n:::\n\n\n### Exploration of `PClass` Variable\n\n::: {#9f9141f0 .cell execution_count=43}\n``` {.python .cell-code}\nsns.barplot(data = train_df, x = 'Pclass', y = 'Survived', color='darkturquoise')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-44-output-1.png){width=593 height=435}\n:::\n:::\n\n\nAs expected, first class passengers were more likely to survive.\n\n### Exploration of `Embarked` Variable\n\n::: {#b9330203 .cell execution_count=44}\n``` {.python .cell-code}\nsns.barplot(x = 'Embarked', y = 'Survived', data=train_df, color=\"teal\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-45-output-1.png){width=593 height=435}\n:::\n:::\n\n\nPassengers who boarded in Cherbourg, France, appear to have the highest survival rate. Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown. This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck). It's also worth noting the size of the whiskers in these plots. Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest. The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker. It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts.\n\n### Exploration of `TravelAlone` Variable\n\n::: {#e0f59a55 .cell execution_count=45}\n``` {.python .cell-code}\nsns.barplot(x = 'TravelAlone', y = 'Survived', data=final_train, color=\"mediumturquoise\")\nplt.xlabel('Travel Alone')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-46-output-1.png){width=593 height=435}\n:::\n:::\n\n\nIndividuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male.\n\n### Exploration of `Gender` Variable\n\n::: {#f184d172 .cell execution_count=46}\n``` {.python .cell-code}\nsns.barplot(x = 'Sex', y = 'Survived', data=train_df, color=\"aquamarine\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-47-output-1.png){width=593 height=435}\n:::\n:::\n\n\n### Chi-square Test of Independence\n\n#### Chi-square Test of Independence betweeen `Survived` and `Sex`\n\n::: {#11f9fb60 .cell execution_count=47}\n``` {.python .cell-code}\npd.crosstab(train_df['Survived'], train_df['Sex'])\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Sex</th>\n      <th>female</th>\n      <th>male</th>\n    </tr>\n    <tr>\n      <th>Survived</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>81</td>\n      <td>468</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>233</td>\n      <td>109</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#8cdbf712 .cell execution_count=48}\n``` {.python .cell-code}\n# Importing scipy package \nfrom scipy import stats\n```\n:::\n\n\n::: {#a70925f9 .cell execution_count=49}\n``` {.python .cell-code}\nstats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\nChi2ContingencyResult(statistic=np.float64(260.71702016732104), pvalue=np.float64(1.1973570627755645e-58), dof=1, expected_freq=array([[193.47474747, 355.52525253],\n       [120.52525253, 221.47474747]]))\n```\n:::\n:::\n\n\n::: {#836359f7 .cell execution_count=50}\n``` {.python .cell-code}\nchi2_stat, p, dof, expected = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\nprint(f\"chi2 statistic:     {chi2_stat:.5g}\")\nprint(f\"p-value:            {p:.5g}\")\nprint(f\"degrees of freedom: {dof}\")\nprint(\"expected frequencies:\\n\",expected)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nchi2 statistic:     260.72\np-value:            1.1974e-58\ndegrees of freedom: 1\nexpected frequencies:\n [[193.47474747 355.52525253]\n [120.52525253 221.47474747]]\n```\n:::\n:::\n\n\n#### Chi-square Test of Independence betweeen `Survived` and `Pclass`\n\n::: {#c3ae6c34 .cell execution_count=51}\n``` {.python .cell-code}\nchi2_stat_2, p_2, dof_2, expected_2 = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Pclass']))\nprint(f\"chi2 statistic:     {chi2_stat_2:.5g}\")\nprint(f\"p-value:            {p_2:.5g}\")\nprint(f\"degrees of freedom: {dof_2}\")\nprint(\"expected frequencies:\\n\",expected_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nchi2 statistic:     102.89\np-value:            4.5493e-23\ndegrees of freedom: 2\nexpected frequencies:\n [[133.09090909 113.37373737 302.53535354]\n [ 82.90909091  70.62626263 188.46464646]]\n```\n:::\n:::\n\n\n### Post Hoc Analysis of `Pclass`\n\nThe explanation of  `Post Hoc` analysis is given in this [link](!https://www.youtube.com/watch?v=-S8EJEYNFIc).\n\n::: {#ed9e0ad2 .cell execution_count=52}\n``` {.python .cell-code}\npclass_cross = pd.crosstab(train_df['Survived'], train_df['Pclass'])\npclass_cross\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Pclass</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n    <tr>\n      <th>Survived</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>80</td>\n      <td>97</td>\n      <td>372</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>136</td>\n      <td>87</td>\n      <td>119</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#9f5e6d23 .cell execution_count=53}\n``` {.python .cell-code}\nimport gc\nfrom itertools import combinations\nimport scipy.stats\nimport statsmodels.stats.multicomp as multi\nfrom statsmodels.stats.multitest import multipletests\n```\n:::\n\n\n::: {#2c015b1d .cell execution_count=54}\n``` {.python .cell-code}\np_vals_chi = []\npairs_of_class = list(combinations(train_df['Pclass'].unique(),2))\n\nfor each_pair in pairs_of_class:\n    each_df = train_df[(train_df['Pclass']==each_pair[0]) | (train_df['Pclass']==each_pair[1])]\n    p_vals_chi.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Pclass']))[1]\n         )\n```\n:::\n\n\n::: {#d1fab25a .cell execution_count=55}\n``` {.python .cell-code}\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of class',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of class'] = pairs_of_class\nbonferroni_results['original p value'] = p_vals_chi\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni = multipletests(p_vals_chi, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni[0]\nbonferroni_results.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pair of class</th>\n      <th>original p value</th>\n      <th>corrected p value</th>\n      <th>Reject Null?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(3, 1)</td>\n      <td>1.212338e-22</td>\n      <td>3.637013e-22</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(3, 2)</td>\n      <td>1.224965e-08</td>\n      <td>3.674894e-08</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(1, 2)</td>\n      <td>2.319805e-03</td>\n      <td>6.959414e-03</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Post Hoc Analysis of `Embarked`\n\n::: {#336d78a3 .cell execution_count=56}\n``` {.python .cell-code}\n# Write code Here \np_vals_chi_embark = []\npairs_of_embark = list(combinations(train_data['Embarked'].unique(),2))\n\nfor each_pair in pairs_of_embark:\n    each_df = train_data[(train_data['Embarked']==each_pair[0]) | (train_data['Embarked']==each_pair[1])]\n    p_vals_chi_embark.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Embarked']))[1]\n         )\n```\n:::\n\n\n::: {#02e31f0f .cell execution_count=57}\n``` {.python .cell-code}\n#Write code Here\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of embark',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of embark'] = pairs_of_embark\nbonferroni_results['original p value'] = p_vals_chi_embark\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni_embark = multipletests(p_vals_chi_embark, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni_embark[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni_embark[0]\nbonferroni_results.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pair of embark</th>\n      <th>original p value</th>\n      <th>corrected p value</th>\n      <th>Reject Null?</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(S, C)</td>\n      <td>5.537903e-07</td>\n      <td>0.000002</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(S, Q)</td>\n      <td>4.493936e-01</td>\n      <td>1.000000</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(C, Q)</td>\n      <td>2.475540e-02</td>\n      <td>0.074266</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Logistic Regression & Results\n\n### Feature Selection\n\n#### Recursive Feature Selection (RFE)\n\nGiven an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a `coef_ attribute` or through a `feature_importances_` attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\n::: {#89bc7a0d .cell execution_count=58}\n``` {.python .cell-code}\n# Our all training dataset \ntrain_df\ntrain_data # impute missing values \ntraining # create the dummies \nfinal_train\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>TravelAlone</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Sex_male</th>\n      <th>IsMinor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>22.0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>38.0</td>\n      <td>71.2833</td>\n      <td>0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>26.0</td>\n      <td>7.9250</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>35.0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>35.0</td>\n      <td>8.0500</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>0</td>\n      <td>27.0</td>\n      <td>13.0000</td>\n      <td>1</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>1</td>\n      <td>19.0</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>0</td>\n      <td>28.0</td>\n      <td>23.4500</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>1</td>\n      <td>26.0</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>0</td>\n      <td>32.0</td>\n      <td>7.7500</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 12 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#6b7fda95 .cell execution_count=59}\n``` {.python .cell-code}\ncols = ['Age', 'Fare', 'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX = final_train[cols] # features vector\ny = final_train['Survived'] # Target vector \n```\n:::\n\n\n::: {#513f694f .cell execution_count=60}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\n```\n:::\n\n\n::: {#4eb958d6 .cell execution_count=61}\n``` {.python .cell-code}\n# Build the model \nmodel = LogisticRegression()\n# Create the RFE model \nrfe = RFE(estimator=model, n_features_to_select=8)\nrfe = rfe.fit(X ,y)\ndir(rfe)\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```\n['__abstractmethods__',\n '__annotations__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__firstlineno__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__sklearn_clone__',\n '__sklearn_tags__',\n '__static_attributes__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_abc_impl',\n '_build_request_for_signature',\n '_check_feature_names',\n '_check_n_features',\n '_doc_link_module',\n '_doc_link_template',\n '_doc_link_url_param_generator',\n '_estimator_type',\n '_fit',\n '_get_default_requests',\n '_get_doc_link',\n '_get_metadata_request',\n '_get_param_names',\n '_get_support_mask',\n '_get_tags',\n '_more_tags',\n '_parameter_constraints',\n '_repr_html_',\n '_repr_html_inner',\n '_repr_mimebundle_',\n '_sklearn_auto_wrap_output_keys',\n '_transform',\n '_validate_data',\n '_validate_params',\n 'classes_',\n 'decision_function',\n 'estimator',\n 'estimator_',\n 'feature_names_in_',\n 'fit',\n 'fit_transform',\n 'get_feature_names_out',\n 'get_metadata_routing',\n 'get_params',\n 'get_support',\n 'importance_getter',\n 'inverse_transform',\n 'n_features_',\n 'n_features_in_',\n 'n_features_to_select',\n 'predict',\n 'predict_log_proba',\n 'predict_proba',\n 'ranking_',\n 'score',\n 'set_output',\n 'set_params',\n 'step',\n 'support_',\n 'transform',\n 'verbose']\n```\n:::\n:::\n\n\n::: {#c43f4c6c .cell execution_count=62}\n``` {.python .cell-code}\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected features: ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_Q', 'Sex_male', 'IsMinor']\n```\n:::\n:::\n\n\n::: {#65b91a95 .cell execution_count=63}\n``` {.python .cell-code}\n# Getting the Regression Results \nimport statsmodels.api as sm\nlogit_model = sm.Logit(y,X.astype(float))\nresult = logit_model.fit()\nprint(result.summary2())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.448075\n         Iterations 6\n                         Results: Logit\n=================================================================\nModel:              Logit            Method:           MLE       \nDependent Variable: Survived         Pseudo R-squared: 0.327     \nDate:               2025-02-10 11:07 AIC:              816.4704  \nNo. Observations:   891              BIC:              859.6015  \nDf Model:           8                Log-Likelihood:   -399.24   \nDf Residuals:       882              LL-Null:          -593.33   \nConverged:          1.0000           LLR p-value:      6.3002e-79\nNo. Iterations:     6.0000           Scale:            1.0000    \n------------------------------------------------------------------\n              Coef.   Std.Err.     z      P>|z|    [0.025   0.975]\n------------------------------------------------------------------\nAge          -0.0121    0.0057   -2.1381  0.0325  -0.0232  -0.0010\nFare          0.0015    0.0022    0.6878  0.4916  -0.0028   0.0058\nTravelAlone   0.2803    0.1936    1.4475  0.1477  -0.0992   0.6597\nPclass_1      2.1646    0.2838    7.6272  0.0000   1.6083   2.7208\nPclass_2      1.3948    0.2309    6.0399  0.0000   0.9422   1.8473\nEmbarked_C    0.6014    0.2332    2.5790  0.0099   0.1443   1.0584\nEmbarked_Q    0.6366    0.3139    2.0282  0.0425   0.0214   1.2517\nSex_male     -2.5527    0.1955  -13.0567  0.0000  -2.9359  -2.1695\nIsMinor       0.9987    0.2737    3.6488  0.0003   0.4623   1.5352\n=================================================================\n\n```\n:::\n:::\n\n\n::: {#b253f5ec .cell execution_count=64}\n``` {.python .cell-code}\ncols2 = ['Age',  'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX_alt = final_train[cols2] # features vector\nlogit_model2 = sm.Logit(y,sm.add_constant(X_alt.astype(float)))\nresult2 = logit_model2.fit()\nprint(result2.summary2())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.446363\n         Iterations 6\n                         Results: Logit\n=================================================================\nModel:              Logit            Method:           MLE       \nDependent Variable: Survived         Pseudo R-squared: 0.330     \nDate:               2025-02-10 11:07 AIC:              813.4192  \nNo. Observations:   891              BIC:              856.5503  \nDf Model:           8                Log-Likelihood:   -397.71   \nDf Residuals:       882              LL-Null:          -593.33   \nConverged:          1.0000           LLR p-value:      1.4027e-79\nNo. Iterations:     6.0000           Scale:            1.0000    \n------------------------------------------------------------------\n              Coef.   Std.Err.     z      P>|z|    [0.025   0.975]\n------------------------------------------------------------------\nconst         0.6181    0.3319    1.8623  0.0626  -0.0324   1.2686\nAge          -0.0245    0.0091   -2.7055  0.0068  -0.0423  -0.0068\nTravelAlone   0.1475    0.1973    0.7476  0.4547  -0.2392   0.5343\nPclass_1      2.2826    0.2546    8.9666  0.0000   1.7837   2.7815\nPclass_2      1.3380    0.2340    5.7178  0.0000   0.8793   1.7966\nEmbarked_C    0.5518    0.2351    2.3471  0.0189   0.0910   1.0126\nEmbarked_Q    0.5323    0.3217    1.6548  0.0980  -0.0982   1.1627\nSex_male     -2.6141    0.1973  -13.2482  0.0000  -3.0009  -2.2274\nIsMinor       0.5957    0.3560    1.6735  0.0942  -0.1020   1.2935\n=================================================================\n\n```\n:::\n:::\n\n\n::: {#89014e8d .cell execution_count=65}\n``` {.python .cell-code}\nfrom stargazer.stargazer import Stargazer\ntitanic_logit = Stargazer([result, result2])\ntitanic_logit\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```{=html}\n<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable: Survived</em></td></tr><tr><td style=\"text-align:left\"></td><tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n\n<tr><td style=\"text-align:left\">Age</td><td>-0.012<sup>**</sup></td><td>-0.025<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.006)</td><td>(0.009)</td></tr>\n<tr><td style=\"text-align:left\">Embarked_C</td><td>0.601<sup>***</sup></td><td>0.552<sup>**</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.233)</td><td>(0.235)</td></tr>\n<tr><td style=\"text-align:left\">Embarked_Q</td><td>0.637<sup>**</sup></td><td>0.532<sup>*</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.314)</td><td>(0.322)</td></tr>\n<tr><td style=\"text-align:left\">Fare</td><td>0.002<sup></sup></td><td></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.002)</td><td></td></tr>\n<tr><td style=\"text-align:left\">IsMinor</td><td>0.999<sup>***</sup></td><td>0.596<sup>*</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.274)</td><td>(0.356)</td></tr>\n<tr><td style=\"text-align:left\">Pclass_1</td><td>2.165<sup>***</sup></td><td>2.283<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.284)</td><td>(0.255)</td></tr>\n<tr><td style=\"text-align:left\">Pclass_2</td><td>1.395<sup>***</sup></td><td>1.338<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.231)</td><td>(0.234)</td></tr>\n<tr><td style=\"text-align:left\">Sex_male</td><td>-2.553<sup>***</sup></td><td>-2.614<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.196)</td><td>(0.197)</td></tr>\n<tr><td style=\"text-align:left\">TravelAlone</td><td>0.280<sup></sup></td><td>0.148<sup></sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.194)</td><td>(0.197)</td></tr>\n<tr><td style=\"text-align:left\">const</td><td></td><td>0.618<sup>*</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td>(0.332)</td></tr>\n\n<td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align: left\">Observations</td><td>891</td><td>891</td></tr><tr><td style=\"text-align: left\">Pseudo R<sup>2</sup></td><td>0.327</td><td>0.330</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td><td colspan=\"2\" style=\"text-align: right\"><sup>*</sup>p&lt;0.1; <sup>**</sup>p&lt;0.05; <sup>***</sup>p&lt;0.01</td></tr></table>\n```\n:::\n:::\n\n\n::: {#0a421f1c .cell execution_count=66}\n``` {.python .cell-code}\n# Interpreting the coefficients, which are the log odds; therefore, we need to convert them into odds ratio. \nnp.exp(-2.5527)\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```\nnp.float64(0.0778711298546485)\n```\n:::\n:::\n\n\n::: {#68f4bf19 .cell execution_count=67}\n``` {.python .cell-code}\nnp.exp(result.params) # Getting the Odds Ratio of all Features \n```\n\n::: {.cell-output .cell-output-display execution_count=67}\n```\nAge            0.987957\nFare           1.001519\nTravelAlone    1.323475\nPclass_1       8.710827\nPclass_2       4.033966\nEmbarked_C     1.824624\nEmbarked_Q     1.889992\nSex_male       0.077872\nIsMinor        2.714860\ndtype: float64\n```\n:::\n:::\n\n\n#### Feature Ranking with Recursive Feature Elimination and Cross-validation (`RFECV`)\n\n`RFECV` performs `RFE` in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation\n\n::: {#94ffe747 .cell execution_count=68}\n``` {.python .cell-code}\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(max_iter = 5000), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\nfor x in dir(rfecv):\n  print(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n_RFECV__metadata_request__fit\n__abstractmethods__\n__annotations__\n__class__\n__delattr__\n__dict__\n__dir__\n__doc__\n__eq__\n__firstlineno__\n__format__\n__ge__\n__getattribute__\n__getstate__\n__gt__\n__hash__\n__init__\n__init_subclass__\n__le__\n__lt__\n__module__\n__ne__\n__new__\n__reduce__\n__reduce_ex__\n__repr__\n__setattr__\n__setstate__\n__sizeof__\n__sklearn_clone__\n__sklearn_tags__\n__static_attributes__\n__str__\n__subclasshook__\n__weakref__\n_abc_impl\n_build_request_for_signature\n_check_feature_names\n_check_n_features\n_doc_link_module\n_doc_link_template\n_doc_link_url_param_generator\n_estimator_type\n_fit\n_get_default_requests\n_get_doc_link\n_get_metadata_request\n_get_param_names\n_get_scorer\n_get_support_mask\n_get_tags\n_more_tags\n_parameter_constraints\n_repr_html_\n_repr_html_inner\n_repr_mimebundle_\n_sklearn_auto_wrap_output_keys\n_transform\n_validate_data\n_validate_params\nclasses_\ncv\ncv_results_\ndecision_function\nestimator\nestimator_\nfeature_names_in_\nfit\nfit_transform\nget_feature_names_out\nget_metadata_routing\nget_params\nget_support\nimportance_getter\ninverse_transform\nmin_features_to_select\nn_features_\nn_features_in_\nn_jobs\npredict\npredict_log_proba\npredict_proba\nranking_\nscore\nscoring\nset_output\nset_params\nstep\nsupport_\ntransform\nverbose\n```\n:::\n:::\n\n\n::: {#97092922 .cell execution_count=69}\n``` {.python .cell-code}\n# To get the accuracy\nrfecv.cv_results_\n```\n\n::: {.cell-output .cell-output-display execution_count=69}\n```\n{'mean_test_score': array([0.78672909, 0.78672909, 0.78672909, 0.79009988, 0.78560549,\n        0.78561798, 0.78790262, 0.79574282, 0.79463171]),\n 'std_test_score': array([0.02859935, 0.02859935, 0.02859935, 0.02989151, 0.02604354,\n        0.02546772, 0.03370206, 0.02689652, 0.02691683]),\n 'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}\n```\n:::\n:::\n\n\n::: {#d5cadc1c .cell execution_count=70}\n``` {.python .cell-code}\ntype(rfecv.cv_results_)\n```\n\n::: {.cell-output .cell-output-display execution_count=70}\n```\ndict\n```\n:::\n:::\n\n\n::: {#701525c9 .cell execution_count=71}\n``` {.python .cell-code}\nrfecv.cv_results_\n```\n\n::: {.cell-output .cell-output-display execution_count=71}\n```\n{'mean_test_score': array([0.78672909, 0.78672909, 0.78672909, 0.79009988, 0.78560549,\n        0.78561798, 0.78790262, 0.79574282, 0.79463171]),\n 'std_test_score': array([0.02859935, 0.02859935, 0.02859935, 0.02989151, 0.02604354,\n        0.02546772, 0.03370206, 0.02689652, 0.02691683]),\n 'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}\n```\n:::\n:::\n\n\n::: {#dd3ce174 .cell execution_count=72}\n``` {.python .cell-code}\nrfecv.n_features_\n```\n\n::: {.cell-output .cell-output-display execution_count=72}\n```\nnp.int64(8)\n```\n:::\n:::\n\n\n::: {#5af1bc94 .cell execution_count=73}\n``` {.python .cell-code}\nlist(X.columns[rfecv.support_])\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```\n['Age',\n 'TravelAlone',\n 'Pclass_1',\n 'Pclass_2',\n 'Embarked_C',\n 'Embarked_Q',\n 'Sex_male',\n 'IsMinor']\n```\n:::\n:::\n\n\n::: {#a3c208da .cell execution_count=74}\n``` {.python .cell-code}\n# To check whether the RFE and RFECV generate the same features\nset(list(X.columns[rfecv.support_])) == set(list((X.columns[rfe.support_])))  \n```\n\n::: {.cell-output .cell-output-display execution_count=74}\n```\nTrue\n```\n:::\n:::\n\n\n::: {#12757384 .cell execution_count=75}\n``` {.python .cell-code}\nrfecv.cv_results_.keys()\n```\n\n::: {.cell-output .cell-output-display execution_count=75}\n```\ndict_keys(['mean_test_score', 'std_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'split5_test_score', 'split6_test_score', 'split7_test_score', 'split8_test_score', 'split9_test_score', 'n_features'])\n```\n:::\n:::\n\n\n::: {#c8cedc58 .cell execution_count=76}\n``` {.python .cell-code}\ncrossVal_results = rfecv.cv_results_\ncrossVal_results\ndel crossVal_results['mean_test_score']\ndel crossVal_results['std_test_score']\ncrossVal_results\n```\n\n::: {.cell-output .cell-output-display execution_count=76}\n```\n{'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}\n```\n:::\n:::\n\n\n## Correlation Matrix\n\n::: {#35f7372f .cell execution_count=77}\n``` {.python .cell-code}\nSelected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n                     'Embarked_S', 'Sex_male', 'IsMinor']\nX = final_train[Selected_features] # Recreated features vector \n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\") # for cmap = 'viridis' can also be used.\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-78-output-1.png){width=684 height=493}\n:::\n:::\n\n\n::: {#f5378d2e .cell execution_count=78}\n``` {.python .cell-code}\nplt.subplots(figsize=(8, 5))\nsns.heatmap(final_train[['Survived', 'Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_S', 'Sex_male', 'IsMinor']].corr(), annot = True, cmap = 'viridis')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-79-output-1.png){width=684 height=493}\n:::\n:::\n\n\n## Model Evaluation Procedures\n\n### Model Evaluation Based on Train/Test Split\n\n::: {#e5b3cf4b .cell execution_count=79}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n```\n:::\n\n\n::: {#0e00c9f1 .cell execution_count=80}\n``` {.python .cell-code}\nX = final_train[Selected_features]\ny = final_train['Survived']\n```\n:::\n\n\n::: {#5f5535ba .cell execution_count=81}\n``` {.python .cell-code}\n# use train/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n```\n:::\n\n\n::: {#d2381b90 .cell execution_count=82}\n``` {.python .cell-code}\n# Check classification scores of Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test) # Prediction actual class\ny_pred_proba = logreg.predict_proba(X_test) [:,1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba)) \nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain/Test split results:\nLogisticRegression accuracy is 0.810\nLogisticRegression log_loss is 0.446\nLogisticRegression auc is 0.847\n```\n:::\n:::\n\n\n### Confusion Matrix\n\n::: {#75683752 .cell execution_count=83}\n``` {.python .cell-code}\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion matrix\n\n [[98 13]\n [21 47]]\n\nTrue Positives(TP) =  98\n\nTrue Negatives(TN) =  47\n\nFalse Positives(FP) =  13\n\nFalse Negatives(FN) =  21\n```\n:::\n:::\n\n\n::: {#127314cb .cell execution_count=84}\n``` {.python .cell-code}\n# Visualizing Confusion Matrix\nplt.figure(figsize=(6,4))\ncm_matrix = pd.DataFrame(data= cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-85-output-1.png){width=469 height=342}\n:::\n:::\n\n\n### Classification Report\n\n::: {#22b84144 .cell execution_count=85}\n``` {.python .cell-code}\nprint (classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.82      0.88      0.85       111\n           1       0.78      0.69      0.73        68\n\n    accuracy                           0.81       179\n   macro avg       0.80      0.79      0.79       179\nweighted avg       0.81      0.81      0.81       179\n\n```\n:::\n:::\n\n\n### ROC-AUC Curve\n\n::: {#35ccb8ad .cell execution_count=86}\n``` {.python .cell-code}\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility (true positive rate (tpr)) > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall/sensitivity)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-87-output-1.png){width=606 height=457}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing a threshold of 0.087 guarantees a sensitivity of 0.956 and a specificity of 0.225, i.e. a false positive rate of 77.48%.\n```\n:::\n:::\n\n\n## Model Evaluation Based on K-fold Cross-validation `cross_val_score()` Function\n\n::: {#b496d31c .cell execution_count=87}\n``` {.python .cell-code}\n# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression(max_iter=5000)\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-fold cross-validation results:\nLogisticRegression average accuracy is 0.796\nLogisticRegression average log_loss is 0.454\nLogisticRegression average auc is 0.850\n```\n:::\n:::\n\n\n### Model Evaluation Based on K-fold Cross-validation Using `cross_validate()` Function\n\n::: {#45965324 .cell execution_count=88}\n``` {.python .cell-code}\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std())) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-fold cross-validation results:\nLogisticRegression average accuracy: 0.796 (+/-0.024)\nLogisticRegression average log_loss: 0.454 (+/-0.037)\nLogisticRegression average auc: 0.850 (+/-0.028)\n```\n:::\n:::\n\n\nWhat happens when we add the feature `Fare` - \n\n::: {#aaf4abd1 .cell execution_count=89}\n``` {.python .cell-code}\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"]\nX = final_train[cols]\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, final_train[cols], y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-fold cross-validation results:\nLogisticRegression average accuracy: 0.795 (+/-0.027)\nLogisticRegression average log_loss: 0.455 (+/-0.037)\nLogisticRegression average auc: 0.849 (+/-0.028)\n```\n:::\n:::\n\n\nWe notice that the model is slightly deteriorated. The `Fare` variable does not carry any useful information. Its presence is just a noise for the logistic regression model.\n\n\n## `GridSearchCV` Evaluating Using Multiple Scorers Simultaneously\n\n::: {#1cf1de3a .cell execution_count=90}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\nX = final_train[Selected_features]\n\nparam_grid = {'C': np.arange(1e-05, 3, 0.1)}\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n\ngs = GridSearchCV(LogisticRegression(max_iter=5000), return_train_score=True,\n                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n\ngs.fit(X, y)\nresults = gs.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(gs.best_estimator_))\nprint(\"best params: \" + str(gs.best_params_))\nprint('best score:', gs.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, param_grid['C'].max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n====================\nbest params: LogisticRegression(C=np.float64(2.50001), max_iter=5000)\nbest params: {'C': np.float64(2.50001)}\nbest score: 0.8069662921348316\n====================\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-91-output-2.png){width=826 height=829}\n:::\n:::\n\n\n### `GridSearchCV` Evaluating Using Multiple scorers, `RepeatedStratifiedKFold` and `pipeline` for Preprocessing Simultaneously\n\n::: {#d8cff0aa .cell execution_count=91}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\n\n#Define simple model\n###############################################################################\nC = np.arange(1e-05, 5.5, 0.1)\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\nlog_reg = LogisticRegression(max_iter=5000)\n\n#Simple pre-processing estimators\n###############################################################################\nstd_scale = StandardScaler(with_mean=False, with_std=False)\n#std_scale = StandardScaler()\n\n#Defining the CV method: Using the Repeated Stratified K Fold\n###############################################################################\n\nn_folds=5\nn_repeats=5\n\nrskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)\n\n#Creating simple pipeline and defining the gridsearch\n###############################################################################\n\nlog_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])\n\nlog_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,\n              scoring=scoring, return_train_score=True,\n              param_grid=dict(clf__C=C), refit='Accuracy')\n\nlog_clf.fit(X, y)\nresults = log_clf.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(log_clf.best_estimator_))\nprint(\"best params: \" + str(log_clf.best_params_))\nprint('best score:', log_clf.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, C.max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_clf__C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n====================\nbest params: Pipeline(steps=[('scale', StandardScaler(with_mean=False, with_std=False)),\n                ('clf',\n                 LogisticRegression(C=np.float64(4.90001), max_iter=5000))])\nbest params: {'clf__C': np.float64(4.90001)}\nbest score: 0.7995505617977527\n====================\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-92-output-2.png){width=826 height=829}\n:::\n:::\n\n\n## Regularization \n\n`Regularization` is a method of preventing overfitting, which is a common problem in machine learning. Overfitting means that your model learns too much from the specific data you have, and fails to generalize well to new or unseen data. This can lead to poor predictions and low performance. Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables.\n\nSince the size of each coefficient depends on the scale of its corresponding variable, scaling the data is required so that the regularization penalizes each variable equally. The regularization strength is determined by `C` and as `C` increases, the regularization term becomes smaller (and for extremely large `C` values, it's as if there is no regularization at all).\n\nIf the initial model is overfit (as in, it fits the training data too well), then adding a strong regularization term (with small `C` value) makes the model perform worse for the training data, but introducing such \"noise\" improves the model's performance on unseen (or test) data.\n\nAn example with 1000 samples and 200 features shown below. As can be seen from the plot of accuracy over different values of C, if C is large (with very little regularization), there is a big gap between how the model performs on training data and test data. However, as C decreases, the model performs worse on training data but performs better on test data (test accuracy increases). However, when C becomes too small (or the regularization becomes too strong), the model begins performing worse again because now the regularization term completely dominates the objective function.\n\n::: {#94bbbbed .cell execution_count=92}\n``` {.python .cell-code}\n# Necessary Python Packages \nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# make sample data\nX, y = make_classification(1000, 200, n_informative=195, random_state=2023)\n# split into train-test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2023)\n\n# normalize the data\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# train Logistic Regression models for different values of C\n# and collect train and test accuracies\nscores = {}\nfor C in (10**k for k in range(-6, 6)):\n    lr = LogisticRegression(C=C)\n    lr.fit(X_train, y_train)\n    scores[C] = {'train accuracy': lr.score(X_train, y_train), \n                 'test accuracy': lr.score(X_test, y_test)}\n\n# plot the accuracy scores for different values of C\npd.DataFrame.from_dict(scores, 'index').plot(logx=True, xlabel='C', ylabel='accuracy')\n```\n\n::: {.cell-output .cell-output-display}\n![](chap04_logistic_files/figure-html/cell-93-output-1.png){width=593 height=441}\n:::\n:::\n\n\n### Types of Regularization \n\n#### `L1` regularization\n\n`L1` regularization, also known as `Lasso regularization`, adds the sum of the absolute values of the model’s coefficients to the loss function. It encourages sparsity in the model by shrinking some coefficients to precisely zero. This has the effect of performing feature selection, as the model can effectively ignore irrelevant or less important features. L1 regularization is particularly useful when dealing with high-dimensional datasets with desired feature selection.\n\nMathematically, the `L1` regularization term can be written as:\n\n**`L1 regularization = λ * Σ|wi|`**\n\nHere, `λ` is the regularization parameter that controls the strength of regularization, `wi` represents the individual model coefficients and the sum is taken over all coefficients.\n\n\n#### `L2` regularization\n\n`L2` regularization, also known as `Ridge regularization`, adds the sum of the squared values of the model’s coefficients to the loss function. Unlike L1 regularization, L2 regularization does not force the coefficients to be exactly zero but instead encourages them to be small. L2 regularization can prevent overfitting by spreading the influence of a single feature across multiple features. It is advantageous when there are correlations between the input features.\n\nMathematically, the `L2` regularization term can be written as:\n\n**`L2 regularization = λ * Σ(wi^2)`**\n\nSimilar to `L1` regularization, `λ` is the regularization parameter, and `wi` represents the model coefficients. The sum is taken over all coefficients, and the squares of the coefficients are summed.\n\n\nThe choice between `L1` and `L2` regularization depends on the specific problem and the characteristics of the data. For example, `L1` regularization produces sparse models, which can be advantageous when feature selection is desired. `L2` regularization, on the other hand, encourages small but non-zero coefficients and can be more suitable when there are strong correlations between features.\n\n## Conclusion\n\n",
    "supporting": [
      "chap04_logistic_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}