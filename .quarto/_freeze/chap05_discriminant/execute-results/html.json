{
  "hash": "4debc19841e8200897a6aefa51175069",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"<center> Chapter # 05 <br> Discriminant Analysis\"\nformat: \n  html: \n    toc: true\n    #toc-title: Table of Contents \n    toc-depth: 5\n    number-sections: true\n    mainfont: emoji\n---\n\n\n\n\n## Introduction\n\n\n\n## Assumptions of Linear Discriminant Analysis \n\nDiscriminant analysis assumes that:\n\n  1. The data is normally distributed.\n  \n  2. Means of each class are specific to that class.\n  \n  3. All classes have a common covariance matrix. \n\nIf these assumptions are realized, DA generates a linear decision boundary.\n\n\n## Loading Python Packages\n\n::: {#cb70fb1a .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Python version that is used for this code file is {}'.format(sys.version))\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\nprint ('The Panda version that is used for this code file is {}'.format(pd.__version__))\nprint ('The Numpy version that is used for this code file is {}'.format(np.__version__))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Python version that is used for this code file is 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\nThe Scikit-learn version that is used for this code file is 1.6.0\nThe Panda version that is used for this code file is 2.2.3\nThe Numpy version that is used for this code file is 2.2.1\n```\n:::\n:::\n\n\n## Working Directory\n\n::: {#a64713d6 .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nos.getcwd()\n```\n:::\n\n\n::: {#ce55720c .cell execution_count=3}\n``` {.python .cell-code}\nfor x in os.listdir():\n  print (x)\n```\n:::\n\n\n## Importing Datasets\n\n::: {#b3b5b7bc .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn import datasets\ndataset = datasets.load_wine()\n```\n:::\n\n\n## Metadata of the Imported Dataset\n\n::: {#975687ef .cell execution_count=5}\n``` {.python .cell-code}\ndataset.keys()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])\n```\n:::\n:::\n\n\n::: {#b0154804 .cell execution_count=6}\n``` {.python .cell-code}\ndataset['data']\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n        1.065e+03],\n       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n        1.050e+03],\n       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n        1.185e+03],\n       ...,\n       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n        8.350e+02],\n       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n        8.400e+02],\n       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n        5.600e+02]], shape=(178, 13))\n```\n:::\n:::\n\n\n::: {#e9eaae91 .cell execution_count=7}\n``` {.python .cell-code}\ndataset['target']\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n```\n:::\n:::\n\n\n::: {#942c6b2c .cell execution_count=8}\n``` {.python .cell-code}\ndataset['target_names']\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray(['class_0', 'class_1', 'class_2'], dtype='<U7')\n```\n:::\n:::\n\n\n::: {#8353276a .cell execution_count=9}\n``` {.python .cell-code}\n# Creating Data frame from the array \ndata = pd.DataFrame(dataset['data'], columns = dataset['feature_names'])\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#43f37db5 .cell execution_count=10}\n``` {.python .cell-code}\n# Feature Vector \nfeatures_df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n# Target Vector \ntarget_df = pd.Categorical.from_codes(dataset.target, dataset.target_names)\n```\n:::\n\n\n::: {#f0321814 .cell execution_count=11}\n``` {.python .cell-code}\ntarget_df\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n['class_0', 'class_0', 'class_0', 'class_0', 'class_0', ..., 'class_2', 'class_2', 'class_2', 'class_2', 'class_2']\nLength: 178\nCategories (3, object): ['class_0', 'class_1', 'class_2']\n```\n:::\n:::\n\n\n::: {#689b0b20 .cell execution_count=12}\n``` {.python .cell-code}\n# Joining the above two datasets \ndf = features_df.join(pd.Series(target_df, name = 'class'))\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n      <td>class_0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#498a6c98 .cell execution_count=13}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype   \n---  ------                        --------------  -----   \n 0   alcohol                       178 non-null    float64 \n 1   malic_acid                    178 non-null    float64 \n 2   ash                           178 non-null    float64 \n 3   alcalinity_of_ash             178 non-null    float64 \n 4   magnesium                     178 non-null    float64 \n 5   total_phenols                 178 non-null    float64 \n 6   flavanoids                    178 non-null    float64 \n 7   nonflavanoid_phenols          178 non-null    float64 \n 8   proanthocyanins               178 non-null    float64 \n 9   color_intensity               178 non-null    float64 \n 10  hue                           178 non-null    float64 \n 11  od280/od315_of_diluted_wines  178 non-null    float64 \n 12  proline                       178 non-null    float64 \n 13  class                         178 non-null    category\ndtypes: category(1), float64(13)\nmemory usage: 18.5 KB\n```\n:::\n:::\n\n\n::: {#7cbc9ad6 .cell execution_count=14}\n``` {.python .cell-code}\ndf.columns\nnum_features= dataset.feature_names\nnum_features\n# Looping functions \ndf.groupby('class')[num_features].mean().transpose()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>class</th>\n      <th>class_0</th>\n      <th>class_1</th>\n      <th>class_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alcohol</th>\n      <td>13.744746</td>\n      <td>12.278732</td>\n      <td>13.153750</td>\n    </tr>\n    <tr>\n      <th>malic_acid</th>\n      <td>2.010678</td>\n      <td>1.932676</td>\n      <td>3.333750</td>\n    </tr>\n    <tr>\n      <th>ash</th>\n      <td>2.455593</td>\n      <td>2.244789</td>\n      <td>2.437083</td>\n    </tr>\n    <tr>\n      <th>alcalinity_of_ash</th>\n      <td>17.037288</td>\n      <td>20.238028</td>\n      <td>21.416667</td>\n    </tr>\n    <tr>\n      <th>magnesium</th>\n      <td>106.338983</td>\n      <td>94.549296</td>\n      <td>99.312500</td>\n    </tr>\n    <tr>\n      <th>total_phenols</th>\n      <td>2.840169</td>\n      <td>2.258873</td>\n      <td>1.678750</td>\n    </tr>\n    <tr>\n      <th>flavanoids</th>\n      <td>2.982373</td>\n      <td>2.080845</td>\n      <td>0.781458</td>\n    </tr>\n    <tr>\n      <th>nonflavanoid_phenols</th>\n      <td>0.290000</td>\n      <td>0.363662</td>\n      <td>0.447500</td>\n    </tr>\n    <tr>\n      <th>proanthocyanins</th>\n      <td>1.899322</td>\n      <td>1.630282</td>\n      <td>1.153542</td>\n    </tr>\n    <tr>\n      <th>color_intensity</th>\n      <td>5.528305</td>\n      <td>3.086620</td>\n      <td>7.396250</td>\n    </tr>\n    <tr>\n      <th>hue</th>\n      <td>1.062034</td>\n      <td>1.056282</td>\n      <td>0.682708</td>\n    </tr>\n    <tr>\n      <th>od280/od315_of_diluted_wines</th>\n      <td>3.157797</td>\n      <td>2.785352</td>\n      <td>1.683542</td>\n    </tr>\n    <tr>\n      <th>proline</th>\n      <td>1115.711864</td>\n      <td>519.507042</td>\n      <td>629.895833</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Analysis of Variance (`ANOVA`)\n\nOne-way ANOVA (also known as \"Analysis of Variance\") is a test that is used to find out whether there exists a statistically significant difference between the mean values of more than one group.\n\nSee @fig-corr to know when you should use which correlation\n\n::: {#fig-corr}\n![](images/corr.webp){fig-align=\"center\"}\n\nWhen You Should Use Which Correlation?\n:::\n\nA one-way ANOVA has the below given null and alternative hypotheses:\n\n**H~0~ (Null hypothesis)**: `μ1 = μ2 = μ3 = … = μk` (It implies that the means of all the population are equal)\n\n**H~1~ (Alternate hypothesis)**: It states that there will be at least one population mean that differs from the rest\n\n::: {#d01a635f .cell execution_count=15}\n``` {.python .cell-code}\ndataset.target_names\nalc_class0 = df[df['class']=='class_0']['alcohol']\ntype(alc_class0)\nalc_class1 = df[df['class']=='class_1']['alcohol']\nalc_class2 = df[df['class']=='class_2']['alcohol']\n\n\nfrom scipy.stats import f_oneway\nf_oneway(alc_class0,alc_class1,alc_class2)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nF_onewayResult(statistic=np.float64(135.07762424279912), pvalue=np.float64(3.319503795619639e-36))\n```\n:::\n:::\n\n\nThe `F statistic` is `135.0776` and `p-value` is `0.000`. Since the p-value is less than 0.05, we reject Null Hypothesis (**H~0~**). The findings imply that there exists a difference between three groups for the variable `alcohol`.\n\nSo, where does the difference come from? We can perform `Post Hoc` Analysis to check where does the differences come from\n\n::: {#dc6e0f01 .cell execution_count=16}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\ntukey = pairwise_tukeyhsd(endog=df['alcohol'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<table class=\"simpletable\">\n<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>\n<tr>\n  <th>group1</th>  <th>group2</th>  <th>meandiff</th> <th>p-adj</th>  <th>lower</th>   <th>upper</th>  <th>reject</th>\n</tr>\n<tr>\n  <td>class_0</td> <td>class_1</td>  <td>-1.466</td>   <td>0.0</td>  <td>-1.6792</td> <td>-1.2528</td>  <td>True</td> \n</tr>\n<tr>\n  <td>class_0</td> <td>class_2</td>  <td>-0.591</td>   <td>0.0</td>  <td>-0.8262</td> <td>-0.3558</td>  <td>True</td> \n</tr>\n<tr>\n  <td>class_1</td> <td>class_2</td>   <td>0.875</td>   <td>0.0</td>  <td>0.6489</td>  <td>1.1011</td>   <td>True</td> \n</tr>\n</table>\n```\n:::\n:::\n\n\nSee @fig-tukey for the sources of differences of target variable and `alcohol`\n\n::: {#cell-fig-tukey .cell execution_count=17}\n``` {.python .cell-code}\ntukey.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Where Does the Difference Come From between Target variable and variable alcohol?](chap05_discriminant_files/figure-html/fig-tukey-output-1.png){#fig-tukey width=835 height=509}\n:::\n:::\n\n\n### Using `ANOVA` for Feature Selection\n\n::: {#1bd98bdd .cell execution_count=18}\n``` {.python .cell-code}\nlist(dataset.target_names)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n[np.str_('class_0'), np.str_('class_1'), np.str_('class_2')]\n```\n:::\n:::\n\n\n::: {#28116c4d .cell execution_count=19}\n``` {.python .cell-code}\ntukey_malic = pairwise_tukeyhsd(endog=df['malic_acid'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey_malic.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<table class=\"simpletable\">\n<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>\n<tr>\n  <th>group1</th>  <th>group2</th>  <th>meandiff</th>  <th>p-adj</th>  <th>lower</th>   <th>upper</th> <th>reject</th>\n</tr>\n<tr>\n  <td>class_0</td> <td>class_1</td>  <td>-0.078</td>  <td>0.8855</td> <td>-0.4703</td> <td>0.3143</td>  <td>False</td>\n</tr>\n<tr>\n  <td>class_0</td> <td>class_2</td>  <td>1.3231</td>    <td>0.0</td>  <td>0.8902</td>  <td>1.7559</td>  <td>True</td> \n</tr>\n<tr>\n  <td>class_1</td> <td>class_2</td>  <td>1.4011</td>    <td>0.0</td>  <td>0.9849</td>  <td>1.8172</td>  <td>True</td> \n</tr>\n</table>\n```\n:::\n:::\n\n\nSee @fig-tukey2 for the difference between target variable and `malic_acid`\n\n::: {#cell-fig-tukey2 .cell execution_count=20}\n``` {.python .cell-code}\ntukey_malic.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Where Does the Difference Come From between Target variable and variable malic_acid?](chap05_discriminant_files/figure-html/fig-tukey2-output-1.png){#fig-tukey2 width=826 height=509}\n:::\n:::\n\n\n::: {#934711cd .cell execution_count=21}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ndf.rename(columns = {\n  'od280/od315_of_diluted_wines': 'diluted_wines'}, inplace = True)\n```\n:::\n\n\n\n## Linear Discriminant Analysis\n\n::: {#98b57523 .cell execution_count=23}\n``` {.python .cell-code}\nX = dataset.data\ny = dataset.target\ntarget_names = dataset.target_names\n```\n:::\n\n\n::: {#74b3fe3e .cell execution_count=24}\n``` {.python .cell-code}\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n```\n:::\n\n\n::: {#7190fc24 .cell execution_count=25}\n``` {.python .cell-code}\nlda = LinearDiscriminantAnalysis(n_components = 2)\n```\n:::\n\n\n::: {#674eeb90 .cell execution_count=26}\n``` {.python .cell-code}\nX_r2 = lda.fit(X,y).transform(X)\n```\n:::\n\n\n::: {#65b5cb5c .cell execution_count=27}\n``` {.python .cell-code}\nX_r2[0:10,]\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\narray([[-4.70024401,  1.97913835],\n       [-4.30195811,  1.17041286],\n       [-3.42071952,  1.42910139],\n       [-4.20575366,  4.00287148],\n       [-1.50998168,  0.4512239 ],\n       [-4.51868934,  3.21313756],\n       [-4.52737794,  3.26912179],\n       [-4.14834781,  3.10411765],\n       [-3.86082876,  1.95338263],\n       [-3.36662444,  1.67864327]])\n```\n:::\n:::\n\n\n::: {#22ce85b6 .cell execution_count=28}\n``` {.python .cell-code}\nlda.explained_variance_ratio_\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([0.68747889, 0.31252111])\n```\n:::\n:::\n\n\n### Plotting the Dataset\n\n::: {#e3e06e37 .cell execution_count=29}\n``` {.python .cell-code}\nplt.figure(figsize = (15,8))\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target,cmap = 'gnuplot', alpha = 0.7)\nplt.xlabel('DF1')\nplt.ylabel('DF2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap05_discriminant_files/figure-html/cell-30-output-1.png){width=1184 height=657}\n:::\n:::\n\n\n### Distribution of LDA Components\n\n::: {#9be7f9ff .cell execution_count=30}\n``` {.python .cell-code}\ndf_lda = pd.DataFrame(zip(X_r2[:,0], X_r2[:,1],y), columns = [\"ld1\", \"ld2\", \"class\"])\nsns.set(rc={'figure.figsize':(12,8)})\nplt.subplot(2,1,1)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld1')\nplt.subplot(2,1,2)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap05_discriminant_files/figure-html/cell-31-output-1.png){width=961 height=657}\n:::\n:::\n\n\n## Using LDA to Solve Classification Problem\n\n::: {#747d5e93 .cell execution_count=31}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 2024)\n```\n:::\n\n\n### Training the Model\n\n::: {#d0582f29 .cell execution_count=32}\n``` {.python .cell-code}\nlda_model = LinearDiscriminantAnalysis(n_components = 2)\nlda_model.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearDiscriminantAnalysis(n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearDiscriminantAnalysis</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">?<span>Documentation for LinearDiscriminantAnalysis</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearDiscriminantAnalysis(n_components=2)</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\n### Testing the Model\n\n::: {#8a80aa54 .cell execution_count=33}\n``` {.python .cell-code}\ny_pred = lda_model.predict(X_test)\n```\n:::\n\n\n### Checking Model Accuracy\n\n::: {#8adc7e31 .cell execution_count=34}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score\nprint (\"The Accuracy of LDA Model is %0.2f%%.\" % (accuracy_score(y_test, y_pred)*100))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe Accuracy of LDA Model is 100.00%.\n```\n:::\n:::\n\n\n::: {#704ba520 .cell execution_count=35}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap05_discriminant_files/figure-html/cell-36-output-1.png){width=863 height=637}\n:::\n:::\n\n\n## Cross Validation\n\n::: {#63f013bf .cell execution_count=36}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n```\n:::\n\n\n::: {#87cee82b .cell execution_count=37}\n``` {.python .cell-code}\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 50, random_state = 1)\nscores = cross_val_score(lda_model, X,y, scoring = \"accuracy\", cv = cv, n_jobs = -1)\nprint(np.mean(scores))\n```\n:::\n\n\n## LDA vs PCA (Visualization Difference)\n\n### PCA Model\n\n::: {#0727f37c .cell execution_count=38}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_pca = pca.fit(X).transform(X)\n```\n:::\n\n\n::: {#b003f055 .cell execution_count=39}\n``` {.python .cell-code}\nfrom pylab import *\nsubplot(2,1,1)\ntitle (\"PCA\")\nplt.scatter(X_pca[:,0], X_pca[:,1], c = dataset.target, cmap = \"gnuplot\")\nsubplot(2,1,2)\ntitle (\"LDA\")\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target, cmap = \"gnuplot\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap05_discriminant_files/figure-html/cell-40-output-1.png){width=964 height=657}\n:::\n:::\n\n\nBoth algorithms have successfully reduced the components but created different clusters because both have reduced the components based on different principles.\n\nNow let's also visualize and compare the distributions of each of the algorithms on their respective components. Here we will visualize the distribution of the first component of each algorithm (LDA-1 and PCA-1).\n\n::: {#61b04720 .cell execution_count=40}\n``` {.python .cell-code}\n# creating dataframs\ndf=pd.DataFrame(zip(X_pca[:,0],X_r2[:,0],y),columns=[\"pc1\",\"ld1\",\"class\"])\n# plotting the lda1\nplt.subplot(2,1,1)\nsns.boxplot(x='class', y='ld1', data=df)\n# plotting pca1\nplt.subplot(2,1,2)\nsns.boxplot(x='class', y='pc1', data=df)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap05_discriminant_files/figure-html/cell-41-output-1.png){width=978 height=657}\n:::\n:::\n\n\nThere is a slight difference in the distribution of both of the algorithms. For example, the PCA result shows outliers only at the first target variable, whereas the LDA result contains outliers for every target variable.\n\n## Variance Covariance Matrix\n\nTo calculate the covariance matrix in Python using NumPy, you can import NumPy as np, create or load your data as a NumPy array, subtract the mean of each column from the data, transpose the array, multiply the transposed array and original array, divide the multiplied array by the number of observations, and print the array. Alternatively, you can use the `np.cov` function which takes the data array as an input and returns the covariance matrix as an output.\n\nTo learn more about [variance-covariance matrix.](https://builtin.com/data-science/covariance-matrix)\n\nTo learn more about [eigenvalues and eigenvectors.](https://github.com/learn-co-students/dsc-3-34-07-pca-implementation-visualization-python-numpy-lab-seattle-ds-career-040119)\n\n::: {#7b71bbed .cell execution_count=41}\n``` {.python .cell-code}\nA = [45, 37, 42, 35, 39]\nB = [38, 31, 26, 28, 33]\nC = [10, 15, 17, 21, 12]\n\ndata = np.array([A, B, C])\n\ncov_matrix = np.cov(data, bias=True)\nprint(cov_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 12.64   7.68  -9.6 ]\n [  7.68  17.36 -13.8 ]\n [ -9.6  -13.8   14.8 ]]\n```\n:::\n:::\n\n\n::: {#406599d1 .cell execution_count=42}\n``` {.python .cell-code}\nnp.var(A)\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\nnp.float64(12.64)\n```\n:::\n:::\n\n\n::: {#d01f04f4 .cell execution_count=43}\n``` {.python .cell-code}\nnp.var(C)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\nnp.float64(14.8)\n```\n:::\n:::\n\n\n### Eigenvalues and Eigenvector for Variance-covariance Matrix\n\n::: {#5ede439c .cell execution_count=44}\n``` {.python .cell-code}\n# eigendecomposition\nfrom numpy.linalg import eig\n```\n:::\n\n\n::: {#fb82fb3f .cell execution_count=45}\n``` {.python .cell-code}\n# calculate eigendecomposition\nvalues, vectors = eig(cov_matrix)\n# Eigenvalues \nprint(values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[36.22111819  6.98906964  1.58981217]\n```\n:::\n:::\n\n\n::: {#729c66b0 .cell execution_count=46}\n``` {.python .cell-code}\n# Eigenvectors \nprint(vectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[-0.45932764 -0.83268027  0.30929225]\n [-0.63870049  0.55159313  0.53647618]\n [ 0.61731661 -0.04887322  0.78519527]]\n```\n:::\n:::\n\n\n## Regularized Discriminant Analysis\n\nSince regularization techniques have been highly successful in the solution of ill-posed and poorly-posed inverse problems so to mitigate this problem the most reliable way is to use the regularization technique. \n\n  * A poorly posed problem occurs when the number of parameters to be estimated is comparable to the number of observations.\n  \n  * Similarly,ill-posed if that number exceeds the sample size.\n  \nIn these cases the parameter estimates can be highly unstable, giving rise to high variance. Regularization would help to improve the estimates by shifting them away from their sample-based values towards values that are more physically valid; this would be achieved by applying shrinkage to each class. \n\nWhile regularization reduces the variance associated with the sample-based estimate, it may also increase bias. This process known as bias-variance trade-off is generally controlled by one or more degree-of-belief parameters that determine how strongly biasing towards “plausible” values of population parameters takes place.\n\nWhenever the sample size is not significantly greater than the dimension of measurement space for any class, Quantitative discriminant analysis (QDA) is ill-posed. Typically, `regularization is applied to a discriminant analysis by replacing the individual class sample covariance matrices with the average weights assigned to the eigenvalues`. \n\nThis applies a considerable degree of regularization by substantially reducing the number of parameters to be estimated. `The regularization parameter () which is added to the equation of QDA and LDA takes a value between 0 to 1`. It controls the degree of shrinkage of the individual class covariance matrix estimates toward the pooled estimate. Values between these limits represent degrees of regularization.\n\n::: {#2055c352 .cell execution_count=47}\n``` {.python .cell-code}\nfrom sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,confusion_matrix\nfrom imblearn.over_sampling import SMOTE # To install module, run this line of code - pip install imblearn\nfrom sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n```\n:::\n\n\n::: {#fc844374 .cell execution_count=48}\n``` {.python .cell-code}\n# Reading a new dataset \ndf = pd.read_csv('DATA/healthcare-dataset-stroke-data.csv')\nprint(\"Records = \", df.shape[0], \"\\nFeatures = \", df.shape[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecords =  5110 \nFeatures =  12\n```\n:::\n:::\n\n\n::: {#2546cf8b .cell execution_count=49}\n``` {.python .cell-code}\ndf.sample(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>hypertension</th>\n      <th>heart_disease</th>\n      <th>ever_married</th>\n      <th>work_type</th>\n      <th>Residence_type</th>\n      <th>avg_glucose_level</th>\n      <th>bmi</th>\n      <th>smoking_status</th>\n      <th>stroke</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3837</th>\n      <td>27854</td>\n      <td>Female</td>\n      <td>23.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>Private</td>\n      <td>Rural</td>\n      <td>96.28</td>\n      <td>31.1</td>\n      <td>never smoked</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3969</th>\n      <td>33185</td>\n      <td>Male</td>\n      <td>59.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>Govt_job</td>\n      <td>Urban</td>\n      <td>83.60</td>\n      <td>27.5</td>\n      <td>formerly smoked</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3924</th>\n      <td>39958</td>\n      <td>Male</td>\n      <td>18.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>No</td>\n      <td>Private</td>\n      <td>Rural</td>\n      <td>118.93</td>\n      <td>22.4</td>\n      <td>never smoked</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>732</th>\n      <td>31308</td>\n      <td>Female</td>\n      <td>49.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Yes</td>\n      <td>Private</td>\n      <td>Urban</td>\n      <td>114.50</td>\n      <td>35.9</td>\n      <td>formerly smoked</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>794</th>\n      <td>21688</td>\n      <td>Female</td>\n      <td>42.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Yes</td>\n      <td>Private</td>\n      <td>Rural</td>\n      <td>88.31</td>\n      <td>24.0</td>\n      <td>smokes</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#e9c59529 .cell execution_count=50}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5110 entries, 0 to 5109\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 5110 non-null   int64  \n 1   gender             5110 non-null   object \n 2   age                5110 non-null   float64\n 3   hypertension       5110 non-null   int64  \n 4   heart_disease      5110 non-null   int64  \n 5   ever_married       5110 non-null   object \n 6   work_type          5110 non-null   object \n 7   Residence_type     5110 non-null   object \n 8   avg_glucose_level  5110 non-null   float64\n 9   bmi                4909 non-null   float64\n 10  smoking_status     5110 non-null   object \n 11  stroke             5110 non-null   int64  \ndtypes: float64(3), int64(4), object(5)\nmemory usage: 479.2+ KB\n```\n:::\n:::\n\n\n::: {#13ebaf03 .cell execution_count=51}\n``` {.python .cell-code}\n# Missing Values \n(df.isnull().sum()/len(df)*100)\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\nid                   0.000000\ngender               0.000000\nage                  0.000000\nhypertension         0.000000\nheart_disease        0.000000\never_married         0.000000\nwork_type            0.000000\nResidence_type       0.000000\navg_glucose_level    0.000000\nbmi                  3.933464\nsmoking_status       0.000000\nstroke               0.000000\ndtype: float64\n```\n:::\n:::\n\n\n::: {#0fc78de1 .cell execution_count=52}\n``` {.python .cell-code}\n# Dropping the Missing Observations\ndf.dropna(axis = 0, inplace = True)\ndf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\n(4909, 12)\n```\n:::\n:::\n\n\n::: {#4ffcf6db .cell execution_count=53}\n``` {.python .cell-code}\n# Creating the Dummies \ndf_pre = pd.get_dummies(df, drop_first = True)\ndf_pre.sample(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>hypertension</th>\n      <th>heart_disease</th>\n      <th>avg_glucose_level</th>\n      <th>bmi</th>\n      <th>stroke</th>\n      <th>gender_Male</th>\n      <th>gender_Other</th>\n      <th>ever_married_Yes</th>\n      <th>work_type_Never_worked</th>\n      <th>work_type_Private</th>\n      <th>work_type_Self-employed</th>\n      <th>work_type_children</th>\n      <th>Residence_type_Urban</th>\n      <th>smoking_status_formerly smoked</th>\n      <th>smoking_status_never smoked</th>\n      <th>smoking_status_smokes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2600</th>\n      <td>44112</td>\n      <td>51.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>219.92</td>\n      <td>33.5</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4691</th>\n      <td>25878</td>\n      <td>55.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>97.68</td>\n      <td>47.1</td>\n      <td>0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1389</th>\n      <td>68235</td>\n      <td>12.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>86.00</td>\n      <td>20.1</td>\n      <td>0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4597</th>\n      <td>40842</td>\n      <td>29.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108.14</td>\n      <td>25.1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>768</th>\n      <td>59521</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>74.88</td>\n      <td>31.6</td>\n      <td>0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#762a2750 .cell execution_count=54}\n``` {.python .cell-code}\n# Training and Testing the Split \nX = df_pre.drop(['stroke'], axis = 1)\ny = df_pre['stroke']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 25)\n```\n:::\n\n\n::: {#cba91c78 .cell execution_count=55}\n``` {.python .cell-code}\n# Building the LDA\nLDA = LinearDiscriminantAnalysis()\nLDA.fit_transform(X_train, y_train)\nX_test['predictions'] = LDA.predict(X_test)\nConfusionMatrixDisplay.from_predictions(y_test, X_test['predictions'])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap05_discriminant_files/figure-html/cell-56-output-1.png){width=771 height=657}\n:::\n:::\n\n\n::: {#4897dd71 .cell execution_count=56}\n``` {.python .cell-code}\ntn, fp, fn, tp = confusion_matrix(list(y_test), list(X_test['predictions']), labels=[0, 1]).ravel()\n```\n:::\n\n\n::: {#cb45d3f7 .cell execution_count=57}\n``` {.python .cell-code}\nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\nprint(\"Precision score\",precision_score(y_test,X_test['predictions']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue Positive : 6\nTrue Negative : 1397\nFalse Positive : 13\nFalse Negative : 57\nPrecision score 0.3157894736842105\n```\n:::\n:::\n\n\nIt has only 32% precision rate, which is very poor performance.\n\n::: {#655eff62 .cell execution_count=58}\n``` {.python .cell-code}\nprint(\"Accuracy Score\",accuracy_score(y_test,X_test['predictions']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy Score 0.9524779361846571\n```\n:::\n:::\n\n\nThe accuracy is approximately 95%, but the precision is 32%. \n\n### Cross Validation of the Dataset \n\n::: {#04bdc727 .cell execution_count=59}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n```\n:::\n\n\n::: {#1097bb08 .cell execution_count=60}\n``` {.python .cell-code}\n#Define method to evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=50, random_state=1)\n\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.23822585829203477\n```\n:::\n:::\n\n\n::: {#782e5edc .cell execution_count=61}\n``` {.python .cell-code}\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9466250593260561\n```\n:::\n:::\n\n\nEven after cross validation, the precision is about 24% and the accuracy is 95% approximately. There is no significant improvement of the metrics of the model. \n\n## Regularizing and Shrinking the LDA\n\n::: {#b7b26579 .cell execution_count=62}\n``` {.python .cell-code}\ndf_pre['stroke'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```\nstroke\n0    4700\n1     209\nName: count, dtype: int64\n```\n:::\n:::\n\n\nAs observed by the value count of the dependent variable the data is imbalanced as the quantity of 1’s is approx 4% of the total dependent variable. So, it needs to be balanced for the learner to be a good predictor.\n\n### Balancing the Dependent Variable \n\nThere are two ways by which the data can be synthesized: one by oversampling and the second, by undersampling. In this scenario, oversampling is better which will synthesize the lesser category linear interpolation.\n\n::: {#176011f8 .cell execution_count=63}\n``` {.python .cell-code}\noversample = SMOTE()\nX_smote, y_smote = oversample.fit_resample(X, y)\nXs_train, Xs_test, ys_train, ys_test = train_test_split(X_smote, y_smote, test_size=0.30, random_state=42)\n```\n:::\n\n\nThe imbalance is mitigated by using the `Synthetic Minority Oversampling Technique (SMOTE)` but this will not help much we also need to regularize the leaner by using the `GridSearchCV` which will find the best parameters for the learner and add a penalty to the solver which will shrink the eigenvalue i.e regularization.\n\n::: {#89c8b4eb .cell execution_count=64}\n``` {.python .cell-code}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\ngrid = dict()\ngrid['solver'] = ['eigen','lsqr']\ngrid['shrinkage'] = ['auto',0.2,1,0.3,0.5]\nsearch = GridSearchCV(LDA, grid, scoring='precision', cv=cv, n_jobs=-1)\nresults = search.fit(Xs_train, ys_train)\nprint('Precision: %.3f' % results.best_score_)\nprint('Configuration:',results.best_params_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision: 0.879\nConfiguration: {'shrinkage': 'auto', 'solver': 'eigen'}\n```\n:::\n:::\n\n\nThe precision score jumped right from 35% to 87% with the help of regularization and shrinkage of the learner and the best solver for the Linear Discriminant Analysis is `eigen` and the shrinkage method is `auto` which uses the Ledoit-Wolf lemma for finding the shrinkage penalty. \n\n## Building the Regularized Discriminant Analysis (RDA)\n\n::: {#66ee6ee6 .cell execution_count=65}\n``` {.python .cell-code}\n# Build the RDA\nLDA_final=LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')\nLDA_final.fit_transform(Xs_train,ys_train)\nXs_test['predictions']=LDA_final.predict(Xs_test)\nConfusionMatrixDisplay.from_predictions(ys_test, Xs_test['predictions'])\nplt.show()\n \ntn, fp, fn, tp = confusion_matrix(list(ys_test), list(Xs_test['predictions']), labels=[0, 1]).ravel()\n \nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\n```\n\n::: {.cell-output .cell-output-display}\n![](chap05_discriminant_files/figure-html/cell-66-output-1.png){width=771 height=657}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue Positive : 1259\nTrue Negative : 1235\nFalse Positive : 172\nFalse Negative : 154\n```\n:::\n:::\n\n\n::: {#3d33dc78 .cell execution_count=66}\n``` {.python .cell-code}\nprint(\"Precision score\",np.round(precision_score(ys_test,Xs_test['predictions']),3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision score 0.88\n```\n:::\n:::\n\n\n::: {#d70503fd .cell execution_count=67}\n``` {.python .cell-code}\nprint(\"Accuracy score\",np.round(accuracy_score(ys_test,Xs_test['predictions']),3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy score 0.884\n```\n:::\n:::\n\n\n## Conclusion \n\n",
    "supporting": [
      "chap05_discriminant_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}