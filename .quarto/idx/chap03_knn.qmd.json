{"title":"<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm","markdown":{"yaml":{"title":"<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm","format":{"html":{"toc":true,"toc-depth":5,"number-sections":true,"mainfont":"emoji"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nThe data set for this lecture practice is available in this [link](https://saluki-my.sharepoint.com/:f:/g/personal/mdshariful_islam_siu_edu/EuCMsHGrmL5FudfCysZh3KcBnkopvKuro6u8vIXFqk7PKg?e=PHP49d).\n\n\n# Importing Necessary `Python` Packages\n\nPlease note that before you import `Python` packages here, you need to install them in `Terminal` by running the following code - `pip install numpy` for `numpy` package for example.\n\n```{python}\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization purposes\nimport seaborn as sns # for data visualization\nimport sklearn # for Machine Learning\n```\n\n# Checking the Working Directory\n\n```{python}\n#| eval: false\nimport os\nos.getcwd() # Checking current working directory\n```\n\n# Importing Dataset\n\n```{python}\npd.__version__ # 2.2.3\nnp.__version__ # 2.2.1\n\ndf = pd.read_csv(\"DATA/breast-cancer-wisconsin.txt\", header=None)\n```\n\n# Metadata\n\n```{python}\ndf.shape\ndf.info()\ndf.head()\n```\n\n# Some Preprocessing on the Dataset\n\n## Assigning Column Names\n\n```{python}\ncol_names = ['Id', 'Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape', 'Marginal_Adhesion','Single_Epithelial_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class']\n\ndf.columns = col_names\n\ndf.columns\n\ndf.info()\n```\n\n```{python}\ndf.head()\n```\n\n## Dropping Redundant Columns\n\n```{python}\ndf.drop('Id', axis=1, inplace=True)\ndf.info()\ndf.dtypes\n```\n\n```{python}\ndf['Class'].value_counts()\n```\n\n## Changing Data Types of Variables\n\n```{python}\ndf['Bare_Nuclei'] = pd.to_numeric(df['Bare_Nuclei'], errors='coerce')\ndf.dtypes\n```\n\n## Checking Missing Observations in the Dataset\n\n```{python}\ndf.isnull().sum() # Checking missing values in variables\ndf.isna().sum() # Checking missing values in the dataframe \n```\n\n```{python}\ndf['Bare_Nuclei'].value_counts()\n```\n\n```{python}\ndf['Bare_Nuclei'].unique()\n```\n\n\n# Summary Statistics\n\n```{python}\nround(df.describe(),2).transpose()\n```\n\n# Data Visualization\n\n```{python}\nplt.rcParams['figure.figsize']=(30,25)\n\ndf.plot(kind='hist', bins=10, subplots=True, layout=(5,2), sharex=False, sharey=False)\n\nplt.show()\n```\n\n```{python}\ndf.info()\n```\n\n\n\n\n## Multivariate Plots\n\n```{python}\ncorrelation = df.corr()\ncorrelation['Class'].sort_values(ascending=False)\n```\n\n*Interpretation*: The correlation coefficient ranges from -1 to +1.\n\nWhen it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is a strong positive correlation between `Class` and `Bare_Nuclei`, `Class` and `Uniformity_Cell_Shape`, `Class` and `Uniformity_Cell_Size`.\n\nWhen it is close to -1, it means that there is a strong negative correlation. When it is close to 0, it means that there is no correlation.\n\nWe can see that all the variables are positively correlated with `Class` variable. Some variables are strongly positive correlated while some variables are negatively correlated.\n\n## Discover Pattern and Relationship\n\nAn important step in EDA is to discover patterns and relationships between variables in the dataset. I will use the `seaborn` `heatmap` to explore the patterns and relationships in the dataset.\n\n```{python}\nplt.figure(figsize=(10,8))\nplt.title('Correlation of Attributes with Class variable')\na = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white')\na.set_xticklabels(a.get_xticklabels(), rotation=90)\na.set_yticklabels(a.get_yticklabels(), rotation=30)           \nplt.show()\n```\n\n\n# Declare Feature Vector and Target Variable\n\n```{python}\n\nX = df.drop(['Class'], axis = 1)\ny = df[\"Class\"]\n```\n\n# Split Data into Separate Training and Test Set\n\n```{python}\n# Split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n```\n\n```{python}\n# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape\n```\n\n# Feature Engineering\n\nFeature Engineering is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables.\n\n```{python}\n# check data types in X_train\n\nX_train.dtypes\n```\n\n```{python}\n# check missing values in numerical variables in X_train\n\nX_train.isnull().sum()\n```\n\n## Engineering Missing Values in Variables\n\n```{python}\n# check missing values in numerical variables in X_test\n\nX_test.isnull().sum()\n```\n\n```{python}\n# print percentage of missing values in the numerical variables in training set\n\nfor col in X_train.columns:\n    if X_train[col].isnull().mean()>0:\n        print(col, round(X_train[col].isnull().mean(),4))\n```\n\n**Assumption** I assume that the data are missing completely at random (MCAR). There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation. When there are outliers in the dataset, we should use median imputation. So, I will use median imputation because median imputation is robust to outliers.\n\nI will impute missing values with the appropriate statistical measures of the data, in this case median. Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting.\n\n```{python}\n#| warning: false\n# impute missing values in X_train and X_test with respective column median in X_train\n\nfor df in [X_train, X_test]:\n    for col in X_train.columns:\n        col_median=X_train[col].median()\n        df[col].fillna(col_median, inplace=True)  \n```\n\n```{python}\n# check again missing values in numerical variables in X_train\n\nX_train.isnull().sum()\n```\n\n```{python}\n# check missing values in numerical variables in X_test\n\nX_test.isnull().sum()\n```\n\nWe now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called feature scaling. I will do it as follows.\n\n\n## Feature Selection \n\nFeature selection involves identifying the features that have the greatest explanatory power to predict the target variables. Therere are many techniques that can be used for feature selection. When there are many variables, using feature selection is very much important. Otherwise, noises might be introduced in the model.\n\n```{python}\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\n```\n\n\n```{python}\nchi_feature = SelectKBest(chi2, k = 4).fit(X_train, y_train)\nprint ('Score: ', chi_feature.scores_)\nprint ('Features: ', X_train.columns)\n```\n\n\n```{python}\nanova_feature = SelectKBest (f_classif, k = 4).fit(X_train, y_train)\nprint ('Scores: ', anova_feature.scores_)\nprint ('Features: ', X_train.columns)\n```\n\n\n## Feature Scaling\n\n```{python}\ncols = X_train.columns\n```\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n\n```\n\n```{python}\nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])\nX_train.head()\n```\n\n# Fit `K` Neighbours Classifier to the Training Set\n\n```{python}\n# import KNeighbors ClaSSifier from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# instantiate the model\nknn = KNeighborsClassifier(n_neighbors=3)\n\n\n# fit the model to the training set\nknn.fit(X_train, y_train)\n```\n\n# Predict Test-set Results\n\n```{python}\ny_pred = knn.predict(X_test)\n\ny_pred\n```\n\n**predict_proba method** predict_proba method gives the probabilities for the target variable(2 and 4) in this case, in array form.\n\n`2 is for probability of benign cancer` and `4 is for probability of malignant cancer`.\n\n```{python}\nknn.predict_proba(X_test)\n```\n\n\n```{python}\n# probability of getting output as 2 - benign cancer\n\nknn.predict_proba(X_test)[:,0]\n```\n\n```{python}\n# probability of getting output as 4 - malignant cancer\n\nknn.predict_proba(X_test)[:,1]\n```\n\n# Check Accuracy Score\n\n```{python}\nfrom sklearn.metrics import accuracy_score\n\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n```\n\nHere, y_test are the true class labels and y_pred are the predicted class labels in the test-set.\n\n## Compare the Train-set and Test-set Accuracy\n\nNow, I will compare the train-set and test-set accuracy to check for overfitting.\n\n```{python}\ny_pred_train = knn.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n```\n\n# Check for Overfitting and Underfitting\n\n```{python}\n# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(knn.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(knn.score(X_test, y_test)))\n```\n\nThe `training-set` accuracy score is `0.9821` while the `test-set` accuracy to be `0.9714`. These two values are quite comparable. So, there is no question of overfitting.\n\n## Compare Model Accuracy with Null Accuracy\n\nSo, the model accuracy is 0.9714. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\n\nSo, we should first check the class distribution in the test set.\n\n```{python}\n# check class distribution in test set\n\ny_test.value_counts()\n```\n\nWe can see that the occurences of most frequent class is 85. So, we can calculate null accuracy by dividing 85 by total number of occurences.\n\n```{python}\n# check null accuracy score\n\nnull_accuracy = (85/(85+55))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))\n\n```\n\nWe can see that our model accuracy score is 0.9714 but null accuracy score is 0.6071. So, we can conclude that our K Nearest Neighbors model is doing a very good job in predicting the class labels.\n\n\n\n\n# Rebuild `KNN` Classification Model Using Different Values of `K`\n\n```{python}\n# instantiate the model with k=5\nknn_5 = KNeighborsClassifier(n_neighbors=5)\n\n\n# fit the model to the training set\nknn_5.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_5 = knn_5.predict(X_test)\n\n\nprint('Model accuracy score with k=5 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_5)))\n```\n\n## Rebuild `KNN` Classification Model Using `K=6`\n\n```{python}\n# instantiate the model with k=6\nknn_6 = KNeighborsClassifier(n_neighbors=6)\n\n\n# fit the model to the training set\nknn_6.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_6 = knn_6.predict(X_test)\n\n\nprint('Model accuracy score with k=6 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_6)))\n```\n\n## Rebuild `KNN` Classification Model Using `K=7`\n\n```{python}\n# instantiate the model with k=7\nknn_7 = KNeighborsClassifier(n_neighbors=7)\n\n\n# fit the model to the training set\nknn_7.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_7 = knn_7.predict(X_test)\n\n\nprint('Model accuracy score with k=7 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_7)))\n```\n\n## Rebuild `KNN` Classification Model Using `K=8`\n\n```{python}\n# instantiate the model with k=8\nknn_8 = KNeighborsClassifier(n_neighbors=8)\n\n\n# fit the model to the training set\nknn_8.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_8 = knn_8.predict(X_test)\n\n\nprint('Model accuracy score with k=8 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_8)))\n```\n\n## Rebuild `KNN` Classification Model Using `K=9`\n\n```{python}\n# instantiate the model with k=9\nknn_9 = KNeighborsClassifier(n_neighbors=9)\n\n\n# fit the model to the training set\nknn_9.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_9 = knn_9.predict(X_test)\n\n\nprint('Model accuracy score with k=9 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_9)))\n```\n\n**Interpretation:** Our original model accuracy score with k=3 is 0.9714. Now, we can see that we get same accuracy score of 0.9714 with k=5. But, if we increase the value of k further, this would result in enhanced accuracy.\n\nWith k=6,7,8 we get accuracy score of 0.9786. So, it results in performance improvement.\n\nIf we increase k to 9, then accuracy decreases again to 0.9714.\n\nNow, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\n\nWe have another tool called Confusion matrix that comes to our rescue.\n\n# Automating the Calculation of the Value of K\n\n```{python}\nfrom sklearn import metrics\n```\n\n\n```{python}\nmean_acc = np.zeros(20)\nfor i in range(1,21):\n    #Train Model and Predict  \n    knn = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n    yhat= knn.predict(X_test)\n    mean_acc[i-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc\n```\n\n```{python}\nloc = np.arange(1,21,step=1.0)\nplt.figure(figsize = (10, 6))\nplt.plot(range(1,21), mean_acc)\nplt.xticks(loc)\nplt.xlabel('Number of Neighbors ')\nplt.ylabel('Accuracy')\nplt.show()\n```\n\n\n# Hyperparameter Tuning\n\nA hyperparameter is a parameter of the model that is set before the start of learning process. Different machine learning models have different hyperparameters. You can find out more about the different hyperparameters of k-NN [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n\nWe will use the **Exhaustive Grid Search** technique for hyperparameter optimization. An exhaustive grid search takes in as many hyperparameters as you would like, and tries every single possible combination of the hyperparameters as well as as many cross-validations as you would like it to perform. An exhaustive grid search is a good way to determine the best hyperparameter values to use, but it can quickly become time consuming with every additional parameter value and cross-validation that you add.\n\n```{python}\nfrom sklearn.model_selection import GridSearchCV\n```\n\nWe will use three hyperparamters- n-neighbors, weights and metric.\n\n 1. `n_neighbors`: Decide the best k based on the values we have computed earlier.\n 2. `weights`: Check whether adding weights to the data points is beneficial to the model or not. 'uniform' assigns no weight, while 'distance' weighs points by the inverse of their distances meaning nearer points will have more weight than the farther points.\n 3. `metric`: The distance metric to be used will calculating the similarity.\n\n\n```{python}\ngrid_params = { 'n_neighbors' : [3,4,5,6,7,8,9,10,11,12],\n               'weights' : ['uniform','distance'],\n               'metric' : ['minkowski','euclidean','manhattan']}\n```\n\n```{python}\ngs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n```\n\nSince we have provided the class validation score as 3( cv= 3), Grid Search will evaluate the model 10 x 2 x 3 x 3 = 180 times with different hyperparameters.\n\n```{python}\n# fit the model on our train set\ng_res = gs.fit(X_train, y_train)\n```\n\n\n```{python}\n# find the best score\ng_res.best_score_\n```\n\n\n```{python}\n# get the hyperparameters with the best score\ng_res.best_params_\n```\n\n\n```{python}\n# use the best hyperparameters\nknn = KNeighborsClassifier(n_neighbors = 5, weights = 'distance', \\\nalgorithm = 'brute',metric = 'manhattan')\nknn.fit(X_train, y_train)\n```\n\n\n```{python}\n# get a prediction\ny_hat = knn.predict(X_train)\ny_knn = knn.predict(X_test)\n```\n\n\n```{python}\nfrom sklearn import metrics\n```\n\n\n```{python}\nprint('Training set accuracy: ', metrics.accuracy_score(y_train, y_hat))\nprint('Test set accuracy: ', metrics.accuracy_score(y_test, y_knn))\n```\n\n# Confusion Matrix\n\nA confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\nFour types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n`True Positives (TP)` -- True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n`True Negatives (TN)` -- True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n`False Positives (FP)` -- False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called `Type I error`.\n\n`False Negatives (FN)` -- False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called `Type II error`.\n\nThese four outcomes are summarized in a confusion matrix given below.\n\n```{python}\n# Print the Confusion Matrix with k =3 and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n```\n\nThe confusion matrix shows `83 + 53 = 136` correct predictions & `2 + 2 = 4` incorrect predictions.\n\nIn this case, we have\n\n-   `True Positives` (Actual Positive:1 and Predict Positive:1) - 83\n-   `True Negatives` (Actual Negative:0 and Predict Negative:0) - 53\n-   `False Positives` (Actual Negative:0 but Predict Positive:1) - 2 (`Type I` error)\n-   `False Negatives` (Actual Positive:1 but Predict Negative:0) - 2 (`Type II` error)\n\n```{python}\n# Print the Confusion Matrix with k =7 and slice it into four pieces\n\ncm_7 = confusion_matrix(y_test, y_pred_7)\n\nprint('Confusion matrix\\n\\n', cm_7)\n\nprint('\\nTrue Positives(TP) = ', cm_7[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm_7[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm_7[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm_7[1,0])\n```\n\nThe above confusion matrix shows `83 + 54 = 137` correct predictions and `2 + 1 = 4` incorrect predictions.\n\nIn this case, we have\n\n-   `True Positives` (Actual Positive:1 and Predict Positive:1) - 83\n-   `True Negatives` (Actual Negative:0 and Predict Negative:0) - 54\n-   `False Positives` (Actual Negative:0 but Predict Positive:1) - 2 (`Type I` error)\n-   `False Negatives` (Actual Positive:1 but Predict Negative:0) - 1 (`Type II` error)\n\n**Comment** So, `KNN` Classification model with `k=7` shows more accurate predictions and less number of errors than k=3 model. Hence, we got performance improvement with `k=7`.\n\n```{python}\n# visualize confusion matrix with seaborn heatmap\n\nplt.figure(figsize=(6,4))\n\ncm_matrix = pd.DataFrame(data=cm_7, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n```\n\n# Classification Matrices\n\n## Classification Report\n\n`Classification report` is another way to evaluate the classification model performance. It displays the `precision`, `recall`, `f1` and `support scores` for the model. I have described these terms in later.\n\nWe can print a classification report as follows:-\n\n```{python}\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_7))\n```\n\n## Classification Accuracy\n\n```{python}\nTP = cm_7[0,0]\nTN = cm_7[1,1]\nFP = cm_7[0,1]\nFN = cm_7[1,0]\n```\n\n```{python}\n# print classification accuracy\n\nclassification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n```\n\n## Classification Error\n\n```{python}\n# print classification error\n\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))\n```\n\n## Precision\n\n`Precision` can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives `(TP + FP)`. **Precision is a metric that tells us about the quality of positive predictions.** So, `Precision` identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class. Precision is a useful metric in cases where False Positive is a higher concern than False Negatives. Precision is important in music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.tt\n\nMathematically, precision can be defined as the ratio of `TP` to `(TP + FP)`.\n\n```{python}\n# print precision score\n\nprecision = TP / float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))\n```\n\n## Recall\n\n`Recall` can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives `(TP + FN)`. **Recall tells us about how well the model identifies true positives.** Recall is also called `Sensitivity`. `Recall` identifies the proportion of correctly predicted actual positives. Mathematically, recall can be given as the ratio of `TP` to `(TP + FN)`. Recall is a useful metric in cases where False Negative triumphs over False Positive.\nRecall is important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n\n```{python}\nrecall = TP / float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))\n```\n\n## Precision vs Recall\n\nData scientists optimize their model to have higher precision or recall depending on the circumstances. A model with higher recall than precision often makes more positive predictions. A model like this comes with higher false positives and low false negatives. In scenarios like disease prediction, models should always be optimized for recall. **False positives are better than false negatives in the healthcare industry.**\n\nOn the other hand, a model with higher precision will have fewer false positives and more false negatives. If you were to build a bot detection machine learning model for an online store, you may want to optimize for higher precision, since banning legitimate users from the website will lead to a decline in sales.\n\n## f1-score\n\nIn practice, when we try to increase the precision of our model, the recall goes down, and vice-versa. The F1-score captures both the trends in a single value:\n\n            f1-score = 2/((1/Recall) + (1/Precision))\n\n`f1-score` is the weighted harmonic mean of precision and recall, and so it gives a combined idea about these two metrics. **It is maximum when Precision is equal to Recall.** The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy.\n\n## Support\n\n`Support` is the actual number of occurrences of the class in our dataset.\n\n\n## True Positive Rate\n\n`True Positive Rate` is synonymous with **`Recall`.**\n\n```{python}\ntrue_positive_rate = TP / float(TP + FN)\n\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n```\n\n## False Positive Rate\n\n```{python}\nfalse_positive_rate = FP / float(FP + TN)\n\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n```\n\n## Specificity (True Negative Rate)\n\n```{python}\nspecificity = TN / (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))\n```\n\n\n\n## Adjusting the Classification Threshold Level\n\n```{python}\n# print the first 10 predicted probabilities of two classes- 2 and 4\n\ny_pred_prob = knn.predict_proba(X_test)[0:10]\n\ny_pred_prob\n```\n\n**Observations** In each row, the numbers sum to 1. There are 2 columns which correspond to 2 classes - 2 and 4.\n\n-   `Class 2` - predicted probability that there is `benign cancer`.\n\n-   `Class 4` - predicted probability that there is `malignant cancer`.\n\n**Importance of predicted probabilities**\n\nWe can rank the observations by probability of benign or malignant cancer.\n\n-   `predict_proba` process\n\n           * Predicts the probabilities\n\n           * Choose the class with the highest probability\n\n-   Classification threshold level\n\n           * There is a classification threshold level of 0.5.\n\n           * Class 4 - probability of malignant cancer is predicted if probability > 0.5.\n\n           * Class 2 - probability of benign cancer is predicted if probability < 0.5.\n\n```{python}\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, \\\ncolumns=['Prob of - benign cancer (2)', 'Prob of - malignant cancer (4)'])\n\ny_pred_prob_df\n```\n\n```{python}\n# print the first 10 predicted probabilities for class 4 - Probability of malignant cancer\n\nknn.predict_proba(X_test)[0:10, 1]\n```\n\n```{python}\n# store the predicted probabilities for class 4 - Probability of malignant cancer\n\ny_pred_1 = knn.predict_proba(X_test)[:, 1]\n```\n\n```{python}\n# plot histogram of predicted probabilities\n\n# adjust figure size\nplt.figure(figsize=(6,4))\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n# plot histogram with 10 bins\nplt.hist(y_pred_1, bins = 10)\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities of malignant cancer')\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n# set the title\nplt.xlabel('Predicted probabilities of malignant cancer')\nplt.ylabel('Frequency')\n```\n\n**Observations**\n\n-   We can see that the above histogram is positively skewed.\n-   The first column tell us that there are approximately 80 observations with 0 \\* \\* \\* \\* probability of malignant cancer.\n-   There are few observations with probability \\> 0.5.\n-   So, these few observations predict that there will be malignant cancer.\n\n**Comments**\n\n-   In binary problems, the threshold of 0.5 is used by default to convert predicted probabilities into class predictions.\n-   Threshold can be adjusted to increase sensitivity or specificity.\n-   **Sensitivity and specificity have an inverse relationship.** Increasing one would always decrease the other and vice versa.\n-   **Adjusting the threshold level should be one of the last step you do in the model-building process.**\n\n# ROC (Receiver Operating Characteristics) - AUC (Area Under ROC Curve) Curve \n\n## ROC Curve\n\nAnother tool to measure the classification model performance visually is `ROC Curve`. `ROC Curve` stands for `Receiver Operating Characteristic` Curve. An `ROC Curve` is a plot which shows the performance of a classification model at various classification **threshold levels.**\n\nThe `ROC Curve` plots the `True Positive Rate (TPR)` against the `False Positive Rate (FPR)` at various threshold levels. `True Positive Rate (TPR)` is also called `Recall`. It is defined as the ratio of `TP to (TP + FN)`. `False Positive Rate (FPR)` is defined as the ratio of `FP to (FP + TN)`.\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positive. It will increase both True Positives (TP) and False Positives (FP).\n\n```{python}\n# plot ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_1, pos_label=4)\n\nplt.figure(figsize=(6,4))\n\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\n\nplt.title('ROC curve for Breast Cancer kNN classifier')\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\n\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()\n```\n\n`ROC curve` help us to choose a threshold level that balances `sensitivity` and `specificity` for a particular context.\n\n## ROC AUC\n\n`ROC AUC` stands for `Receiver Operating Characteristic - Area Under Curve`. It is a technique to compare classifier performance. In this technique, we measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. So, `ROC AUC` is the percentage of the ROC plot that is underneath the curve.\n\n```{python}\n# compute ROC AUC\n\nfrom sklearn.metrics import roc_auc_score\n\nROC_AUC = roc_auc_score(y_test, y_pred_1)\n\nprint('ROC AUC : {:.4f}'.format(ROC_AUC))\n```\n\n**Interpretation**\n\n-   ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\n\n-   ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it is benign or malignant cancer.\n\n```{python}\nfrom sklearn.model_selection import cross_val_score\n\nCross_validated_ROC_AUC = \\\ncross_val_score(knn_7, X_train, y_train, cv=5,scoring='roc_auc').mean()\n\nprint('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))\n```\n\n**Interpretation**\n\nOur Cross Validated ROC AUC is very close to 1. So, we can conclude that, the KNN classifier is indeed a very good model.\n\n# K-fold Cross Validation\n\nIn this section, I will apply k-fold Cross Validation technique to improve the model performance. Cross-validation is a statistical method of evaluating generalization performance It is more stable and thorough than using a train-test split to evaluate model performance.\n\n```{python}\n# Applying 10-Fold Cross Validation\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(knn_7, X_train, y_train, cv = 10, scoring='accuracy')\n\nprint('Cross-validation scores:{}'.format(scores))\n```\n\nWe can summarize the cross-validation accuracy by calculating its mean.\n\n```{python}\n# compute Average cross-validation score\n\nprint('Average cross-validation score: {:.4f}'.format(scores.mean()))\n```\n\n**Interpretation**\n\n-   Using the mean cross-validation, we can conclude that we expect the model to be around 96.46 % accurate on average.\n\n-   If we look at all the 10 scores produced by the 10-fold cross-validation, we can also conclude that there is a relatively high variance in the accuracy between folds, ranging from 100% accuracy to 87.72% accuracy. So, we can conclude that the model is very dependent on the particular folds used for training, but it also be the consequence of the small size of the dataset.\n\n-   We can see that 10-fold cross-validation accuracy does not result in performance improvement for this model.\n\n# Results and Conclusion\n\n1.  In this project, I build a kNN classifier model to classify the patients suffering from breast cancer. The model yields very good performance as indicated by the model accuracy which was found to be 0.9786 with k=7.\n\n2.  With k=3, the training-set accuracy score is 0.9821 while the test-set accuracy to be 0.9714. These two values are quite comparable. So, there is no question of overfitting.\n\n3.  I have compared the model accuracy score which is 0.9714 with null accuracy score which is 0.6071. So, we can conclude that our K Nearest Neighbors model is doing a very good job in predicting the class labels.\n\n4.  Our original model accuracy score with k=3 is 0.9714. Now, we can see that we get same accuracy score of 0.9714 with k=5. But, if we increase the value of k further, this would result in enhanced accuracy. With k=6,7,8 we get accuracy score of 0.9786. So, it results in performance improvement. If we increase k to 9, then accuracy decreases again to 0.9714. So, we can conclude that our optimal value of k is 7.\n\n5.  kNN Classification model with k=7 shows more accurate predictions and less number of errors than k=3 model. Hence, we got performance improvement with k=7.\n\n6.  ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it is benign or malignant cancer.\n\n7.  Using the mean cross-validation, we can conclude that we expect the model to be around 96.46 % accurate on average.\n\n8.  If we look at all the 10 scores produced by the 10-fold cross-validation, we can also conclude that there is a relatively high variance in the accuracy between folds, ranging from 100% accuracy to 87.72% accuracy. So, we can conclude that the model is very dependent on the particular folds used for training, but it also be the consequence of the small size of the dataset.\n\n# Reference\n\nThe code for this analysis was collected from the link - <https://www.kaggle.com/code/prashant111/knn-classifier-tutorial/notebook#11.-Feature-Engineering->\n\n","srcMarkdownNoYaml":"\n\n## Introduction \nThe data set for this lecture practice is available in this [link](https://saluki-my.sharepoint.com/:f:/g/personal/mdshariful_islam_siu_edu/EuCMsHGrmL5FudfCysZh3KcBnkopvKuro6u8vIXFqk7PKg?e=PHP49d).\n\n\n# Importing Necessary `Python` Packages\n\nPlease note that before you import `Python` packages here, you need to install them in `Terminal` by running the following code - `pip install numpy` for `numpy` package for example.\n\n```{python}\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization purposes\nimport seaborn as sns # for data visualization\nimport sklearn # for Machine Learning\n```\n\n# Checking the Working Directory\n\n```{python}\n#| eval: false\nimport os\nos.getcwd() # Checking current working directory\n```\n\n# Importing Dataset\n\n```{python}\npd.__version__ # 2.2.3\nnp.__version__ # 2.2.1\n\ndf = pd.read_csv(\"DATA/breast-cancer-wisconsin.txt\", header=None)\n```\n\n# Metadata\n\n```{python}\ndf.shape\ndf.info()\ndf.head()\n```\n\n# Some Preprocessing on the Dataset\n\n## Assigning Column Names\n\n```{python}\ncol_names = ['Id', 'Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape', 'Marginal_Adhesion','Single_Epithelial_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class']\n\ndf.columns = col_names\n\ndf.columns\n\ndf.info()\n```\n\n```{python}\ndf.head()\n```\n\n## Dropping Redundant Columns\n\n```{python}\ndf.drop('Id', axis=1, inplace=True)\ndf.info()\ndf.dtypes\n```\n\n```{python}\ndf['Class'].value_counts()\n```\n\n## Changing Data Types of Variables\n\n```{python}\ndf['Bare_Nuclei'] = pd.to_numeric(df['Bare_Nuclei'], errors='coerce')\ndf.dtypes\n```\n\n## Checking Missing Observations in the Dataset\n\n```{python}\ndf.isnull().sum() # Checking missing values in variables\ndf.isna().sum() # Checking missing values in the dataframe \n```\n\n```{python}\ndf['Bare_Nuclei'].value_counts()\n```\n\n```{python}\ndf['Bare_Nuclei'].unique()\n```\n\n\n# Summary Statistics\n\n```{python}\nround(df.describe(),2).transpose()\n```\n\n# Data Visualization\n\n```{python}\nplt.rcParams['figure.figsize']=(30,25)\n\ndf.plot(kind='hist', bins=10, subplots=True, layout=(5,2), sharex=False, sharey=False)\n\nplt.show()\n```\n\n```{python}\ndf.info()\n```\n\n\n\n\n## Multivariate Plots\n\n```{python}\ncorrelation = df.corr()\ncorrelation['Class'].sort_values(ascending=False)\n```\n\n*Interpretation*: The correlation coefficient ranges from -1 to +1.\n\nWhen it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is a strong positive correlation between `Class` and `Bare_Nuclei`, `Class` and `Uniformity_Cell_Shape`, `Class` and `Uniformity_Cell_Size`.\n\nWhen it is close to -1, it means that there is a strong negative correlation. When it is close to 0, it means that there is no correlation.\n\nWe can see that all the variables are positively correlated with `Class` variable. Some variables are strongly positive correlated while some variables are negatively correlated.\n\n## Discover Pattern and Relationship\n\nAn important step in EDA is to discover patterns and relationships between variables in the dataset. I will use the `seaborn` `heatmap` to explore the patterns and relationships in the dataset.\n\n```{python}\nplt.figure(figsize=(10,8))\nplt.title('Correlation of Attributes with Class variable')\na = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white')\na.set_xticklabels(a.get_xticklabels(), rotation=90)\na.set_yticklabels(a.get_yticklabels(), rotation=30)           \nplt.show()\n```\n\n\n# Declare Feature Vector and Target Variable\n\n```{python}\n\nX = df.drop(['Class'], axis = 1)\ny = df[\"Class\"]\n```\n\n# Split Data into Separate Training and Test Set\n\n```{python}\n# Split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n```\n\n```{python}\n# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape\n```\n\n# Feature Engineering\n\nFeature Engineering is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables.\n\n```{python}\n# check data types in X_train\n\nX_train.dtypes\n```\n\n```{python}\n# check missing values in numerical variables in X_train\n\nX_train.isnull().sum()\n```\n\n## Engineering Missing Values in Variables\n\n```{python}\n# check missing values in numerical variables in X_test\n\nX_test.isnull().sum()\n```\n\n```{python}\n# print percentage of missing values in the numerical variables in training set\n\nfor col in X_train.columns:\n    if X_train[col].isnull().mean()>0:\n        print(col, round(X_train[col].isnull().mean(),4))\n```\n\n**Assumption** I assume that the data are missing completely at random (MCAR). There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation. When there are outliers in the dataset, we should use median imputation. So, I will use median imputation because median imputation is robust to outliers.\n\nI will impute missing values with the appropriate statistical measures of the data, in this case median. Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting.\n\n```{python}\n#| warning: false\n# impute missing values in X_train and X_test with respective column median in X_train\n\nfor df in [X_train, X_test]:\n    for col in X_train.columns:\n        col_median=X_train[col].median()\n        df[col].fillna(col_median, inplace=True)  \n```\n\n```{python}\n# check again missing values in numerical variables in X_train\n\nX_train.isnull().sum()\n```\n\n```{python}\n# check missing values in numerical variables in X_test\n\nX_test.isnull().sum()\n```\n\nWe now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called feature scaling. I will do it as follows.\n\n\n## Feature Selection \n\nFeature selection involves identifying the features that have the greatest explanatory power to predict the target variables. Therere are many techniques that can be used for feature selection. When there are many variables, using feature selection is very much important. Otherwise, noises might be introduced in the model.\n\n```{python}\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\n```\n\n\n```{python}\nchi_feature = SelectKBest(chi2, k = 4).fit(X_train, y_train)\nprint ('Score: ', chi_feature.scores_)\nprint ('Features: ', X_train.columns)\n```\n\n\n```{python}\nanova_feature = SelectKBest (f_classif, k = 4).fit(X_train, y_train)\nprint ('Scores: ', anova_feature.scores_)\nprint ('Features: ', X_train.columns)\n```\n\n\n## Feature Scaling\n\n```{python}\ncols = X_train.columns\n```\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n\n```\n\n```{python}\nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])\nX_train.head()\n```\n\n# Fit `K` Neighbours Classifier to the Training Set\n\n```{python}\n# import KNeighbors ClaSSifier from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# instantiate the model\nknn = KNeighborsClassifier(n_neighbors=3)\n\n\n# fit the model to the training set\nknn.fit(X_train, y_train)\n```\n\n# Predict Test-set Results\n\n```{python}\ny_pred = knn.predict(X_test)\n\ny_pred\n```\n\n**predict_proba method** predict_proba method gives the probabilities for the target variable(2 and 4) in this case, in array form.\n\n`2 is for probability of benign cancer` and `4 is for probability of malignant cancer`.\n\n```{python}\nknn.predict_proba(X_test)\n```\n\n\n```{python}\n# probability of getting output as 2 - benign cancer\n\nknn.predict_proba(X_test)[:,0]\n```\n\n```{python}\n# probability of getting output as 4 - malignant cancer\n\nknn.predict_proba(X_test)[:,1]\n```\n\n# Check Accuracy Score\n\n```{python}\nfrom sklearn.metrics import accuracy_score\n\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n```\n\nHere, y_test are the true class labels and y_pred are the predicted class labels in the test-set.\n\n## Compare the Train-set and Test-set Accuracy\n\nNow, I will compare the train-set and test-set accuracy to check for overfitting.\n\n```{python}\ny_pred_train = knn.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n```\n\n# Check for Overfitting and Underfitting\n\n```{python}\n# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(knn.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(knn.score(X_test, y_test)))\n```\n\nThe `training-set` accuracy score is `0.9821` while the `test-set` accuracy to be `0.9714`. These two values are quite comparable. So, there is no question of overfitting.\n\n## Compare Model Accuracy with Null Accuracy\n\nSo, the model accuracy is 0.9714. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\n\nSo, we should first check the class distribution in the test set.\n\n```{python}\n# check class distribution in test set\n\ny_test.value_counts()\n```\n\nWe can see that the occurences of most frequent class is 85. So, we can calculate null accuracy by dividing 85 by total number of occurences.\n\n```{python}\n# check null accuracy score\n\nnull_accuracy = (85/(85+55))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))\n\n```\n\nWe can see that our model accuracy score is 0.9714 but null accuracy score is 0.6071. So, we can conclude that our K Nearest Neighbors model is doing a very good job in predicting the class labels.\n\n\n\n\n# Rebuild `KNN` Classification Model Using Different Values of `K`\n\n```{python}\n# instantiate the model with k=5\nknn_5 = KNeighborsClassifier(n_neighbors=5)\n\n\n# fit the model to the training set\nknn_5.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_5 = knn_5.predict(X_test)\n\n\nprint('Model accuracy score with k=5 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_5)))\n```\n\n## Rebuild `KNN` Classification Model Using `K=6`\n\n```{python}\n# instantiate the model with k=6\nknn_6 = KNeighborsClassifier(n_neighbors=6)\n\n\n# fit the model to the training set\nknn_6.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_6 = knn_6.predict(X_test)\n\n\nprint('Model accuracy score with k=6 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_6)))\n```\n\n## Rebuild `KNN` Classification Model Using `K=7`\n\n```{python}\n# instantiate the model with k=7\nknn_7 = KNeighborsClassifier(n_neighbors=7)\n\n\n# fit the model to the training set\nknn_7.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_7 = knn_7.predict(X_test)\n\n\nprint('Model accuracy score with k=7 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_7)))\n```\n\n## Rebuild `KNN` Classification Model Using `K=8`\n\n```{python}\n# instantiate the model with k=8\nknn_8 = KNeighborsClassifier(n_neighbors=8)\n\n\n# fit the model to the training set\nknn_8.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_8 = knn_8.predict(X_test)\n\n\nprint('Model accuracy score with k=8 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_8)))\n```\n\n## Rebuild `KNN` Classification Model Using `K=9`\n\n```{python}\n# instantiate the model with k=9\nknn_9 = KNeighborsClassifier(n_neighbors=9)\n\n\n# fit the model to the training set\nknn_9.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_9 = knn_9.predict(X_test)\n\n\nprint('Model accuracy score with k=9 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_9)))\n```\n\n**Interpretation:** Our original model accuracy score with k=3 is 0.9714. Now, we can see that we get same accuracy score of 0.9714 with k=5. But, if we increase the value of k further, this would result in enhanced accuracy.\n\nWith k=6,7,8 we get accuracy score of 0.9786. So, it results in performance improvement.\n\nIf we increase k to 9, then accuracy decreases again to 0.9714.\n\nNow, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\n\nWe have another tool called Confusion matrix that comes to our rescue.\n\n# Automating the Calculation of the Value of K\n\n```{python}\nfrom sklearn import metrics\n```\n\n\n```{python}\nmean_acc = np.zeros(20)\nfor i in range(1,21):\n    #Train Model and Predict  \n    knn = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n    yhat= knn.predict(X_test)\n    mean_acc[i-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc\n```\n\n```{python}\nloc = np.arange(1,21,step=1.0)\nplt.figure(figsize = (10, 6))\nplt.plot(range(1,21), mean_acc)\nplt.xticks(loc)\nplt.xlabel('Number of Neighbors ')\nplt.ylabel('Accuracy')\nplt.show()\n```\n\n\n# Hyperparameter Tuning\n\nA hyperparameter is a parameter of the model that is set before the start of learning process. Different machine learning models have different hyperparameters. You can find out more about the different hyperparameters of k-NN [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n\nWe will use the **Exhaustive Grid Search** technique for hyperparameter optimization. An exhaustive grid search takes in as many hyperparameters as you would like, and tries every single possible combination of the hyperparameters as well as as many cross-validations as you would like it to perform. An exhaustive grid search is a good way to determine the best hyperparameter values to use, but it can quickly become time consuming with every additional parameter value and cross-validation that you add.\n\n```{python}\nfrom sklearn.model_selection import GridSearchCV\n```\n\nWe will use three hyperparamters- n-neighbors, weights and metric.\n\n 1. `n_neighbors`: Decide the best k based on the values we have computed earlier.\n 2. `weights`: Check whether adding weights to the data points is beneficial to the model or not. 'uniform' assigns no weight, while 'distance' weighs points by the inverse of their distances meaning nearer points will have more weight than the farther points.\n 3. `metric`: The distance metric to be used will calculating the similarity.\n\n\n```{python}\ngrid_params = { 'n_neighbors' : [3,4,5,6,7,8,9,10,11,12],\n               'weights' : ['uniform','distance'],\n               'metric' : ['minkowski','euclidean','manhattan']}\n```\n\n```{python}\ngs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n```\n\nSince we have provided the class validation score as 3( cv= 3), Grid Search will evaluate the model 10 x 2 x 3 x 3 = 180 times with different hyperparameters.\n\n```{python}\n# fit the model on our train set\ng_res = gs.fit(X_train, y_train)\n```\n\n\n```{python}\n# find the best score\ng_res.best_score_\n```\n\n\n```{python}\n# get the hyperparameters with the best score\ng_res.best_params_\n```\n\n\n```{python}\n# use the best hyperparameters\nknn = KNeighborsClassifier(n_neighbors = 5, weights = 'distance', \\\nalgorithm = 'brute',metric = 'manhattan')\nknn.fit(X_train, y_train)\n```\n\n\n```{python}\n# get a prediction\ny_hat = knn.predict(X_train)\ny_knn = knn.predict(X_test)\n```\n\n\n```{python}\nfrom sklearn import metrics\n```\n\n\n```{python}\nprint('Training set accuracy: ', metrics.accuracy_score(y_train, y_hat))\nprint('Test set accuracy: ', metrics.accuracy_score(y_test, y_knn))\n```\n\n# Confusion Matrix\n\nA confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\nFour types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n`True Positives (TP)` -- True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n`True Negatives (TN)` -- True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n`False Positives (FP)` -- False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called `Type I error`.\n\n`False Negatives (FN)` -- False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called `Type II error`.\n\nThese four outcomes are summarized in a confusion matrix given below.\n\n```{python}\n# Print the Confusion Matrix with k =3 and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n```\n\nThe confusion matrix shows `83 + 53 = 136` correct predictions & `2 + 2 = 4` incorrect predictions.\n\nIn this case, we have\n\n-   `True Positives` (Actual Positive:1 and Predict Positive:1) - 83\n-   `True Negatives` (Actual Negative:0 and Predict Negative:0) - 53\n-   `False Positives` (Actual Negative:0 but Predict Positive:1) - 2 (`Type I` error)\n-   `False Negatives` (Actual Positive:1 but Predict Negative:0) - 2 (`Type II` error)\n\n```{python}\n# Print the Confusion Matrix with k =7 and slice it into four pieces\n\ncm_7 = confusion_matrix(y_test, y_pred_7)\n\nprint('Confusion matrix\\n\\n', cm_7)\n\nprint('\\nTrue Positives(TP) = ', cm_7[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm_7[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm_7[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm_7[1,0])\n```\n\nThe above confusion matrix shows `83 + 54 = 137` correct predictions and `2 + 1 = 4` incorrect predictions.\n\nIn this case, we have\n\n-   `True Positives` (Actual Positive:1 and Predict Positive:1) - 83\n-   `True Negatives` (Actual Negative:0 and Predict Negative:0) - 54\n-   `False Positives` (Actual Negative:0 but Predict Positive:1) - 2 (`Type I` error)\n-   `False Negatives` (Actual Positive:1 but Predict Negative:0) - 1 (`Type II` error)\n\n**Comment** So, `KNN` Classification model with `k=7` shows more accurate predictions and less number of errors than k=3 model. Hence, we got performance improvement with `k=7`.\n\n```{python}\n# visualize confusion matrix with seaborn heatmap\n\nplt.figure(figsize=(6,4))\n\ncm_matrix = pd.DataFrame(data=cm_7, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n```\n\n# Classification Matrices\n\n## Classification Report\n\n`Classification report` is another way to evaluate the classification model performance. It displays the `precision`, `recall`, `f1` and `support scores` for the model. I have described these terms in later.\n\nWe can print a classification report as follows:-\n\n```{python}\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_7))\n```\n\n## Classification Accuracy\n\n```{python}\nTP = cm_7[0,0]\nTN = cm_7[1,1]\nFP = cm_7[0,1]\nFN = cm_7[1,0]\n```\n\n```{python}\n# print classification accuracy\n\nclassification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n```\n\n## Classification Error\n\n```{python}\n# print classification error\n\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))\n```\n\n## Precision\n\n`Precision` can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives `(TP + FP)`. **Precision is a metric that tells us about the quality of positive predictions.** So, `Precision` identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class. Precision is a useful metric in cases where False Positive is a higher concern than False Negatives. Precision is important in music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.tt\n\nMathematically, precision can be defined as the ratio of `TP` to `(TP + FP)`.\n\n```{python}\n# print precision score\n\nprecision = TP / float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))\n```\n\n## Recall\n\n`Recall` can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives `(TP + FN)`. **Recall tells us about how well the model identifies true positives.** Recall is also called `Sensitivity`. `Recall` identifies the proportion of correctly predicted actual positives. Mathematically, recall can be given as the ratio of `TP` to `(TP + FN)`. Recall is a useful metric in cases where False Negative triumphs over False Positive.\nRecall is important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n\n```{python}\nrecall = TP / float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))\n```\n\n## Precision vs Recall\n\nData scientists optimize their model to have higher precision or recall depending on the circumstances. A model with higher recall than precision often makes more positive predictions. A model like this comes with higher false positives and low false negatives. In scenarios like disease prediction, models should always be optimized for recall. **False positives are better than false negatives in the healthcare industry.**\n\nOn the other hand, a model with higher precision will have fewer false positives and more false negatives. If you were to build a bot detection machine learning model for an online store, you may want to optimize for higher precision, since banning legitimate users from the website will lead to a decline in sales.\n\n## f1-score\n\nIn practice, when we try to increase the precision of our model, the recall goes down, and vice-versa. The F1-score captures both the trends in a single value:\n\n            f1-score = 2/((1/Recall) + (1/Precision))\n\n`f1-score` is the weighted harmonic mean of precision and recall, and so it gives a combined idea about these two metrics. **It is maximum when Precision is equal to Recall.** The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy.\n\n## Support\n\n`Support` is the actual number of occurrences of the class in our dataset.\n\n\n## True Positive Rate\n\n`True Positive Rate` is synonymous with **`Recall`.**\n\n```{python}\ntrue_positive_rate = TP / float(TP + FN)\n\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n```\n\n## False Positive Rate\n\n```{python}\nfalse_positive_rate = FP / float(FP + TN)\n\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n```\n\n## Specificity (True Negative Rate)\n\n```{python}\nspecificity = TN / (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))\n```\n\n\n\n## Adjusting the Classification Threshold Level\n\n```{python}\n# print the first 10 predicted probabilities of two classes- 2 and 4\n\ny_pred_prob = knn.predict_proba(X_test)[0:10]\n\ny_pred_prob\n```\n\n**Observations** In each row, the numbers sum to 1. There are 2 columns which correspond to 2 classes - 2 and 4.\n\n-   `Class 2` - predicted probability that there is `benign cancer`.\n\n-   `Class 4` - predicted probability that there is `malignant cancer`.\n\n**Importance of predicted probabilities**\n\nWe can rank the observations by probability of benign or malignant cancer.\n\n-   `predict_proba` process\n\n           * Predicts the probabilities\n\n           * Choose the class with the highest probability\n\n-   Classification threshold level\n\n           * There is a classification threshold level of 0.5.\n\n           * Class 4 - probability of malignant cancer is predicted if probability > 0.5.\n\n           * Class 2 - probability of benign cancer is predicted if probability < 0.5.\n\n```{python}\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, \\\ncolumns=['Prob of - benign cancer (2)', 'Prob of - malignant cancer (4)'])\n\ny_pred_prob_df\n```\n\n```{python}\n# print the first 10 predicted probabilities for class 4 - Probability of malignant cancer\n\nknn.predict_proba(X_test)[0:10, 1]\n```\n\n```{python}\n# store the predicted probabilities for class 4 - Probability of malignant cancer\n\ny_pred_1 = knn.predict_proba(X_test)[:, 1]\n```\n\n```{python}\n# plot histogram of predicted probabilities\n\n# adjust figure size\nplt.figure(figsize=(6,4))\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n# plot histogram with 10 bins\nplt.hist(y_pred_1, bins = 10)\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities of malignant cancer')\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n# set the title\nplt.xlabel('Predicted probabilities of malignant cancer')\nplt.ylabel('Frequency')\n```\n\n**Observations**\n\n-   We can see that the above histogram is positively skewed.\n-   The first column tell us that there are approximately 80 observations with 0 \\* \\* \\* \\* probability of malignant cancer.\n-   There are few observations with probability \\> 0.5.\n-   So, these few observations predict that there will be malignant cancer.\n\n**Comments**\n\n-   In binary problems, the threshold of 0.5 is used by default to convert predicted probabilities into class predictions.\n-   Threshold can be adjusted to increase sensitivity or specificity.\n-   **Sensitivity and specificity have an inverse relationship.** Increasing one would always decrease the other and vice versa.\n-   **Adjusting the threshold level should be one of the last step you do in the model-building process.**\n\n# ROC (Receiver Operating Characteristics) - AUC (Area Under ROC Curve) Curve \n\n## ROC Curve\n\nAnother tool to measure the classification model performance visually is `ROC Curve`. `ROC Curve` stands for `Receiver Operating Characteristic` Curve. An `ROC Curve` is a plot which shows the performance of a classification model at various classification **threshold levels.**\n\nThe `ROC Curve` plots the `True Positive Rate (TPR)` against the `False Positive Rate (FPR)` at various threshold levels. `True Positive Rate (TPR)` is also called `Recall`. It is defined as the ratio of `TP to (TP + FN)`. `False Positive Rate (FPR)` is defined as the ratio of `FP to (FP + TN)`.\n\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positive. It will increase both True Positives (TP) and False Positives (FP).\n\n```{python}\n# plot ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_1, pos_label=4)\n\nplt.figure(figsize=(6,4))\n\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\n\nplt.title('ROC curve for Breast Cancer kNN classifier')\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\n\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()\n```\n\n`ROC curve` help us to choose a threshold level that balances `sensitivity` and `specificity` for a particular context.\n\n## ROC AUC\n\n`ROC AUC` stands for `Receiver Operating Characteristic - Area Under Curve`. It is a technique to compare classifier performance. In this technique, we measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. So, `ROC AUC` is the percentage of the ROC plot that is underneath the curve.\n\n```{python}\n# compute ROC AUC\n\nfrom sklearn.metrics import roc_auc_score\n\nROC_AUC = roc_auc_score(y_test, y_pred_1)\n\nprint('ROC AUC : {:.4f}'.format(ROC_AUC))\n```\n\n**Interpretation**\n\n-   ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\n\n-   ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it is benign or malignant cancer.\n\n```{python}\nfrom sklearn.model_selection import cross_val_score\n\nCross_validated_ROC_AUC = \\\ncross_val_score(knn_7, X_train, y_train, cv=5,scoring='roc_auc').mean()\n\nprint('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))\n```\n\n**Interpretation**\n\nOur Cross Validated ROC AUC is very close to 1. So, we can conclude that, the KNN classifier is indeed a very good model.\n\n# K-fold Cross Validation\n\nIn this section, I will apply k-fold Cross Validation technique to improve the model performance. Cross-validation is a statistical method of evaluating generalization performance It is more stable and thorough than using a train-test split to evaluate model performance.\n\n```{python}\n# Applying 10-Fold Cross Validation\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(knn_7, X_train, y_train, cv = 10, scoring='accuracy')\n\nprint('Cross-validation scores:{}'.format(scores))\n```\n\nWe can summarize the cross-validation accuracy by calculating its mean.\n\n```{python}\n# compute Average cross-validation score\n\nprint('Average cross-validation score: {:.4f}'.format(scores.mean()))\n```\n\n**Interpretation**\n\n-   Using the mean cross-validation, we can conclude that we expect the model to be around 96.46 % accurate on average.\n\n-   If we look at all the 10 scores produced by the 10-fold cross-validation, we can also conclude that there is a relatively high variance in the accuracy between folds, ranging from 100% accuracy to 87.72% accuracy. So, we can conclude that the model is very dependent on the particular folds used for training, but it also be the consequence of the small size of the dataset.\n\n-   We can see that 10-fold cross-validation accuracy does not result in performance improvement for this model.\n\n# Results and Conclusion\n\n1.  In this project, I build a kNN classifier model to classify the patients suffering from breast cancer. The model yields very good performance as indicated by the model accuracy which was found to be 0.9786 with k=7.\n\n2.  With k=3, the training-set accuracy score is 0.9821 while the test-set accuracy to be 0.9714. These two values are quite comparable. So, there is no question of overfitting.\n\n3.  I have compared the model accuracy score which is 0.9714 with null accuracy score which is 0.6071. So, we can conclude that our K Nearest Neighbors model is doing a very good job in predicting the class labels.\n\n4.  Our original model accuracy score with k=3 is 0.9714. Now, we can see that we get same accuracy score of 0.9714 with k=5. But, if we increase the value of k further, this would result in enhanced accuracy. With k=6,7,8 we get accuracy score of 0.9786. So, it results in performance improvement. If we increase k to 9, then accuracy decreases again to 0.9714. So, we can conclude that our optimal value of k is 7.\n\n5.  kNN Classification model with k=7 shows more accurate predictions and less number of errors than k=3 model. Hence, we got performance improvement with k=7.\n\n6.  ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it is benign or malignant cancer.\n\n7.  Using the mean cross-validation, we can conclude that we expect the model to be around 96.46 % accurate on average.\n\n8.  If we look at all the 10 scores produced by the 10-fold cross-validation, we can also conclude that there is a relatively high variance in the accuracy between folds, ranging from 100% accuracy to 87.72% accuracy. So, we can conclude that the model is very dependent on the particular folds used for training, but it also be the consequence of the small size of the dataset.\n\n# Reference\n\nThe code for this analysis was collected from the link - <https://www.kaggle.com/code/prashant111/knn-classifier-tutorial/notebook#11.-Feature-Engineering->\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"toc-depth":5,"number-sections":true,"output-file":"chap03_knn.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"solar","light":"flatly","linkcolor":"orangered","mainfont":"emoji","title":"<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}