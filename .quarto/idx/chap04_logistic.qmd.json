{"title":"<center> Chapter # 04 <br> Logistic Regression","markdown":{"yaml":{"title":"<center> Chapter # 04 <br> Logistic Regression","format":{"html":{"toc":true,"toc-depth":5,"number-sections":true,"mainfont":"emoji"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n\n## Loading Python Package \n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\n```\n\n## Loading Dataset \n\n```{python}\n# Importing Training Dataset \ntrain_df = pd.read_csv(\"DATA/train.csv\")\n# Importing Testing Dataset\ntest_df = pd.read_csv(\"DATA/test.csv\")\n```\n\n### Metadata of the Dataset \n\n```{python}\n\nprint(\"The total number of rows and columns in the dataset is {} and {} respectively.\".format(train_df.shape[0],train_df.shape[1]))\nprint (\"\\nThe names and the types of the variables of the dataset:\")\ntrain_df.info()\ntrain_df.head()\nprint(\"\\nThe types of the variables in the dataset are:\")\ntrain_df.dtypes\n```\n\n```{python}\nfor x in train_df.columns:\n  print  (x)\ntrain_df['Pclass'].value_counts(sort = True, ascending = True)\n```\n\n```{python}\ntrain_df['Sex'].value_counts()\n```\n\n## Data Quality & Missing Value Assessment\n\n```{python}\n# Missing value in Training Dataset \ntrain_df.isna().sum()\n```\n\n```{python}\n# Missing value in Testing Dataset \ntest_df.isnull().sum()\n```\n\n### Age - Missing Value\n\n```{python}\nprint ('The percent of missing \"Age\" record in the training dataset is %0.2f%%' %(train_df['Age'].isnull().sum()/train_df.shape[0]*100))\n```\n\n```{python}\nprint ('The percent of missing \"Age\" record in testing dataset is %0.3f%%' %(test_df['Age'].isnull().sum()/test_df.shape[0]*100))\n```\n\n```{python}\nplt.figure(figsize = (10,8))\ntrain_df['Age'].hist(bins = 15, density = True, color = 'teal', alpha = 0.60)\ntrain_df['Age'].plot(kind = 'density', color = 'teal', alpha = 0.60)\nplt.show()\n```\n\nSince the variable `Age` is a little bit right skewed, using the mean to replace the missing observations might bias our results. Therefore, it is recommended that median be used to replace the missing observations.\n\n```{python}\nprint ('The mean of \"Age\" variable is %0.3f.' %(train_df['Age'].mean(skipna = True)))\n```\n\n```{python}\nprint ('The median of \"Age\" variable is %0.2f.' %(train_df['Age'].median(skipna = True)))\n```\n\n### Cabin - Missing Value\n\n```{python}\ntrain_df['Cabin'].value_counts()\n```\n\n```{python}\nprint ('The percent of cabin variable missing value is %0.2f%%.' %(train_df['Cabin'].isnull().sum()/train_df.shape[0]*100))\n```\n\n77% observations of the `Cabin` variable is missing. Therefore, it is better to prune the variable from the dataset. Moreover, the drop of the `Cabin` variable is justified because it has correlation with two other variables - `Fare` and `Pclass`.\n\n### Embarked - Missing Value\n\n```{python}\ntrain_df['Embarked'].value_counts()\n```\n\n```{python}\n# Percent of missing 'Embarked' Variable\nprint(\n    \"The percent of missing 'Embarked' records is %.2f%%\" %\n    (train_df['Embarked'].isnull().sum()/train_df.shape[0]*100)\n)\n```\n\nSince there are only 0.22% missing observation for `Embarked`, we can impute the missing values with the port where most people embarked.\n\n```{python}\nprint('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\nprint(train_df['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train_df, palette='Set2')\nplt.show()\n```\n\n```{python}\nprint('The most common boarding port of embarkation is %s.' %train_df['Embarked'].value_counts().idxmax())\n```\n\n## Final Adjustment to the Datasets (Training & Testing)\n\nBased on the assessment of the missing values in the dataset, We will make the following changes to the data:\n\n-   The missing value of `Age` variable will be imputed with 28 (median value `Age`)\n-   The missing value of `Embarked` variable will be imputed with `S` (the most common boarding point)\n-   There are many missing values for the variable `Cabin`; therefore, the variable will be dropped. Moreover, the drop will not affect the model as the variable is associated with two other variables - `Pclass` and `Fare`.\n\n```{python}\ntrain_data = train_df.copy()\ntrain_data['Age'].fillna(train_data['Age'].median(skipna = True), inplace = True)\ntrain_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace = True)\ntrain_data.drop(['Cabin'], axis = 1, inplace = True)\ntrain_data.isnull().sum()\n```\n\n```{python}\ntrain_data.tail()\n```\n\n```{python}\nplt.figure(figsize=(15,8))\n# Data with missing observations\nax = train_df['Age'].hist(bins = 15, density = True, stacked = True, color = 'teal', alpha = 0.6)\ntrain_df['Age'].plot(kind = 'density', color = 'teal')\n# Data without missing observations\nax = train_data['Age'].hist(bins = 15, density = True, stacked = True, color = 'orange', alpha = 0.6)\ntrain_data['Age'].plot(kind = 'density', color = 'orange')\nplt.xlim(-10,85)\nax.legend([\"Raw Age\", \"Adjusted Age\"])\nax.set(xlabel = 'Age')\nplt.show()\n```\n\n### Additional Variables\n\nThe variable `SibSp` means whether the passenger has sibling or spouse aboard and the variable `Parch` means whether the passenger has parents or children aboard. For the sake of simplicity and to account for **multicollinearity**, these two variables will be combined into a categorical variable: whether or not the individual was traveling alone.\n\n```{python}\n# Creating categorical variable for Traveling alone\ntrain_data['TravelAlone'] = np.where((train_data['SibSp']+train_data['Parch']) > 0, 0, 1)\ntrain_data.drop('SibSp', axis = 1, inplace = True)\ntrain_data.drop('Parch', axis = 1, inplace = True)\n```\n\nFor variables `Pclass`, `Sex`, and `Embarked`, categorical variables will be created\n\n```{python}\ntraining = pd.get_dummies(train_data, columns= [\"Pclass\", \"Embarked\", \"Sex\"])\ntraining.info()\n```\n\n```{python}\ntraining[['Pclass_1','Pclass_2','Pclass_3', 'Embarked_C','Embarked_Q','Embarked_S', 'Sex_female', 'Sex_male']].head()\n```\n\n```{python}\ntraining.drop(['Sex_female', 'PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\ntraining.tail()\n```\n\n```{python}\ntraining.info()\n```\n\n```{python}\nfinal_train = training\nfinal_train.tail()\n```\n\n```{python}\ntest_df.isna().sum()\n```\n\nNow we apply the same changes in the testing dataset.\n\n-   We will apply to same imputation for `Age` in the Test data as we did for my Training data (if missing, `Age` = 28).\n-   We will also remove the `Cabin` variable from the test data, as we've decided not to include it in my analysis.\n-   There were no missing values in the `Embarked` port variable.\n-   We will add the dummy variables to finalize the test set.\n-   Finally, We will impute the 1 missing value for `Fare` with the median, 14.45.\n\n```{python}\nprint('The median value of \"Fare\" variable in testing dataset is %0.3f.' %(train_df['Fare'].median(skipna = True)))\n```\n\n```{python}\ntest_data = test_df.copy()\ntest_data['Age'].fillna(test_data['Age'].median(skipna = True), inplace = True)\ntest_data['Fare'].fillna(test_data['Fare'].median(skipna = True), inplace = True)\ntest_data.drop(['Cabin'], axis  = 1, inplace = True)\n```\n\n### Creating New Variables\n\n```{python}\n# Creating new variable - TravelAlone\ntest_data['TravelAlone']= np.where(test_data['SibSp']+test_data['Parch']>0,0,1)\ntest_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\ntest_data.sample(5)\n```\n\n#### Creating the Dummies for Categorical Variables\n\n```{python}\ntesting = pd.get_dummies(test_data, columns = ['Pclass', 'Sex', 'Embarked'])\ntesting.drop(['Sex_female','PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)\ntesting.tail()\n```\n\n```{python}\nfinal_test = testing\nfinal_test.head()\n```\n\n## Exploratory Data Analysis (EDA)\n\n### Exploration of `Age` Variable\n\n```{python}\nfinal_train.info()\n```\n\n```{python}\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Age'][final_train.Survived == 1], shade=True, color = 'darkturquoise')\nsns.kdeplot(final_train['Age'][final_train.Survived == 0], shade=True, color = 'lightcoral')\nax.legend(['Survived', 'Died']) # or you can use plt.legend(['Survived', 'Died])\nplt.title('Density Plot for Surviving Population and Deceased Population')\nplt.xlabel('Age') # or you can use ax.set(xlabel = 'Age')\nplt.xlim(-10,85)\nplt.show()\n```\n\nThe age distribution for survivors and deceased is actually very similar. One notable difference is that, of the survivors, a larger proportion were children. The passengers evidently made an attempt to save children by giving them a place on the life rafts.\n\n```{python}\navg_age = final_train.groupby(['Survived']) ['Age'].mean()\navg_age.to_frame().reset_index()\n```\n\n```{python}\nsns.boxplot(data = final_train, x  = 'Survived', y = 'Age', palette='Set2')\nplt.title(\"Comparison of Age of Passengers Conditioned on Survived\")\nplt.show()\n```\n\n```{python}\n# Creating a Dummy Variable IsMinor\nfinal_train['IsMinor'] = np.where(final_train['Age'] <= 16, 1, 0)\nfinal_test['IsMinor'] = np.where(final_test['Age'] <= 16, 1, 0)\n```\n\n### Exploration of `Fare` Variable\n\n```{python}\ntrain_df.groupby(['Survived']) ['Fare'].mean().to_frame().reset_index()\n```\n\n```{python}\nsns.boxplot(data = final_train, x = 'Survived', y = 'Fare', palette='Set2')\nplt.ylim(0, 100)\nplt.title('Comparison of Fare of Passengers Conditioned on Survived')\nplt.show()\n```\n\n```{python}\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Fare'][final_train.Survived == 1],shade=True, color='darkturquoise')\nsns.kdeplot(final_train['Fare'][final_train.Survived==0], shade=True, color='lightcoral')\nax.legend(['Survived', 'Died'])\nax.set(xlabel= 'Fare')\nplt.xlim(-20,200)\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nplt.show()\n```\n\nAs the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class, which we'll look at next.\n\n```{python}\n#| warning: false\n# Pair Plot of two continuous variables (Age and Fare)\nplt.figure(figsize=(15,8))\nsns.pairplot(data=train_data, hue='Survived', vars= ['Age', 'Fare'])\nplt.show()\n```\n\n### Exploration of `PClass` Variable\n\n```{python}\nsns.barplot(data = train_df, x = 'Pclass', y = 'Survived', color='darkturquoise')\nplt.show()\n```\n\nAs expected, first class passengers were more likely to survive.\n\n### Exploration of `Embarked` Variable\n\n```{python}\nsns.barplot(x = 'Embarked', y = 'Survived', data=train_df, color=\"teal\")\nplt.show()\n```\n\nPassengers who boarded in Cherbourg, France, appear to have the highest survival rate. Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown. This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck). It's also worth noting the size of the whiskers in these plots. Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest. The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker. It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts.\n\n### Exploration of `TravelAlone` Variable\n\n```{python}\nsns.barplot(x = 'TravelAlone', y = 'Survived', data=final_train, color=\"mediumturquoise\")\nplt.xlabel('Travel Alone')\nplt.show()\n```\n\nIndividuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male.\n\n### Exploration of `Gender` Variable\n\n```{python}\nsns.barplot(x = 'Sex', y = 'Survived', data=train_df, color=\"aquamarine\")\nplt.show()\n```\n\n### Chi-square Test of Independence\n\n#### Chi-square Test of Independence betweeen `Survived` and `Sex`\n\n```{python}\npd.crosstab(train_df['Survived'], train_df['Sex'])\n```\n\n```{python}\n# Importing scipy package \nfrom scipy import stats\n```\n\n```{python}\nstats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\n```\n\n\n```{python}\nchi2_stat, p, dof, expected = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\nprint(f\"chi2 statistic:     {chi2_stat:.5g}\")\nprint(f\"p-value:            {p:.5g}\")\nprint(f\"degrees of freedom: {dof}\")\nprint(\"expected frequencies:\\n\",expected)\n\n```\n#### Chi-square Test of Independence betweeen `Survived` and `Pclass`\n\n```{python}\nchi2_stat_2, p_2, dof_2, expected_2 = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Pclass']))\nprint(f\"chi2 statistic:     {chi2_stat_2:.5g}\")\nprint(f\"p-value:            {p_2:.5g}\")\nprint(f\"degrees of freedom: {dof_2}\")\nprint(\"expected frequencies:\\n\",expected_2)\n```\n\n### Post Hoc Analysis of `Pclass`\n\nThe explanation of  `Post Hoc` analysis is given in this [link](!https://www.youtube.com/watch?v=-S8EJEYNFIc).\n\n\n```{python}\npclass_cross = pd.crosstab(train_df['Survived'], train_df['Pclass'])\npclass_cross\n```\n\n```{python}\nimport gc\nfrom itertools import combinations\nimport scipy.stats\nimport statsmodels.stats.multicomp as multi\nfrom statsmodels.stats.multitest import multipletests\n```\n\n\n```{python}\np_vals_chi = []\npairs_of_class = list(combinations(train_df['Pclass'].unique(),2))\n\nfor each_pair in pairs_of_class:\n    each_df = train_df[(train_df['Pclass']==each_pair[0]) | (train_df['Pclass']==each_pair[1])]\n    p_vals_chi.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Pclass']))[1]\n         )\n```\n\n\n```{python}\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of class',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of class'] = pairs_of_class\nbonferroni_results['original p value'] = p_vals_chi\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni = multipletests(p_vals_chi, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni[0]\nbonferroni_results.head()\n```\n\n### Post Hoc Analysis of `Embarked`\n\n```{python}\n# Write code Here \np_vals_chi_embark = []\npairs_of_embark = list(combinations(train_data['Embarked'].unique(),2))\n\nfor each_pair in pairs_of_embark:\n    each_df = train_data[(train_data['Embarked']==each_pair[0]) | (train_data['Embarked']==each_pair[1])]\n    p_vals_chi_embark.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Embarked']))[1]\n         )\n```\n\n```{python}\n#Write code Here\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of embark',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of embark'] = pairs_of_embark\nbonferroni_results['original p value'] = p_vals_chi_embark\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni_embark = multipletests(p_vals_chi_embark, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni_embark[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni_embark[0]\nbonferroni_results.head()\n```\n\n## Logistic Regression & Results\n\n### Feature Selection\n\n#### Recursive Feature Selection (RFE)\n\nGiven an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a `coef_ attribute` or through a `feature_importances_` attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\n```{python}\n# Our all training dataset \ntrain_df\ntrain_data # impute missing values \ntraining # create the dummies \nfinal_train\n```\n\n```{python}\ncols = ['Age', 'Fare', 'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX = final_train[cols] # features vector\ny = final_train['Survived'] # Target vector \n```\n\n```{python}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\n```\n\n```{python}\n# Build the model \nmodel = LogisticRegression()\n# Create the RFE model \nrfe = RFE(estimator=model, n_features_to_select=8)\nrfe = rfe.fit(X ,y)\ndir(rfe)\n\n```\n\n```{python}\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))\n```\n\n```{python}\n# Getting the Regression Results \nimport statsmodels.api as sm\nlogit_model = sm.Logit(y,X.astype(float))\nresult = logit_model.fit()\nprint(result.summary2())\n```\n\n```{python}\ncols2 = ['Age',  'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX_alt = final_train[cols2] # features vector\nlogit_model2 = sm.Logit(y,sm.add_constant(X_alt.astype(float)))\nresult2 = logit_model2.fit()\nprint(result2.summary2())\n```\n\n```{python}\nfrom stargazer.stargazer import Stargazer\ntitanic_logit = Stargazer([result, result2])\ntitanic_logit\n```\n\n```{python}\n# Interpreting the coefficients, which are the log odds; therefore, we need to convert them into odds ratio. \nnp.exp(-2.5527)\n```\n\n```{python}\nnp.exp(result.params) # Getting the Odds Ratio of all Features \n```\n\n#### Feature Ranking with Recursive Feature Elimination and Cross-validation (`RFECV`)\n\n`RFECV` performs `RFE` in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation\n\n```{python}\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(max_iter = 5000), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\nfor x in dir(rfecv):\n  print(x)\n```\n\n```{python}\n# To get the accuracy\nrfecv.cv_results_\n```\n\n```{python}\ntype(rfecv.cv_results_)\n```\n\n```{python}\nrfecv.cv_results_\n\n```\n\n```{python}\nrfecv.n_features_\n```\n\n```{python}\nlist(X.columns[rfecv.support_])\n```\n\n```{python}\n# To check whether the RFE and RFECV generate the same features\nset(list(X.columns[rfecv.support_])) == set(list((X.columns[rfe.support_])))  \n```\n\n```{python}\nrfecv.cv_results_.keys()\n```\n\n```{python}\ncrossVal_results = rfecv.cv_results_\ncrossVal_results\ndel crossVal_results['mean_test_score']\ndel crossVal_results['std_test_score']\ncrossVal_results\n```\n\n## Correlation Matrix\n\n```{python}\nSelected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n                     'Embarked_S', 'Sex_male', 'IsMinor']\nX = final_train[Selected_features] # Recreated features vector \n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\") # for cmap = 'viridis' can also be used.\nplt.show()\n```\n\n```{python}\nplt.subplots(figsize=(8, 5))\nsns.heatmap(final_train[['Survived', 'Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_S', 'Sex_male', 'IsMinor']].corr(), annot = True, cmap = 'viridis')\nplt.show()\n```\n\n## Model Evaluation Procedures\n\n### Model Evaluation Based on Train/Test Split\n\n```{python}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n```\n\n```{python}\nX = final_train[Selected_features]\ny = final_train['Survived']\n```\n\n```{python}\n# use train/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n```\n\n```{python}\n# Check classification scores of Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test) # Prediction actual class\ny_pred_proba = logreg.predict_proba(X_test) [:,1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba)) \nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr)) \n\n```\n\n### Confusion Matrix\n\n```{python}\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n```\n\n```{python}\n# Visualizing Confusion Matrix\nplt.figure(figsize=(6,4))\ncm_matrix = pd.DataFrame(data= cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()\n```\n\n### Classification Report\n\n```{python}\nprint (classification_report(y_test, y_pred))\n```\n\n### ROC-AUC Curve\n\n```{python}\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility (true positive rate (tpr)) > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall/sensitivity)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))\n```\n\n## Model Evaluation Based on K-fold Cross-validation `cross_val_score()` Function\n\n```{python}\n# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression(max_iter=5000)\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())\n\n```\n\n### Model Evaluation Based on K-fold Cross-validation Using `cross_validate()` Function\n\n```{python}\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std())) \n```\n\nWhat happens when we add the feature `Fare` - \n\n```{python}\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"]\nX = final_train[cols]\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, final_train[cols], y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))\n```\nWe notice that the model is slightly deteriorated. The `Fare` variable does not carry any useful information. Its presence is just a noise for the logistic regression model.\n\n\n## `GridSearchCV` Evaluating Using Multiple Scorers Simultaneously\n\n\n\n\n\n```{python}\nfrom sklearn.model_selection import GridSearchCV\n\nX = final_train[Selected_features]\n\nparam_grid = {'C': np.arange(1e-05, 3, 0.1)}\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n\ngs = GridSearchCV(LogisticRegression(max_iter=5000), return_train_score=True,\n                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n\ngs.fit(X, y)\nresults = gs.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(gs.best_estimator_))\nprint(\"best params: \" + str(gs.best_params_))\nprint('best score:', gs.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, param_grid['C'].max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n```\n\n### `GridSearchCV` Evaluating Using Multiple scorers, `RepeatedStratifiedKFold` and `pipeline` for Preprocessing Simultaneously\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\n\n#Define simple model\n###############################################################################\nC = np.arange(1e-05, 5.5, 0.1)\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\nlog_reg = LogisticRegression(max_iter=5000)\n\n#Simple pre-processing estimators\n###############################################################################\nstd_scale = StandardScaler(with_mean=False, with_std=False)\n#std_scale = StandardScaler()\n\n#Defining the CV method: Using the Repeated Stratified K Fold\n###############################################################################\n\nn_folds=5\nn_repeats=5\n\nrskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)\n\n#Creating simple pipeline and defining the gridsearch\n###############################################################################\n\nlog_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])\n\nlog_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,\n              scoring=scoring, return_train_score=True,\n              param_grid=dict(clf__C=C), refit='Accuracy')\n\nlog_clf.fit(X, y)\nresults = log_clf.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(log_clf.best_estimator_))\nprint(\"best params: \" + str(log_clf.best_params_))\nprint('best score:', log_clf.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, C.max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_clf__C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n```\n## Regularization \n\n`Regularization` is a method of preventing overfitting, which is a common problem in machine learning. Overfitting means that your model learns too much from the specific data you have, and fails to generalize well to new or unseen data. This can lead to poor predictions and low performance. Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables.\n\nSince the size of each coefficient depends on the scale of its corresponding variable, scaling the data is required so that the regularization penalizes each variable equally. The regularization strength is determined by `C` and as `C` increases, the regularization term becomes smaller (and for extremely large `C` values, it's as if there is no regularization at all).\n\nIf the initial model is overfit (as in, it fits the training data too well), then adding a strong regularization term (with small `C` value) makes the model perform worse for the training data, but introducing such \"noise\" improves the model's performance on unseen (or test) data.\n\nAn example with 1000 samples and 200 features shown below. As can be seen from the plot of accuracy over different values of C, if C is large (with very little regularization), there is a big gap between how the model performs on training data and test data. However, as C decreases, the model performs worse on training data but performs better on test data (test accuracy increases). However, when C becomes too small (or the regularization becomes too strong), the model begins performing worse again because now the regularization term completely dominates the objective function.\n\n\n\n```{python}\n# Necessary Python Packages \nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# make sample data\nX, y = make_classification(1000, 200, n_informative=195, random_state=2023)\n# split into train-test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2023)\n\n# normalize the data\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# train Logistic Regression models for different values of C\n# and collect train and test accuracies\nscores = {}\nfor C in (10**k for k in range(-6, 6)):\n    lr = LogisticRegression(C=C)\n    lr.fit(X_train, y_train)\n    scores[C] = {'train accuracy': lr.score(X_train, y_train), \n                 'test accuracy': lr.score(X_test, y_test)}\n\n# plot the accuracy scores for different values of C\npd.DataFrame.from_dict(scores, 'index').plot(logx=True, xlabel='C', ylabel='accuracy')\n```\n\n\n### Types of Regularization \n\n#### `L1` regularization\n\n`L1` regularization, also known as `Lasso regularization`, adds the sum of the absolute values of the model’s coefficients to the loss function. It encourages sparsity in the model by shrinking some coefficients to precisely zero. This has the effect of performing feature selection, as the model can effectively ignore irrelevant or less important features. L1 regularization is particularly useful when dealing with high-dimensional datasets with desired feature selection.\n\nMathematically, the `L1` regularization term can be written as:\n\n**`L1 regularization = λ * Σ|wi|`**\n\nHere, `λ` is the regularization parameter that controls the strength of regularization, `wi` represents the individual model coefficients and the sum is taken over all coefficients.\n\n\n#### `L2` regularization\n\n`L2` regularization, also known as `Ridge regularization`, adds the sum of the squared values of the model’s coefficients to the loss function. Unlike L1 regularization, L2 regularization does not force the coefficients to be exactly zero but instead encourages them to be small. L2 regularization can prevent overfitting by spreading the influence of a single feature across multiple features. It is advantageous when there are correlations between the input features.\n\nMathematically, the `L2` regularization term can be written as:\n\n**`L2 regularization = λ * Σ(wi^2)`**\n\nSimilar to `L1` regularization, `λ` is the regularization parameter, and `wi` represents the model coefficients. The sum is taken over all coefficients, and the squares of the coefficients are summed.\n\n\nThe choice between `L1` and `L2` regularization depends on the specific problem and the characteristics of the data. For example, `L1` regularization produces sparse models, which can be advantageous when feature selection is desired. `L2` regularization, on the other hand, encourages small but non-zero coefficients and can be more suitable when there are strong correlations between features.\n\n## Conclusion\n\n\n\n\n","srcMarkdownNoYaml":"\n\n## Introduction \n\n\n## Loading Python Package \n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\n```\n\n## Loading Dataset \n\n```{python}\n# Importing Training Dataset \ntrain_df = pd.read_csv(\"DATA/train.csv\")\n# Importing Testing Dataset\ntest_df = pd.read_csv(\"DATA/test.csv\")\n```\n\n### Metadata of the Dataset \n\n```{python}\n\nprint(\"The total number of rows and columns in the dataset is {} and {} respectively.\".format(train_df.shape[0],train_df.shape[1]))\nprint (\"\\nThe names and the types of the variables of the dataset:\")\ntrain_df.info()\ntrain_df.head()\nprint(\"\\nThe types of the variables in the dataset are:\")\ntrain_df.dtypes\n```\n\n```{python}\nfor x in train_df.columns:\n  print  (x)\ntrain_df['Pclass'].value_counts(sort = True, ascending = True)\n```\n\n```{python}\ntrain_df['Sex'].value_counts()\n```\n\n## Data Quality & Missing Value Assessment\n\n```{python}\n# Missing value in Training Dataset \ntrain_df.isna().sum()\n```\n\n```{python}\n# Missing value in Testing Dataset \ntest_df.isnull().sum()\n```\n\n### Age - Missing Value\n\n```{python}\nprint ('The percent of missing \"Age\" record in the training dataset is %0.2f%%' %(train_df['Age'].isnull().sum()/train_df.shape[0]*100))\n```\n\n```{python}\nprint ('The percent of missing \"Age\" record in testing dataset is %0.3f%%' %(test_df['Age'].isnull().sum()/test_df.shape[0]*100))\n```\n\n```{python}\nplt.figure(figsize = (10,8))\ntrain_df['Age'].hist(bins = 15, density = True, color = 'teal', alpha = 0.60)\ntrain_df['Age'].plot(kind = 'density', color = 'teal', alpha = 0.60)\nplt.show()\n```\n\nSince the variable `Age` is a little bit right skewed, using the mean to replace the missing observations might bias our results. Therefore, it is recommended that median be used to replace the missing observations.\n\n```{python}\nprint ('The mean of \"Age\" variable is %0.3f.' %(train_df['Age'].mean(skipna = True)))\n```\n\n```{python}\nprint ('The median of \"Age\" variable is %0.2f.' %(train_df['Age'].median(skipna = True)))\n```\n\n### Cabin - Missing Value\n\n```{python}\ntrain_df['Cabin'].value_counts()\n```\n\n```{python}\nprint ('The percent of cabin variable missing value is %0.2f%%.' %(train_df['Cabin'].isnull().sum()/train_df.shape[0]*100))\n```\n\n77% observations of the `Cabin` variable is missing. Therefore, it is better to prune the variable from the dataset. Moreover, the drop of the `Cabin` variable is justified because it has correlation with two other variables - `Fare` and `Pclass`.\n\n### Embarked - Missing Value\n\n```{python}\ntrain_df['Embarked'].value_counts()\n```\n\n```{python}\n# Percent of missing 'Embarked' Variable\nprint(\n    \"The percent of missing 'Embarked' records is %.2f%%\" %\n    (train_df['Embarked'].isnull().sum()/train_df.shape[0]*100)\n)\n```\n\nSince there are only 0.22% missing observation for `Embarked`, we can impute the missing values with the port where most people embarked.\n\n```{python}\nprint('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\nprint(train_df['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train_df, palette='Set2')\nplt.show()\n```\n\n```{python}\nprint('The most common boarding port of embarkation is %s.' %train_df['Embarked'].value_counts().idxmax())\n```\n\n## Final Adjustment to the Datasets (Training & Testing)\n\nBased on the assessment of the missing values in the dataset, We will make the following changes to the data:\n\n-   The missing value of `Age` variable will be imputed with 28 (median value `Age`)\n-   The missing value of `Embarked` variable will be imputed with `S` (the most common boarding point)\n-   There are many missing values for the variable `Cabin`; therefore, the variable will be dropped. Moreover, the drop will not affect the model as the variable is associated with two other variables - `Pclass` and `Fare`.\n\n```{python}\ntrain_data = train_df.copy()\ntrain_data['Age'].fillna(train_data['Age'].median(skipna = True), inplace = True)\ntrain_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace = True)\ntrain_data.drop(['Cabin'], axis = 1, inplace = True)\ntrain_data.isnull().sum()\n```\n\n```{python}\ntrain_data.tail()\n```\n\n```{python}\nplt.figure(figsize=(15,8))\n# Data with missing observations\nax = train_df['Age'].hist(bins = 15, density = True, stacked = True, color = 'teal', alpha = 0.6)\ntrain_df['Age'].plot(kind = 'density', color = 'teal')\n# Data without missing observations\nax = train_data['Age'].hist(bins = 15, density = True, stacked = True, color = 'orange', alpha = 0.6)\ntrain_data['Age'].plot(kind = 'density', color = 'orange')\nplt.xlim(-10,85)\nax.legend([\"Raw Age\", \"Adjusted Age\"])\nax.set(xlabel = 'Age')\nplt.show()\n```\n\n### Additional Variables\n\nThe variable `SibSp` means whether the passenger has sibling or spouse aboard and the variable `Parch` means whether the passenger has parents or children aboard. For the sake of simplicity and to account for **multicollinearity**, these two variables will be combined into a categorical variable: whether or not the individual was traveling alone.\n\n```{python}\n# Creating categorical variable for Traveling alone\ntrain_data['TravelAlone'] = np.where((train_data['SibSp']+train_data['Parch']) > 0, 0, 1)\ntrain_data.drop('SibSp', axis = 1, inplace = True)\ntrain_data.drop('Parch', axis = 1, inplace = True)\n```\n\nFor variables `Pclass`, `Sex`, and `Embarked`, categorical variables will be created\n\n```{python}\ntraining = pd.get_dummies(train_data, columns= [\"Pclass\", \"Embarked\", \"Sex\"])\ntraining.info()\n```\n\n```{python}\ntraining[['Pclass_1','Pclass_2','Pclass_3', 'Embarked_C','Embarked_Q','Embarked_S', 'Sex_female', 'Sex_male']].head()\n```\n\n```{python}\ntraining.drop(['Sex_female', 'PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\ntraining.tail()\n```\n\n```{python}\ntraining.info()\n```\n\n```{python}\nfinal_train = training\nfinal_train.tail()\n```\n\n```{python}\ntest_df.isna().sum()\n```\n\nNow we apply the same changes in the testing dataset.\n\n-   We will apply to same imputation for `Age` in the Test data as we did for my Training data (if missing, `Age` = 28).\n-   We will also remove the `Cabin` variable from the test data, as we've decided not to include it in my analysis.\n-   There were no missing values in the `Embarked` port variable.\n-   We will add the dummy variables to finalize the test set.\n-   Finally, We will impute the 1 missing value for `Fare` with the median, 14.45.\n\n```{python}\nprint('The median value of \"Fare\" variable in testing dataset is %0.3f.' %(train_df['Fare'].median(skipna = True)))\n```\n\n```{python}\ntest_data = test_df.copy()\ntest_data['Age'].fillna(test_data['Age'].median(skipna = True), inplace = True)\ntest_data['Fare'].fillna(test_data['Fare'].median(skipna = True), inplace = True)\ntest_data.drop(['Cabin'], axis  = 1, inplace = True)\n```\n\n### Creating New Variables\n\n```{python}\n# Creating new variable - TravelAlone\ntest_data['TravelAlone']= np.where(test_data['SibSp']+test_data['Parch']>0,0,1)\ntest_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\ntest_data.sample(5)\n```\n\n#### Creating the Dummies for Categorical Variables\n\n```{python}\ntesting = pd.get_dummies(test_data, columns = ['Pclass', 'Sex', 'Embarked'])\ntesting.drop(['Sex_female','PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)\ntesting.tail()\n```\n\n```{python}\nfinal_test = testing\nfinal_test.head()\n```\n\n## Exploratory Data Analysis (EDA)\n\n### Exploration of `Age` Variable\n\n```{python}\nfinal_train.info()\n```\n\n```{python}\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Age'][final_train.Survived == 1], shade=True, color = 'darkturquoise')\nsns.kdeplot(final_train['Age'][final_train.Survived == 0], shade=True, color = 'lightcoral')\nax.legend(['Survived', 'Died']) # or you can use plt.legend(['Survived', 'Died])\nplt.title('Density Plot for Surviving Population and Deceased Population')\nplt.xlabel('Age') # or you can use ax.set(xlabel = 'Age')\nplt.xlim(-10,85)\nplt.show()\n```\n\nThe age distribution for survivors and deceased is actually very similar. One notable difference is that, of the survivors, a larger proportion were children. The passengers evidently made an attempt to save children by giving them a place on the life rafts.\n\n```{python}\navg_age = final_train.groupby(['Survived']) ['Age'].mean()\navg_age.to_frame().reset_index()\n```\n\n```{python}\nsns.boxplot(data = final_train, x  = 'Survived', y = 'Age', palette='Set2')\nplt.title(\"Comparison of Age of Passengers Conditioned on Survived\")\nplt.show()\n```\n\n```{python}\n# Creating a Dummy Variable IsMinor\nfinal_train['IsMinor'] = np.where(final_train['Age'] <= 16, 1, 0)\nfinal_test['IsMinor'] = np.where(final_test['Age'] <= 16, 1, 0)\n```\n\n### Exploration of `Fare` Variable\n\n```{python}\ntrain_df.groupby(['Survived']) ['Fare'].mean().to_frame().reset_index()\n```\n\n```{python}\nsns.boxplot(data = final_train, x = 'Survived', y = 'Fare', palette='Set2')\nplt.ylim(0, 100)\nplt.title('Comparison of Fare of Passengers Conditioned on Survived')\nplt.show()\n```\n\n```{python}\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Fare'][final_train.Survived == 1],shade=True, color='darkturquoise')\nsns.kdeplot(final_train['Fare'][final_train.Survived==0], shade=True, color='lightcoral')\nax.legend(['Survived', 'Died'])\nax.set(xlabel= 'Fare')\nplt.xlim(-20,200)\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nplt.show()\n```\n\nAs the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class, which we'll look at next.\n\n```{python}\n#| warning: false\n# Pair Plot of two continuous variables (Age and Fare)\nplt.figure(figsize=(15,8))\nsns.pairplot(data=train_data, hue='Survived', vars= ['Age', 'Fare'])\nplt.show()\n```\n\n### Exploration of `PClass` Variable\n\n```{python}\nsns.barplot(data = train_df, x = 'Pclass', y = 'Survived', color='darkturquoise')\nplt.show()\n```\n\nAs expected, first class passengers were more likely to survive.\n\n### Exploration of `Embarked` Variable\n\n```{python}\nsns.barplot(x = 'Embarked', y = 'Survived', data=train_df, color=\"teal\")\nplt.show()\n```\n\nPassengers who boarded in Cherbourg, France, appear to have the highest survival rate. Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown. This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck). It's also worth noting the size of the whiskers in these plots. Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest. The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker. It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts.\n\n### Exploration of `TravelAlone` Variable\n\n```{python}\nsns.barplot(x = 'TravelAlone', y = 'Survived', data=final_train, color=\"mediumturquoise\")\nplt.xlabel('Travel Alone')\nplt.show()\n```\n\nIndividuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male.\n\n### Exploration of `Gender` Variable\n\n```{python}\nsns.barplot(x = 'Sex', y = 'Survived', data=train_df, color=\"aquamarine\")\nplt.show()\n```\n\n### Chi-square Test of Independence\n\n#### Chi-square Test of Independence betweeen `Survived` and `Sex`\n\n```{python}\npd.crosstab(train_df['Survived'], train_df['Sex'])\n```\n\n```{python}\n# Importing scipy package \nfrom scipy import stats\n```\n\n```{python}\nstats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\n```\n\n\n```{python}\nchi2_stat, p, dof, expected = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\nprint(f\"chi2 statistic:     {chi2_stat:.5g}\")\nprint(f\"p-value:            {p:.5g}\")\nprint(f\"degrees of freedom: {dof}\")\nprint(\"expected frequencies:\\n\",expected)\n\n```\n#### Chi-square Test of Independence betweeen `Survived` and `Pclass`\n\n```{python}\nchi2_stat_2, p_2, dof_2, expected_2 = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Pclass']))\nprint(f\"chi2 statistic:     {chi2_stat_2:.5g}\")\nprint(f\"p-value:            {p_2:.5g}\")\nprint(f\"degrees of freedom: {dof_2}\")\nprint(\"expected frequencies:\\n\",expected_2)\n```\n\n### Post Hoc Analysis of `Pclass`\n\nThe explanation of  `Post Hoc` analysis is given in this [link](!https://www.youtube.com/watch?v=-S8EJEYNFIc).\n\n\n```{python}\npclass_cross = pd.crosstab(train_df['Survived'], train_df['Pclass'])\npclass_cross\n```\n\n```{python}\nimport gc\nfrom itertools import combinations\nimport scipy.stats\nimport statsmodels.stats.multicomp as multi\nfrom statsmodels.stats.multitest import multipletests\n```\n\n\n```{python}\np_vals_chi = []\npairs_of_class = list(combinations(train_df['Pclass'].unique(),2))\n\nfor each_pair in pairs_of_class:\n    each_df = train_df[(train_df['Pclass']==each_pair[0]) | (train_df['Pclass']==each_pair[1])]\n    p_vals_chi.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Pclass']))[1]\n         )\n```\n\n\n```{python}\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of class',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of class'] = pairs_of_class\nbonferroni_results['original p value'] = p_vals_chi\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni = multipletests(p_vals_chi, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni[0]\nbonferroni_results.head()\n```\n\n### Post Hoc Analysis of `Embarked`\n\n```{python}\n# Write code Here \np_vals_chi_embark = []\npairs_of_embark = list(combinations(train_data['Embarked'].unique(),2))\n\nfor each_pair in pairs_of_embark:\n    each_df = train_data[(train_data['Embarked']==each_pair[0]) | (train_data['Embarked']==each_pair[1])]\n    p_vals_chi_embark.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Embarked']))[1]\n         )\n```\n\n```{python}\n#Write code Here\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of embark',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of embark'] = pairs_of_embark\nbonferroni_results['original p value'] = p_vals_chi_embark\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni_embark = multipletests(p_vals_chi_embark, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni_embark[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni_embark[0]\nbonferroni_results.head()\n```\n\n## Logistic Regression & Results\n\n### Feature Selection\n\n#### Recursive Feature Selection (RFE)\n\nGiven an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a `coef_ attribute` or through a `feature_importances_` attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\n```{python}\n# Our all training dataset \ntrain_df\ntrain_data # impute missing values \ntraining # create the dummies \nfinal_train\n```\n\n```{python}\ncols = ['Age', 'Fare', 'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX = final_train[cols] # features vector\ny = final_train['Survived'] # Target vector \n```\n\n```{python}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\n```\n\n```{python}\n# Build the model \nmodel = LogisticRegression()\n# Create the RFE model \nrfe = RFE(estimator=model, n_features_to_select=8)\nrfe = rfe.fit(X ,y)\ndir(rfe)\n\n```\n\n```{python}\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))\n```\n\n```{python}\n# Getting the Regression Results \nimport statsmodels.api as sm\nlogit_model = sm.Logit(y,X.astype(float))\nresult = logit_model.fit()\nprint(result.summary2())\n```\n\n```{python}\ncols2 = ['Age',  'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX_alt = final_train[cols2] # features vector\nlogit_model2 = sm.Logit(y,sm.add_constant(X_alt.astype(float)))\nresult2 = logit_model2.fit()\nprint(result2.summary2())\n```\n\n```{python}\nfrom stargazer.stargazer import Stargazer\ntitanic_logit = Stargazer([result, result2])\ntitanic_logit\n```\n\n```{python}\n# Interpreting the coefficients, which are the log odds; therefore, we need to convert them into odds ratio. \nnp.exp(-2.5527)\n```\n\n```{python}\nnp.exp(result.params) # Getting the Odds Ratio of all Features \n```\n\n#### Feature Ranking with Recursive Feature Elimination and Cross-validation (`RFECV`)\n\n`RFECV` performs `RFE` in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation\n\n```{python}\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(max_iter = 5000), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\nfor x in dir(rfecv):\n  print(x)\n```\n\n```{python}\n# To get the accuracy\nrfecv.cv_results_\n```\n\n```{python}\ntype(rfecv.cv_results_)\n```\n\n```{python}\nrfecv.cv_results_\n\n```\n\n```{python}\nrfecv.n_features_\n```\n\n```{python}\nlist(X.columns[rfecv.support_])\n```\n\n```{python}\n# To check whether the RFE and RFECV generate the same features\nset(list(X.columns[rfecv.support_])) == set(list((X.columns[rfe.support_])))  \n```\n\n```{python}\nrfecv.cv_results_.keys()\n```\n\n```{python}\ncrossVal_results = rfecv.cv_results_\ncrossVal_results\ndel crossVal_results['mean_test_score']\ndel crossVal_results['std_test_score']\ncrossVal_results\n```\n\n## Correlation Matrix\n\n```{python}\nSelected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n                     'Embarked_S', 'Sex_male', 'IsMinor']\nX = final_train[Selected_features] # Recreated features vector \n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\") # for cmap = 'viridis' can also be used.\nplt.show()\n```\n\n```{python}\nplt.subplots(figsize=(8, 5))\nsns.heatmap(final_train[['Survived', 'Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_S', 'Sex_male', 'IsMinor']].corr(), annot = True, cmap = 'viridis')\nplt.show()\n```\n\n## Model Evaluation Procedures\n\n### Model Evaluation Based on Train/Test Split\n\n```{python}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n```\n\n```{python}\nX = final_train[Selected_features]\ny = final_train['Survived']\n```\n\n```{python}\n# use train/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n```\n\n```{python}\n# Check classification scores of Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test) # Prediction actual class\ny_pred_proba = logreg.predict_proba(X_test) [:,1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba)) \nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr)) \n\n```\n\n### Confusion Matrix\n\n```{python}\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n```\n\n```{python}\n# Visualizing Confusion Matrix\nplt.figure(figsize=(6,4))\ncm_matrix = pd.DataFrame(data= cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()\n```\n\n### Classification Report\n\n```{python}\nprint (classification_report(y_test, y_pred))\n```\n\n### ROC-AUC Curve\n\n```{python}\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility (true positive rate (tpr)) > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall/sensitivity)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))\n```\n\n## Model Evaluation Based on K-fold Cross-validation `cross_val_score()` Function\n\n```{python}\n# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression(max_iter=5000)\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())\n\n```\n\n### Model Evaluation Based on K-fold Cross-validation Using `cross_validate()` Function\n\n```{python}\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std())) \n```\n\nWhat happens when we add the feature `Fare` - \n\n```{python}\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"]\nX = final_train[cols]\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, final_train[cols], y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))\n```\nWe notice that the model is slightly deteriorated. The `Fare` variable does not carry any useful information. Its presence is just a noise for the logistic regression model.\n\n\n## `GridSearchCV` Evaluating Using Multiple Scorers Simultaneously\n\n\n\n\n\n```{python}\nfrom sklearn.model_selection import GridSearchCV\n\nX = final_train[Selected_features]\n\nparam_grid = {'C': np.arange(1e-05, 3, 0.1)}\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n\ngs = GridSearchCV(LogisticRegression(max_iter=5000), return_train_score=True,\n                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n\ngs.fit(X, y)\nresults = gs.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(gs.best_estimator_))\nprint(\"best params: \" + str(gs.best_params_))\nprint('best score:', gs.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, param_grid['C'].max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n```\n\n### `GridSearchCV` Evaluating Using Multiple scorers, `RepeatedStratifiedKFold` and `pipeline` for Preprocessing Simultaneously\n\n```{python}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\n\n#Define simple model\n###############################################################################\nC = np.arange(1e-05, 5.5, 0.1)\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\nlog_reg = LogisticRegression(max_iter=5000)\n\n#Simple pre-processing estimators\n###############################################################################\nstd_scale = StandardScaler(with_mean=False, with_std=False)\n#std_scale = StandardScaler()\n\n#Defining the CV method: Using the Repeated Stratified K Fold\n###############################################################################\n\nn_folds=5\nn_repeats=5\n\nrskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)\n\n#Creating simple pipeline and defining the gridsearch\n###############################################################################\n\nlog_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])\n\nlog_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,\n              scoring=scoring, return_train_score=True,\n              param_grid=dict(clf__C=C), refit='Accuracy')\n\nlog_clf.fit(X, y)\nresults = log_clf.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(log_clf.best_estimator_))\nprint(\"best params: \" + str(log_clf.best_params_))\nprint('best score:', log_clf.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, C.max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_clf__C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n```\n## Regularization \n\n`Regularization` is a method of preventing overfitting, which is a common problem in machine learning. Overfitting means that your model learns too much from the specific data you have, and fails to generalize well to new or unseen data. This can lead to poor predictions and low performance. Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables.\n\nSince the size of each coefficient depends on the scale of its corresponding variable, scaling the data is required so that the regularization penalizes each variable equally. The regularization strength is determined by `C` and as `C` increases, the regularization term becomes smaller (and for extremely large `C` values, it's as if there is no regularization at all).\n\nIf the initial model is overfit (as in, it fits the training data too well), then adding a strong regularization term (with small `C` value) makes the model perform worse for the training data, but introducing such \"noise\" improves the model's performance on unseen (or test) data.\n\nAn example with 1000 samples and 200 features shown below. As can be seen from the plot of accuracy over different values of C, if C is large (with very little regularization), there is a big gap between how the model performs on training data and test data. However, as C decreases, the model performs worse on training data but performs better on test data (test accuracy increases). However, when C becomes too small (or the regularization becomes too strong), the model begins performing worse again because now the regularization term completely dominates the objective function.\n\n\n\n```{python}\n# Necessary Python Packages \nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# make sample data\nX, y = make_classification(1000, 200, n_informative=195, random_state=2023)\n# split into train-test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2023)\n\n# normalize the data\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# train Logistic Regression models for different values of C\n# and collect train and test accuracies\nscores = {}\nfor C in (10**k for k in range(-6, 6)):\n    lr = LogisticRegression(C=C)\n    lr.fit(X_train, y_train)\n    scores[C] = {'train accuracy': lr.score(X_train, y_train), \n                 'test accuracy': lr.score(X_test, y_test)}\n\n# plot the accuracy scores for different values of C\npd.DataFrame.from_dict(scores, 'index').plot(logx=True, xlabel='C', ylabel='accuracy')\n```\n\n\n### Types of Regularization \n\n#### `L1` regularization\n\n`L1` regularization, also known as `Lasso regularization`, adds the sum of the absolute values of the model’s coefficients to the loss function. It encourages sparsity in the model by shrinking some coefficients to precisely zero. This has the effect of performing feature selection, as the model can effectively ignore irrelevant or less important features. L1 regularization is particularly useful when dealing with high-dimensional datasets with desired feature selection.\n\nMathematically, the `L1` regularization term can be written as:\n\n**`L1 regularization = λ * Σ|wi|`**\n\nHere, `λ` is the regularization parameter that controls the strength of regularization, `wi` represents the individual model coefficients and the sum is taken over all coefficients.\n\n\n#### `L2` regularization\n\n`L2` regularization, also known as `Ridge regularization`, adds the sum of the squared values of the model’s coefficients to the loss function. Unlike L1 regularization, L2 regularization does not force the coefficients to be exactly zero but instead encourages them to be small. L2 regularization can prevent overfitting by spreading the influence of a single feature across multiple features. It is advantageous when there are correlations between the input features.\n\nMathematically, the `L2` regularization term can be written as:\n\n**`L2 regularization = λ * Σ(wi^2)`**\n\nSimilar to `L1` regularization, `λ` is the regularization parameter, and `wi` represents the model coefficients. The sum is taken over all coefficients, and the squares of the coefficients are summed.\n\n\nThe choice between `L1` and `L2` regularization depends on the specific problem and the characteristics of the data. For example, `L1` regularization produces sparse models, which can be advantageous when feature selection is desired. `L2` regularization, on the other hand, encourages small but non-zero coefficients and can be more suitable when there are strong correlations between features.\n\n## Conclusion\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"toc-depth":5,"number-sections":true,"output-file":"chap04_logistic.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"solar","light":"flatly","linkcolor":"orangered","mainfont":"emoji","title":"<center> Chapter # 04 <br> Logistic Regression"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}