{"title":"<center> Chapter # 05 <br> Discriminant Analysis","markdown":{"yaml":{"title":"<center> Chapter # 05 <br> Discriminant Analysis","format":{"html":{"toc":true,"toc-title":"Table of Contents","toc-depth":5,"number-sections":true,"mainfont":"emoji"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n\n\n## Assumptions of Linear Discriminant Analysis \n\nDiscriminant analysis assumes that:\n\n  1. The data is normally distributed.\n  \n  2. Means of each class are specific to that class.\n  \n  3. All classes have a common covariance matrix. \n\nIf these assumptions are realized, DA generates a linear decision boundary.\n\n\n## Loading Python Packages\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Python version that is used for this code file is {}'.format(sys.version))\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\nprint ('The Panda version that is used for this code file is {}'.format(pd.__version__))\nprint ('The Numpy version that is used for this code file is {}'.format(np.__version__))\n```\n\n## Working Directory\n\n```{python}\n#| eval: false\nimport os\nos.getcwd()\n```\n\n```{python}\n#| eval: false\nfor x in os.listdir():\n  print (x)\n```\n\n## Importing Datasets\n\n```{python}\nfrom sklearn import datasets\ndataset = datasets.load_wine()\n```\n\n## Metadata of the Imported Dataset\n\n```{python}\ndataset.keys()\n```\n\n```{python}\ndataset['data']\n```\n\n```{python}\ndataset['target']\n```\n\n```{python}\ndataset['target_names']\n```\n\n```{python}\n# Creating Data frame from the array \ndata = pd.DataFrame(dataset['data'], columns = dataset['feature_names'])\ndata.head()\n```\n\n```{python}\n# Feature Vector \nfeatures_df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n# Target Vector \ntarget_df = pd.Categorical.from_codes(dataset.target, dataset.target_names)\n```\n\n```{python}\ntarget_df\n```\n\n```{python}\n# Joining the above two datasets \ndf = features_df.join(pd.Series(target_df, name = 'class'))\ndf.head()\n```\n\n```{python}\ndf.info()\n```\n\n```{python}\ndf.columns\nnum_features= dataset.feature_names\nnum_features\n# Looping functions \ndf.groupby('class')[num_features].mean().transpose()\n```\n\n## Analysis of Variance (`ANOVA`)\n\nOne-way ANOVA (also known as \"Analysis of Variance\") is a test that is used to find out whether there exists a statistically significant difference between the mean values of more than one group.\n\nSee @fig-corr to know when you should use which correlation\n\n::: {#fig-corr}\n![](images/corr.webp){fig-align=\"center\"}\n\nWhen You Should Use Which Correlation?\n:::\n\nA one-way ANOVA has the below given null and alternative hypotheses:\n\n**H~0~ (Null hypothesis)**: `μ1 = μ2 = μ3 = … = μk` (It implies that the means of all the population are equal)\n\n**H~1~ (Alternate hypothesis)**: It states that there will be at least one population mean that differs from the rest\n\n```{python}\ndataset.target_names\nalc_class0 = df[df['class']=='class_0']['alcohol']\ntype(alc_class0)\nalc_class1 = df[df['class']=='class_1']['alcohol']\nalc_class2 = df[df['class']=='class_2']['alcohol']\n\n\nfrom scipy.stats import f_oneway\nf_oneway(alc_class0,alc_class1,alc_class2)\n```\n\nThe `F statistic` is `135.0776` and `p-value` is `0.000`. Since the p-value is less than 0.05, we reject Null Hypothesis (**H~0~**). The findings imply that there exists a difference between three groups for the variable `alcohol`.\n\nSo, where does the difference come from? We can perform `Post Hoc` Analysis to check where does the differences come from\n\n```{python}\nimport statsmodels.api as sm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\ntukey = pairwise_tukeyhsd(endog=df['alcohol'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey.summary()\n```\n\nSee @fig-tukey for the sources of differences of target variable and `alcohol`\n\n```{python}\n#| label: fig-tukey\n#| fig-cap: Where Does the Difference Come From between Target variable and variable alcohol?\ntukey.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n```\n\n### Using `ANOVA` for Feature Selection\n\n```{python}\nlist(dataset.target_names)\n```\n\n```{python}\ntukey_malic = pairwise_tukeyhsd(endog=df['malic_acid'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey_malic.summary()\n```\n\nSee @fig-tukey2 for the difference between target variable and `malic_acid`\n\n```{python}\n#| label: fig-tukey2\n#| fig-cap: Where Does the Difference Come From between Target variable and variable malic_acid? \ntukey_malic.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n```\n\n```{python}\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ndf.rename(columns = {\n  'od280/od315_of_diluted_wines': 'diluted_wines'}, inplace = True)\n```\n\n```{python}\n#| eval: false\n#| echo: false\n#| include: false\n# To calculate the anova results for all variables \nkeys = []\ntables = []\nfor variable in df.columns:\n  model = ols('{} ~ class'.format(variable), data = df).fit()\n  anova_table = sm.stats.anova_lm(model, typ=2)\n  keys.append(variable)\n  tables.append(anova_table)\n  \n  \nkeys\ndf_anova = pd.concat(tables, keys = keys, axis = 0)\ndf_anova\n```\n\n## Linear Discriminant Analysis\n\n```{python}\nX = dataset.data\ny = dataset.target\ntarget_names = dataset.target_names\n```\n\n```{python}\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n```\n\n```{python}\nlda = LinearDiscriminantAnalysis(n_components = 2)\n```\n\n```{python}\nX_r2 = lda.fit(X,y).transform(X)\n```\n\n```{python}\nX_r2[0:10,]\n```\n\n```{python}\nlda.explained_variance_ratio_\n```\n\n### Plotting the Dataset\n\n```{python}\nplt.figure(figsize = (15,8))\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target,cmap = 'gnuplot', alpha = 0.7)\nplt.xlabel('DF1')\nplt.ylabel('DF2')\nplt.show()\n```\n\n### Distribution of LDA Components\n\n```{python}\ndf_lda = pd.DataFrame(zip(X_r2[:,0], X_r2[:,1],y), columns = [\"ld1\", \"ld2\", \"class\"])\nsns.set(rc={'figure.figsize':(12,8)})\nplt.subplot(2,1,1)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld1')\nplt.subplot(2,1,2)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld2')\nplt.show()\n```\n\n## Using LDA to Solve Classification Problem\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 2024)\n```\n\n### Training the Model\n\n```{python}\nlda_model = LinearDiscriminantAnalysis(n_components = 2)\nlda_model.fit(X_train, y_train)\n```\n\n### Testing the Model\n\n```{python}\ny_pred = lda_model.predict(X_test)\n\n```\n\n### Checking Model Accuracy\n\n```{python}\nfrom sklearn.metrics import accuracy_score\nprint (\"The Accuracy of LDA Model is %0.2f%%.\" % (accuracy_score(y_test, y_pred)*100))\n```\n\n```{python}\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\nplt.show()\n```\n\n## Cross Validation\n\n```{python}\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n```\n\n```{python}\n#| eval: false\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 50, random_state = 1)\nscores = cross_val_score(lda_model, X,y, scoring = \"accuracy\", cv = cv, n_jobs = -1)\nprint(np.mean(scores))\n```\n\n## LDA vs PCA (Visualization Difference)\n\n### PCA Model\n\n```{python}\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_pca = pca.fit(X).transform(X)\n```\n\n```{python}\nfrom pylab import *\nsubplot(2,1,1)\ntitle (\"PCA\")\nplt.scatter(X_pca[:,0], X_pca[:,1], c = dataset.target, cmap = \"gnuplot\")\nsubplot(2,1,2)\ntitle (\"LDA\")\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target, cmap = \"gnuplot\")\nplt.show()\n```\n\nBoth algorithms have successfully reduced the components but created different clusters because both have reduced the components based on different principles.\n\nNow let's also visualize and compare the distributions of each of the algorithms on their respective components. Here we will visualize the distribution of the first component of each algorithm (LDA-1 and PCA-1).\n\n```{python}\n# creating dataframs\ndf=pd.DataFrame(zip(X_pca[:,0],X_r2[:,0],y),columns=[\"pc1\",\"ld1\",\"class\"])\n# plotting the lda1\nplt.subplot(2,1,1)\nsns.boxplot(x='class', y='ld1', data=df)\n# plotting pca1\nplt.subplot(2,1,2)\nsns.boxplot(x='class', y='pc1', data=df)\nplt.show()\n```\n\nThere is a slight difference in the distribution of both of the algorithms. For example, the PCA result shows outliers only at the first target variable, whereas the LDA result contains outliers for every target variable.\n\n## Variance Covariance Matrix\n\nTo calculate the covariance matrix in Python using NumPy, you can import NumPy as np, create or load your data as a NumPy array, subtract the mean of each column from the data, transpose the array, multiply the transposed array and original array, divide the multiplied array by the number of observations, and print the array. Alternatively, you can use the `np.cov` function which takes the data array as an input and returns the covariance matrix as an output.\n\nTo learn more about [variance-covariance matrix.](https://builtin.com/data-science/covariance-matrix)\n\nTo learn more about [eigenvalues and eigenvectors.](https://github.com/learn-co-students/dsc-3-34-07-pca-implementation-visualization-python-numpy-lab-seattle-ds-career-040119)\n\n```{python}\nA = [45, 37, 42, 35, 39]\nB = [38, 31, 26, 28, 33]\nC = [10, 15, 17, 21, 12]\n\ndata = np.array([A, B, C])\n\ncov_matrix = np.cov(data, bias=True)\nprint(cov_matrix)\n```\n```{python}\nnp.var(A)\n```\n\n\n```{python}\nnp.var(C)\n```\n\n\n### Eigenvalues and Eigenvector for Variance-covariance Matrix\n\n```{python}\n# eigendecomposition\nfrom numpy.linalg import eig\n```\n\n\n\n```{python}\n# calculate eigendecomposition\nvalues, vectors = eig(cov_matrix)\n# Eigenvalues \nprint(values)\n\n```\n\n```{python}\n# Eigenvectors \nprint(vectors)\n```\n\n## Regularized Discriminant Analysis\n\nSince regularization techniques have been highly successful in the solution of ill-posed and poorly-posed inverse problems so to mitigate this problem the most reliable way is to use the regularization technique. \n\n  * A poorly posed problem occurs when the number of parameters to be estimated is comparable to the number of observations.\n  \n  * Similarly,ill-posed if that number exceeds the sample size.\n  \nIn these cases the parameter estimates can be highly unstable, giving rise to high variance. Regularization would help to improve the estimates by shifting them away from their sample-based values towards values that are more physically valid; this would be achieved by applying shrinkage to each class. \n\nWhile regularization reduces the variance associated with the sample-based estimate, it may also increase bias. This process known as bias-variance trade-off is generally controlled by one or more degree-of-belief parameters that determine how strongly biasing towards “plausible” values of population parameters takes place.\n\nWhenever the sample size is not significantly greater than the dimension of measurement space for any class, Quantitative discriminant analysis (QDA) is ill-posed. Typically, `regularization is applied to a discriminant analysis by replacing the individual class sample covariance matrices with the average weights assigned to the eigenvalues`. \n\nThis applies a considerable degree of regularization by substantially reducing the number of parameters to be estimated. `The regularization parameter () which is added to the equation of QDA and LDA takes a value between 0 to 1`. It controls the degree of shrinkage of the individual class covariance matrix estimates toward the pooled estimate. Values between these limits represent degrees of regularization.\n\n```{python}\nfrom sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,confusion_matrix\nfrom imblearn.over_sampling import SMOTE # To install module, run this line of code - pip install imblearn\nfrom sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n```\n\n```{python}\n# Reading a new dataset \ndf = pd.read_csv('DATA/healthcare-dataset-stroke-data.csv')\nprint(\"Records = \", df.shape[0], \"\\nFeatures = \", df.shape[1])\n```\n\n```{python}\ndf.sample(5)\n```\n```{python}\ndf.info()\n```\n\n```{python}\n# Missing Values \n(df.isnull().sum()/len(df)*100)\n```\n\n```{python}\n# Dropping the Missing Observations\ndf.dropna(axis = 0, inplace = True)\ndf.shape\n```\n\n```{python}\n# Creating the Dummies \ndf_pre = pd.get_dummies(df, drop_first = True)\ndf_pre.sample(5)\n```\n\n```{python}\n# Training and Testing the Split \nX = df_pre.drop(['stroke'], axis = 1)\ny = df_pre['stroke']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 25)\n```\n\n```{python}\n# Building the LDA\nLDA = LinearDiscriminantAnalysis()\nLDA.fit_transform(X_train, y_train)\nX_test['predictions'] = LDA.predict(X_test)\nConfusionMatrixDisplay.from_predictions(y_test, X_test['predictions'])\nplt.show()\n```\n```{python}\ntn, fp, fn, tp = confusion_matrix(list(y_test), list(X_test['predictions']), labels=[0, 1]).ravel()\n```\n\n\n```{python}\nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\nprint(\"Precision score\",precision_score(y_test,X_test['predictions']))\n```\nIt has only 32% precision rate, which is very poor performance.\n\n\n```{python}\nprint(\"Accuracy Score\",accuracy_score(y_test,X_test['predictions']))\n```\nThe accuracy is approximately 95%, but the precision is 32%. \n\n### Cross Validation of the Dataset \n\n```{python}\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n```\n\n```{python}\n#Define method to evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=50, random_state=1)\n\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n```\n\n```{python}\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n```\nEven after cross validation, the precision is about 24% and the accuracy is 95% approximately. There is no significant improvement of the metrics of the model. \n\n## Regularizing and Shrinking the LDA\n\n```{python}\ndf_pre['stroke'].value_counts()\n```\nAs observed by the value count of the dependent variable the data is imbalanced as the quantity of 1’s is approx 4% of the total dependent variable. So, it needs to be balanced for the learner to be a good predictor.\n\n### Balancing the Dependent Variable \n\nThere are two ways by which the data can be synthesized: one by oversampling and the second, by undersampling. In this scenario, oversampling is better which will synthesize the lesser category linear interpolation.\n\n\n```{python}\noversample = SMOTE()\nX_smote, y_smote = oversample.fit_resample(X, y)\nXs_train, Xs_test, ys_train, ys_test = train_test_split(X_smote, y_smote, test_size=0.30, random_state=42)\n```\n\nThe imbalance is mitigated by using the `Synthetic Minority Oversampling Technique (SMOTE)` but this will not help much we also need to regularize the leaner by using the `GridSearchCV` which will find the best parameters for the learner and add a penalty to the solver which will shrink the eigenvalue i.e regularization.\n\n```{python}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\ngrid = dict()\ngrid['solver'] = ['eigen','lsqr']\ngrid['shrinkage'] = ['auto',0.2,1,0.3,0.5]\nsearch = GridSearchCV(LDA, grid, scoring='precision', cv=cv, n_jobs=-1)\nresults = search.fit(Xs_train, ys_train)\nprint('Precision: %.3f' % results.best_score_)\nprint('Configuration:',results.best_params_)\n```\n\nThe precision score jumped right from 35% to 87% with the help of regularization and shrinkage of the learner and the best solver for the Linear Discriminant Analysis is `eigen` and the shrinkage method is `auto` which uses the Ledoit-Wolf lemma for finding the shrinkage penalty. \n\n## Building the Regularized Discriminant Analysis (RDA)\n\n```{python}\n# Build the RDA\nLDA_final=LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')\nLDA_final.fit_transform(Xs_train,ys_train)\nXs_test['predictions']=LDA_final.predict(Xs_test)\nConfusionMatrixDisplay.from_predictions(ys_test, Xs_test['predictions'])\nplt.show()\n \ntn, fp, fn, tp = confusion_matrix(list(ys_test), list(Xs_test['predictions']), labels=[0, 1]).ravel()\n \nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\n```\n\n\n```{python}\nprint(\"Precision score\",np.round(precision_score(ys_test,Xs_test['predictions']),3))\n```\n\n```{python}\nprint(\"Accuracy score\",np.round(accuracy_score(ys_test,Xs_test['predictions']),3))\n```\n\n## Conclusion \n","srcMarkdownNoYaml":"\n\n## Introduction\n\n\n\n## Assumptions of Linear Discriminant Analysis \n\nDiscriminant analysis assumes that:\n\n  1. The data is normally distributed.\n  \n  2. Means of each class are specific to that class.\n  \n  3. All classes have a common covariance matrix. \n\nIf these assumptions are realized, DA generates a linear decision boundary.\n\n\n## Loading Python Packages\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Python version that is used for this code file is {}'.format(sys.version))\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\nprint ('The Panda version that is used for this code file is {}'.format(pd.__version__))\nprint ('The Numpy version that is used for this code file is {}'.format(np.__version__))\n```\n\n## Working Directory\n\n```{python}\n#| eval: false\nimport os\nos.getcwd()\n```\n\n```{python}\n#| eval: false\nfor x in os.listdir():\n  print (x)\n```\n\n## Importing Datasets\n\n```{python}\nfrom sklearn import datasets\ndataset = datasets.load_wine()\n```\n\n## Metadata of the Imported Dataset\n\n```{python}\ndataset.keys()\n```\n\n```{python}\ndataset['data']\n```\n\n```{python}\ndataset['target']\n```\n\n```{python}\ndataset['target_names']\n```\n\n```{python}\n# Creating Data frame from the array \ndata = pd.DataFrame(dataset['data'], columns = dataset['feature_names'])\ndata.head()\n```\n\n```{python}\n# Feature Vector \nfeatures_df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n# Target Vector \ntarget_df = pd.Categorical.from_codes(dataset.target, dataset.target_names)\n```\n\n```{python}\ntarget_df\n```\n\n```{python}\n# Joining the above two datasets \ndf = features_df.join(pd.Series(target_df, name = 'class'))\ndf.head()\n```\n\n```{python}\ndf.info()\n```\n\n```{python}\ndf.columns\nnum_features= dataset.feature_names\nnum_features\n# Looping functions \ndf.groupby('class')[num_features].mean().transpose()\n```\n\n## Analysis of Variance (`ANOVA`)\n\nOne-way ANOVA (also known as \"Analysis of Variance\") is a test that is used to find out whether there exists a statistically significant difference between the mean values of more than one group.\n\nSee @fig-corr to know when you should use which correlation\n\n::: {#fig-corr}\n![](images/corr.webp){fig-align=\"center\"}\n\nWhen You Should Use Which Correlation?\n:::\n\nA one-way ANOVA has the below given null and alternative hypotheses:\n\n**H~0~ (Null hypothesis)**: `μ1 = μ2 = μ3 = … = μk` (It implies that the means of all the population are equal)\n\n**H~1~ (Alternate hypothesis)**: It states that there will be at least one population mean that differs from the rest\n\n```{python}\ndataset.target_names\nalc_class0 = df[df['class']=='class_0']['alcohol']\ntype(alc_class0)\nalc_class1 = df[df['class']=='class_1']['alcohol']\nalc_class2 = df[df['class']=='class_2']['alcohol']\n\n\nfrom scipy.stats import f_oneway\nf_oneway(alc_class0,alc_class1,alc_class2)\n```\n\nThe `F statistic` is `135.0776` and `p-value` is `0.000`. Since the p-value is less than 0.05, we reject Null Hypothesis (**H~0~**). The findings imply that there exists a difference between three groups for the variable `alcohol`.\n\nSo, where does the difference come from? We can perform `Post Hoc` Analysis to check where does the differences come from\n\n```{python}\nimport statsmodels.api as sm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\ntukey = pairwise_tukeyhsd(endog=df['alcohol'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey.summary()\n```\n\nSee @fig-tukey for the sources of differences of target variable and `alcohol`\n\n```{python}\n#| label: fig-tukey\n#| fig-cap: Where Does the Difference Come From between Target variable and variable alcohol?\ntukey.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n```\n\n### Using `ANOVA` for Feature Selection\n\n```{python}\nlist(dataset.target_names)\n```\n\n```{python}\ntukey_malic = pairwise_tukeyhsd(endog=df['malic_acid'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey_malic.summary()\n```\n\nSee @fig-tukey2 for the difference between target variable and `malic_acid`\n\n```{python}\n#| label: fig-tukey2\n#| fig-cap: Where Does the Difference Come From between Target variable and variable malic_acid? \ntukey_malic.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n```\n\n```{python}\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ndf.rename(columns = {\n  'od280/od315_of_diluted_wines': 'diluted_wines'}, inplace = True)\n```\n\n```{python}\n#| eval: false\n#| echo: false\n#| include: false\n# To calculate the anova results for all variables \nkeys = []\ntables = []\nfor variable in df.columns:\n  model = ols('{} ~ class'.format(variable), data = df).fit()\n  anova_table = sm.stats.anova_lm(model, typ=2)\n  keys.append(variable)\n  tables.append(anova_table)\n  \n  \nkeys\ndf_anova = pd.concat(tables, keys = keys, axis = 0)\ndf_anova\n```\n\n## Linear Discriminant Analysis\n\n```{python}\nX = dataset.data\ny = dataset.target\ntarget_names = dataset.target_names\n```\n\n```{python}\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n```\n\n```{python}\nlda = LinearDiscriminantAnalysis(n_components = 2)\n```\n\n```{python}\nX_r2 = lda.fit(X,y).transform(X)\n```\n\n```{python}\nX_r2[0:10,]\n```\n\n```{python}\nlda.explained_variance_ratio_\n```\n\n### Plotting the Dataset\n\n```{python}\nplt.figure(figsize = (15,8))\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target,cmap = 'gnuplot', alpha = 0.7)\nplt.xlabel('DF1')\nplt.ylabel('DF2')\nplt.show()\n```\n\n### Distribution of LDA Components\n\n```{python}\ndf_lda = pd.DataFrame(zip(X_r2[:,0], X_r2[:,1],y), columns = [\"ld1\", \"ld2\", \"class\"])\nsns.set(rc={'figure.figsize':(12,8)})\nplt.subplot(2,1,1)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld1')\nplt.subplot(2,1,2)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld2')\nplt.show()\n```\n\n## Using LDA to Solve Classification Problem\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 2024)\n```\n\n### Training the Model\n\n```{python}\nlda_model = LinearDiscriminantAnalysis(n_components = 2)\nlda_model.fit(X_train, y_train)\n```\n\n### Testing the Model\n\n```{python}\ny_pred = lda_model.predict(X_test)\n\n```\n\n### Checking Model Accuracy\n\n```{python}\nfrom sklearn.metrics import accuracy_score\nprint (\"The Accuracy of LDA Model is %0.2f%%.\" % (accuracy_score(y_test, y_pred)*100))\n```\n\n```{python}\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\nplt.show()\n```\n\n## Cross Validation\n\n```{python}\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n```\n\n```{python}\n#| eval: false\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 50, random_state = 1)\nscores = cross_val_score(lda_model, X,y, scoring = \"accuracy\", cv = cv, n_jobs = -1)\nprint(np.mean(scores))\n```\n\n## LDA vs PCA (Visualization Difference)\n\n### PCA Model\n\n```{python}\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_pca = pca.fit(X).transform(X)\n```\n\n```{python}\nfrom pylab import *\nsubplot(2,1,1)\ntitle (\"PCA\")\nplt.scatter(X_pca[:,0], X_pca[:,1], c = dataset.target, cmap = \"gnuplot\")\nsubplot(2,1,2)\ntitle (\"LDA\")\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target, cmap = \"gnuplot\")\nplt.show()\n```\n\nBoth algorithms have successfully reduced the components but created different clusters because both have reduced the components based on different principles.\n\nNow let's also visualize and compare the distributions of each of the algorithms on their respective components. Here we will visualize the distribution of the first component of each algorithm (LDA-1 and PCA-1).\n\n```{python}\n# creating dataframs\ndf=pd.DataFrame(zip(X_pca[:,0],X_r2[:,0],y),columns=[\"pc1\",\"ld1\",\"class\"])\n# plotting the lda1\nplt.subplot(2,1,1)\nsns.boxplot(x='class', y='ld1', data=df)\n# plotting pca1\nplt.subplot(2,1,2)\nsns.boxplot(x='class', y='pc1', data=df)\nplt.show()\n```\n\nThere is a slight difference in the distribution of both of the algorithms. For example, the PCA result shows outliers only at the first target variable, whereas the LDA result contains outliers for every target variable.\n\n## Variance Covariance Matrix\n\nTo calculate the covariance matrix in Python using NumPy, you can import NumPy as np, create or load your data as a NumPy array, subtract the mean of each column from the data, transpose the array, multiply the transposed array and original array, divide the multiplied array by the number of observations, and print the array. Alternatively, you can use the `np.cov` function which takes the data array as an input and returns the covariance matrix as an output.\n\nTo learn more about [variance-covariance matrix.](https://builtin.com/data-science/covariance-matrix)\n\nTo learn more about [eigenvalues and eigenvectors.](https://github.com/learn-co-students/dsc-3-34-07-pca-implementation-visualization-python-numpy-lab-seattle-ds-career-040119)\n\n```{python}\nA = [45, 37, 42, 35, 39]\nB = [38, 31, 26, 28, 33]\nC = [10, 15, 17, 21, 12]\n\ndata = np.array([A, B, C])\n\ncov_matrix = np.cov(data, bias=True)\nprint(cov_matrix)\n```\n```{python}\nnp.var(A)\n```\n\n\n```{python}\nnp.var(C)\n```\n\n\n### Eigenvalues and Eigenvector for Variance-covariance Matrix\n\n```{python}\n# eigendecomposition\nfrom numpy.linalg import eig\n```\n\n\n\n```{python}\n# calculate eigendecomposition\nvalues, vectors = eig(cov_matrix)\n# Eigenvalues \nprint(values)\n\n```\n\n```{python}\n# Eigenvectors \nprint(vectors)\n```\n\n## Regularized Discriminant Analysis\n\nSince regularization techniques have been highly successful in the solution of ill-posed and poorly-posed inverse problems so to mitigate this problem the most reliable way is to use the regularization technique. \n\n  * A poorly posed problem occurs when the number of parameters to be estimated is comparable to the number of observations.\n  \n  * Similarly,ill-posed if that number exceeds the sample size.\n  \nIn these cases the parameter estimates can be highly unstable, giving rise to high variance. Regularization would help to improve the estimates by shifting them away from their sample-based values towards values that are more physically valid; this would be achieved by applying shrinkage to each class. \n\nWhile regularization reduces the variance associated with the sample-based estimate, it may also increase bias. This process known as bias-variance trade-off is generally controlled by one or more degree-of-belief parameters that determine how strongly biasing towards “plausible” values of population parameters takes place.\n\nWhenever the sample size is not significantly greater than the dimension of measurement space for any class, Quantitative discriminant analysis (QDA) is ill-posed. Typically, `regularization is applied to a discriminant analysis by replacing the individual class sample covariance matrices with the average weights assigned to the eigenvalues`. \n\nThis applies a considerable degree of regularization by substantially reducing the number of parameters to be estimated. `The regularization parameter () which is added to the equation of QDA and LDA takes a value between 0 to 1`. It controls the degree of shrinkage of the individual class covariance matrix estimates toward the pooled estimate. Values between these limits represent degrees of regularization.\n\n```{python}\nfrom sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,confusion_matrix\nfrom imblearn.over_sampling import SMOTE # To install module, run this line of code - pip install imblearn\nfrom sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n```\n\n```{python}\n# Reading a new dataset \ndf = pd.read_csv('DATA/healthcare-dataset-stroke-data.csv')\nprint(\"Records = \", df.shape[0], \"\\nFeatures = \", df.shape[1])\n```\n\n```{python}\ndf.sample(5)\n```\n```{python}\ndf.info()\n```\n\n```{python}\n# Missing Values \n(df.isnull().sum()/len(df)*100)\n```\n\n```{python}\n# Dropping the Missing Observations\ndf.dropna(axis = 0, inplace = True)\ndf.shape\n```\n\n```{python}\n# Creating the Dummies \ndf_pre = pd.get_dummies(df, drop_first = True)\ndf_pre.sample(5)\n```\n\n```{python}\n# Training and Testing the Split \nX = df_pre.drop(['stroke'], axis = 1)\ny = df_pre['stroke']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 25)\n```\n\n```{python}\n# Building the LDA\nLDA = LinearDiscriminantAnalysis()\nLDA.fit_transform(X_train, y_train)\nX_test['predictions'] = LDA.predict(X_test)\nConfusionMatrixDisplay.from_predictions(y_test, X_test['predictions'])\nplt.show()\n```\n```{python}\ntn, fp, fn, tp = confusion_matrix(list(y_test), list(X_test['predictions']), labels=[0, 1]).ravel()\n```\n\n\n```{python}\nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\nprint(\"Precision score\",precision_score(y_test,X_test['predictions']))\n```\nIt has only 32% precision rate, which is very poor performance.\n\n\n```{python}\nprint(\"Accuracy Score\",accuracy_score(y_test,X_test['predictions']))\n```\nThe accuracy is approximately 95%, but the precision is 32%. \n\n### Cross Validation of the Dataset \n\n```{python}\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n```\n\n```{python}\n#Define method to evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=50, random_state=1)\n\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n```\n\n```{python}\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n```\nEven after cross validation, the precision is about 24% and the accuracy is 95% approximately. There is no significant improvement of the metrics of the model. \n\n## Regularizing and Shrinking the LDA\n\n```{python}\ndf_pre['stroke'].value_counts()\n```\nAs observed by the value count of the dependent variable the data is imbalanced as the quantity of 1’s is approx 4% of the total dependent variable. So, it needs to be balanced for the learner to be a good predictor.\n\n### Balancing the Dependent Variable \n\nThere are two ways by which the data can be synthesized: one by oversampling and the second, by undersampling. In this scenario, oversampling is better which will synthesize the lesser category linear interpolation.\n\n\n```{python}\noversample = SMOTE()\nX_smote, y_smote = oversample.fit_resample(X, y)\nXs_train, Xs_test, ys_train, ys_test = train_test_split(X_smote, y_smote, test_size=0.30, random_state=42)\n```\n\nThe imbalance is mitigated by using the `Synthetic Minority Oversampling Technique (SMOTE)` but this will not help much we also need to regularize the leaner by using the `GridSearchCV` which will find the best parameters for the learner and add a penalty to the solver which will shrink the eigenvalue i.e regularization.\n\n```{python}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\ngrid = dict()\ngrid['solver'] = ['eigen','lsqr']\ngrid['shrinkage'] = ['auto',0.2,1,0.3,0.5]\nsearch = GridSearchCV(LDA, grid, scoring='precision', cv=cv, n_jobs=-1)\nresults = search.fit(Xs_train, ys_train)\nprint('Precision: %.3f' % results.best_score_)\nprint('Configuration:',results.best_params_)\n```\n\nThe precision score jumped right from 35% to 87% with the help of regularization and shrinkage of the learner and the best solver for the Linear Discriminant Analysis is `eigen` and the shrinkage method is `auto` which uses the Ledoit-Wolf lemma for finding the shrinkage penalty. \n\n## Building the Regularized Discriminant Analysis (RDA)\n\n```{python}\n# Build the RDA\nLDA_final=LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')\nLDA_final.fit_transform(Xs_train,ys_train)\nXs_test['predictions']=LDA_final.predict(Xs_test)\nConfusionMatrixDisplay.from_predictions(ys_test, Xs_test['predictions'])\nplt.show()\n \ntn, fp, fn, tp = confusion_matrix(list(ys_test), list(Xs_test['predictions']), labels=[0, 1]).ravel()\n \nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\n```\n\n\n```{python}\nprint(\"Precision score\",np.round(precision_score(ys_test,Xs_test['predictions']),3))\n```\n\n```{python}\nprint(\"Accuracy score\",np.round(accuracy_score(ys_test,Xs_test['predictions']),3))\n```\n\n## Conclusion \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"toc-depth":5,"number-sections":true,"output-file":"chap05_discriminant.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"solar","light":"flatly","title":"<center> Chapter # 05 <br> Discriminant Analysis","toc-title":"Table of Contents","mainfont":"emoji"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}