{"title":"<center> Chapter # 10 <br> K-means Clustering","markdown":{"yaml":{"title":"<center> Chapter # 10 <br> K-means Clustering","format":"html"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nClustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.\n\n# Application of Clustering in Real World\n\nClustering is a widely used technique in the industry. It is being used in almost every domain, from banking and recommendation engines to document clustering and image segmentation.\n\n-   **Customer Segmentation** -\n\nOne of the most common applications of clustering is customer segmentation. And it isn't just limited to banking. This strategy is across functions, including telecom, e-commerce, sports, advertising, sales, etc.\n\n-   **Document Clustering** -\n\nAnother common application of clustering. Let's say you have multiple documents and you need to cluster similar documents together. Clustering helps us group these documents such that similar documents are in the same clusters.\n\n-   **Image Segmentation** -\n\nWe can also use clustering to perform image segmentation. In image segmentation, we try to club similar pixels in the image together. We can apply clustering to create clusters having similar pixels in the same group.\n\n-   **Recommendation Engines** -\n\nClustering can also be used in recommendation engines. Let's say you want to recommend songs to your friends. You can look at the songs liked by that person and then use clustering to find similar songs and finally recommend the most similar songs.\n\n# K-means Clustering\n\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms. You'll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster. Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares. In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The 'means' in the K-means refers to averaging of the data; that is, finding the centroid. Please see @fig-kmeanscluster to get an idea about K-means clustering.\n\n::: {#fig-kmeanscluster}\n![](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning.png)\n\nAn Example of K-means Clustering\n:::\n\n# Steps in K-Means Algorithm\n\n1.  Choose the number of clusters K.\n2.  Select at random K points, the centroids(not necessarily from your dataset).\n3.  Assign each data point to the closest centroid â†’ that forms K clusters.\n4.  Compute the mean of data points within a cluster and place the new centroid of each cluster.\n5.  Reassign each data point to the new closest centroid. If any reassignment took place, go to step 4, otherwise, the model is ready.\n\n# How to Choose the Value of K?\n\nThere are different techniques available to find the optimal value of K. The most common technique is the **elbow method.**\n\n## Elbow Method\n\nThe elbow method is used to determine the optimal number of clusters in K-means clustering. The elbow method plots the value of the Within cluster sum of square of the data points(WCSS) produced by different values of K. @fig-elbowmethod shows how the elbow method works.\n\n**Distortion**: It is calculated as the average of the squared distances from the cluster centers of the respective clusters to each data point. Typically, the Euclidean distance metric is used.\n\n$$\n Distortion = 1/n * \\sum(distance(point, centroid)^2)\n $$\n\n**Inertia**: It is the sum of the squared distances of samples to their closest cluster center.\n\n$$\nInertia = \\sum(distance(point, centroid)^2)\n$$\n\n::: {#fig-elbowmethod}\n![](elbowmethod.png)\n\nElbow Method for Optimal K\n:::\n\nWe can see that if K increases, average distortion will decrease. We should choose that k where the distortion decline drastically. From the above diagram the best value of k will be 3.\n\n## Silhouette Score\n\nAnother method to find the optimal value of K is **silhouette score**. The silhouette score and plot are used to evaluate the quality of a clustering solution produced by the k-means algorithm. The silhouette score measures -\n\na)  How close the data point is to other points in the cluster\n\nb)  How far away the data point is from points in other clusters\n\nSilhouette coefficient values range between -1 and 1. Larger numbers indicate that samples are closer to their clusters than they are to other clusters. A silhouette score close to 0 suggests overlapping clusters, and a negative score suggests poor clustering solutions.\n\n# When will K-means Clustering Fail?\n\nK-means clustering performs best on data that are spherical. Spherical data are data that group in space in close proximity to each other either. This can be visualized in 2 or 3 dimensional space more easily. Data that aren't spherical or should not be spherical do not work well with k-means clustering. For example, k-means clustering would not do well on the below data (@fig-kmeansNOT) as we would not be able to find distinct centroids to cluster the two circles or arcs differently, despite them clearly visually being two distinct circles and arcs that should be labeled as such.\n\n::: {#fig-kmeansNOT}\n![](kmeansNOTworking.png)\n\nWhen K means is not working\n:::\n\n# Importing Python Libraries\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\n\nfrom scipy.spatial.distance import cdist\nfrom sklearn import metrics\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n```\n\n# An Example of Clustering Using a Simulated Dataset\n\n```{python}\n# Creating the data\nx1 = np.array([3, 1, 1, 2, 1, 6, 6, 6, 5, 6,\\\n               7, 8, 9, 8, 9, 9, 8, 4, 4, 5, 4])\nx2 = np.array([5, 4, 5, 6, 5, 8, 6, 7, 6, 7, \\\n               1, 2, 1, 2, 3, 2, 3, 9, 10, 9, 10])\nX = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\n \n# Visualizing the data\nsns.set_style('whitegrid')\nplt.plot()\nplt.xlim([0, 12])\nplt.ylim([0, 12])\n#plt.title('Dataset')\nplt.scatter(x1, x2)\nplt.show()\n```\n\n```{python}\ndistortions = []\ninertias = []\nmapping1 = {}\nmapping2 = {}\nK = range(1, 10)\n \nfor k in K:\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n \n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n                                        'euclidean'), axis=1)) / X.shape[0])\n    inertias.append(kmeanModel.inertia_)\n \n    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n                                   'euclidean'), axis=1)) / X.shape[0]\n    mapping2[k] = kmeanModel.inertia_\n```\n\nPlease see @fig-ditortion to see elbow method using distortion\n\n```{python}\n#| label: fig-ditortion\n#| fig-cap: The Elbow Method using Distortion\nsns.set_style('whitegrid')\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Distortion')\n#plt.title('The Elbow Method using Distortion')\nplt.show()\n```\n\nPlease see @fig-inertia to see elbow method using inertia\n\n```{python}\n#| label: fig-inertia\n#| fig-cap: The Elbow Method using Inertia\nsns.set_style('whitegrid')\nplt.plot(K, inertias, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Inertia')\n#plt.title('The Elbow Method using Inertia')\nplt.show()\n```\n\nPlease see @fig-silhouette for Silhouette Score.\n\n```{python}\n # A list holds the silhouette coefficients for each k\nkmeans_kwargs = {\n        \"init\": \"random\",\n        \"n_init\": 10,\n        \"max_iter\": 300,\n        \"random_state\": 42,\n    }\n   \nsilhouette_coefficients = []\n\n# Notice you start at 2 clusters for silhouette coefficient\nfrom sklearn.metrics import silhouette_score\n\nfor k in range(2, 11):\n   kmeans = KMeans(n_clusters=k,**kmeans_kwargs)\n   kmeans.fit(X)\n   score = silhouette_score(X, kmeans.labels_)\n   silhouette_coefficients.append(score)\n```\n\n```{python}\n#| label: fig-silhouette\n#| fig-cap: The Silhouette Score for Optimal K\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(2, 11), silhouette_coefficients)\nplt.xticks(range(2, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()\n```\n\n## Clustered Data Points For Different K Values\n\nPlease see @fig-differentk for different clusters based on different values of K\n\n```{python}\n#| layout-nrows: 4\n#| label: fig-differentk\n#| fig-cap: Clustering Using Different K Values\n#| fig-subcap: \n#|    - \"Number of Cluster K = 1\"\n#|    - \"Number of Cluster K = 2\"\n#|    - \"Number of Cluster K = 3\"\n#|    - \"Number of Cluster K = 4\"\n#|    - \"Elbow Method\"\n\n\n\n\n# Create a range of values for k\nk_range = range(1, 5)\n \n# Initialize an empty list to \n# store the inertia values for each k\ninertia_values = []\n \n# Fit and plot the data for each k value\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, \\\n                    init='k-means++', random_state=42)\n    y_kmeans = kmeans.fit_predict(X)\n    inertia_values.append(kmeans.inertia_)\n    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans)\n    plt.scatter(kmeans.cluster_centers_[:, 0],\\\n                kmeans.cluster_centers_[:, 1], \\\n                s=100, c='red')\n    #plt.title('K-means clustering (k={})'.format(k))\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.show()\n \n# Plot the inertia values for each k\nplt.plot(k_range, inertia_values, 'bo-')\n#plt.title('Elbow Method')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Inertia')\nplt.show()\n```\n\n# Loading Dataset\n\n```{python}\ndf = pd.read_csv('DATA/Mall_Customers.csv')\n```\n\n## Metadata of the Dataset\n\n```{python}\n# first few rows\ndf.head()\n```\n\n```{python}\n# size of datset\ndf.shape\n```\n\n```{python}\n# summary about dataset\ndf.info()\n```\n\n```{python}\ndf.isnull().sum()\n```\n\n```{python}\ndf.describe()\n```\n\n```{python}\ndf.describe().T\n```\n\n# Exploratory Analysis\n\n```{python}\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 15)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n```\n\n```{python}\nsns.pairplot(df, vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue = \"Gender\")\nplt.show()\n```\n\n# Clustering Based on Age and Spending Score\n\n```{python}\nsns.set_style('whitegrid')\nplt.figure(1 , figsize = (10 , 5))\nplt.title('Scatter plot of Age v/s Spending Score', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, s = 100)\nplt.show()\n```\n\n## Deciding `K` Value\n\n```{python}\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 15):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\n```\n\n```{python}\nplt.figure(1 , figsize = (10,5))\nplt.plot(np.arange(1 , 15) , inertia , 'o')\nplt.plot(np.arange(1 , 15) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n```\n\n-   The KMeans algorithm clusters data by trying to separate samples in n groups of equal variances, minimizing a criterion known as `inertia`, or `within-cluster sum-of-squares Inertia`, or the `within-cluster sum of squares criterion`, can be recognized as a measure of how internally coherent clusters are.\n\n-   The k-means algorithm divides a set of `N` samples into `K` disjoint clusters `C`, each described by the mean `j` of the samples in the cluster. The means are commonly called the cluster centroids.\n\n-   The K-means algorithm aims to choose centroids that **minimize** the inertia, or within-cluster sum of squared criterion.\n\n## Applying KMeans for `K = 4`\n\n```{python}\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n```\n\n```{python}\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n```\n\n```{python}\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n```\n\n## Applying KMeans for `K = 5`\n\n```{python}\nalgorithm = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111 , algorithm='elkan'))\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n```\n\n```{python}\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n```\n\n```{python}\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n```\n\n# Clustering Based on Annual Income and Spending Score\n\n```{python}\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X2)\n    inertia.append(algorithm.inertia_)\n```\n\n```{python}\nplt.figure(1 , figsize = (10,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n```\n\n```{python}\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X2)\nlabels2 = algorithm.labels_\ncentroids2 = algorithm.cluster_centers_\n```\n\n```{python}\nh = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n```\n\n```{python}\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \n            s = 100 )\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\nplt.show()\n```\n\n# Clustering Age, Annual Income, and Spending Score\n\n```{python}\nX3 = df[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111, algorithm='elkan'))\n    algorithm.fit(X3)\n    inertia.append(algorithm.inertia_)\n```\n\n```{python}\nplt.figure(1 , figsize = (10,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n```\n\n```{python}\nalgorithm = (KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X3)\nlabels3 = algorithm.labels_\ncentroids3 = algorithm.cluster_centers_\n\ny_kmeans = algorithm.fit_predict(X3)\ndf['cluster'] = pd.DataFrame(y_kmeans)\ndf.head()\n```\n\n```{python}\nimport plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x= df['Age'],\n    y= df['Spending Score (1-100)'],\n    z= df['Annual Income (k$)'],\n    mode='markers',\n     marker=dict(\n        color = df['cluster'], \n        size= 10,\n        line=dict(\n            color= df['cluster'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndata = [trace1]\nlayout = go.Layout(\n    title= 'Clusters wrt Age, Income and Spending Scores',\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Spending Score'),\n            zaxis = dict(title  = 'Annual Income')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)\n```\n\n# Conclusion\n\n```{python}\n#| include: false\n#| eval: false\nTo learn more about different clustering techniques, please go through this [link](https://www.kaggle.com/code/marcinrutecki/clustering-methods-comprehensive-study)\n\n# Project Link \n\nhttps://www.datacamp.com/tutorial/k-means-clustering-python\n```\n","srcMarkdownNoYaml":"\n\n# Introduction\n\nClustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.\n\n# Application of Clustering in Real World\n\nClustering is a widely used technique in the industry. It is being used in almost every domain, from banking and recommendation engines to document clustering and image segmentation.\n\n-   **Customer Segmentation** -\n\nOne of the most common applications of clustering is customer segmentation. And it isn't just limited to banking. This strategy is across functions, including telecom, e-commerce, sports, advertising, sales, etc.\n\n-   **Document Clustering** -\n\nAnother common application of clustering. Let's say you have multiple documents and you need to cluster similar documents together. Clustering helps us group these documents such that similar documents are in the same clusters.\n\n-   **Image Segmentation** -\n\nWe can also use clustering to perform image segmentation. In image segmentation, we try to club similar pixels in the image together. We can apply clustering to create clusters having similar pixels in the same group.\n\n-   **Recommendation Engines** -\n\nClustering can also be used in recommendation engines. Let's say you want to recommend songs to your friends. You can look at the songs liked by that person and then use clustering to find similar songs and finally recommend the most similar songs.\n\n# K-means Clustering\n\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms. You'll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster. Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares. In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The 'means' in the K-means refers to averaging of the data; that is, finding the centroid. Please see @fig-kmeanscluster to get an idea about K-means clustering.\n\n::: {#fig-kmeanscluster}\n![](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning.png)\n\nAn Example of K-means Clustering\n:::\n\n# Steps in K-Means Algorithm\n\n1.  Choose the number of clusters K.\n2.  Select at random K points, the centroids(not necessarily from your dataset).\n3.  Assign each data point to the closest centroid â†’ that forms K clusters.\n4.  Compute the mean of data points within a cluster and place the new centroid of each cluster.\n5.  Reassign each data point to the new closest centroid. If any reassignment took place, go to step 4, otherwise, the model is ready.\n\n# How to Choose the Value of K?\n\nThere are different techniques available to find the optimal value of K. The most common technique is the **elbow method.**\n\n## Elbow Method\n\nThe elbow method is used to determine the optimal number of clusters in K-means clustering. The elbow method plots the value of the Within cluster sum of square of the data points(WCSS) produced by different values of K. @fig-elbowmethod shows how the elbow method works.\n\n**Distortion**: It is calculated as the average of the squared distances from the cluster centers of the respective clusters to each data point. Typically, the Euclidean distance metric is used.\n\n$$\n Distortion = 1/n * \\sum(distance(point, centroid)^2)\n $$\n\n**Inertia**: It is the sum of the squared distances of samples to their closest cluster center.\n\n$$\nInertia = \\sum(distance(point, centroid)^2)\n$$\n\n::: {#fig-elbowmethod}\n![](elbowmethod.png)\n\nElbow Method for Optimal K\n:::\n\nWe can see that if K increases, average distortion will decrease. We should choose that k where the distortion decline drastically. From the above diagram the best value of k will be 3.\n\n## Silhouette Score\n\nAnother method to find the optimal value of K is **silhouette score**. The silhouette score and plot are used to evaluate the quality of a clustering solution produced by the k-means algorithm. The silhouette score measures -\n\na)  How close the data point is to other points in the cluster\n\nb)  How far away the data point is from points in other clusters\n\nSilhouette coefficient values range between -1 and 1. Larger numbers indicate that samples are closer to their clusters than they are to other clusters. A silhouette score close to 0 suggests overlapping clusters, and a negative score suggests poor clustering solutions.\n\n# When will K-means Clustering Fail?\n\nK-means clustering performs best on data that are spherical. Spherical data are data that group in space in close proximity to each other either. This can be visualized in 2 or 3 dimensional space more easily. Data that aren't spherical or should not be spherical do not work well with k-means clustering. For example, k-means clustering would not do well on the below data (@fig-kmeansNOT) as we would not be able to find distinct centroids to cluster the two circles or arcs differently, despite them clearly visually being two distinct circles and arcs that should be labeled as such.\n\n::: {#fig-kmeansNOT}\n![](kmeansNOTworking.png)\n\nWhen K means is not working\n:::\n\n# Importing Python Libraries\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\n\nfrom scipy.spatial.distance import cdist\nfrom sklearn import metrics\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n```\n\n# An Example of Clustering Using a Simulated Dataset\n\n```{python}\n# Creating the data\nx1 = np.array([3, 1, 1, 2, 1, 6, 6, 6, 5, 6,\\\n               7, 8, 9, 8, 9, 9, 8, 4, 4, 5, 4])\nx2 = np.array([5, 4, 5, 6, 5, 8, 6, 7, 6, 7, \\\n               1, 2, 1, 2, 3, 2, 3, 9, 10, 9, 10])\nX = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\n \n# Visualizing the data\nsns.set_style('whitegrid')\nplt.plot()\nplt.xlim([0, 12])\nplt.ylim([0, 12])\n#plt.title('Dataset')\nplt.scatter(x1, x2)\nplt.show()\n```\n\n```{python}\ndistortions = []\ninertias = []\nmapping1 = {}\nmapping2 = {}\nK = range(1, 10)\n \nfor k in K:\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n \n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n                                        'euclidean'), axis=1)) / X.shape[0])\n    inertias.append(kmeanModel.inertia_)\n \n    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n                                   'euclidean'), axis=1)) / X.shape[0]\n    mapping2[k] = kmeanModel.inertia_\n```\n\nPlease see @fig-ditortion to see elbow method using distortion\n\n```{python}\n#| label: fig-ditortion\n#| fig-cap: The Elbow Method using Distortion\nsns.set_style('whitegrid')\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Distortion')\n#plt.title('The Elbow Method using Distortion')\nplt.show()\n```\n\nPlease see @fig-inertia to see elbow method using inertia\n\n```{python}\n#| label: fig-inertia\n#| fig-cap: The Elbow Method using Inertia\nsns.set_style('whitegrid')\nplt.plot(K, inertias, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Inertia')\n#plt.title('The Elbow Method using Inertia')\nplt.show()\n```\n\nPlease see @fig-silhouette for Silhouette Score.\n\n```{python}\n # A list holds the silhouette coefficients for each k\nkmeans_kwargs = {\n        \"init\": \"random\",\n        \"n_init\": 10,\n        \"max_iter\": 300,\n        \"random_state\": 42,\n    }\n   \nsilhouette_coefficients = []\n\n# Notice you start at 2 clusters for silhouette coefficient\nfrom sklearn.metrics import silhouette_score\n\nfor k in range(2, 11):\n   kmeans = KMeans(n_clusters=k,**kmeans_kwargs)\n   kmeans.fit(X)\n   score = silhouette_score(X, kmeans.labels_)\n   silhouette_coefficients.append(score)\n```\n\n```{python}\n#| label: fig-silhouette\n#| fig-cap: The Silhouette Score for Optimal K\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(2, 11), silhouette_coefficients)\nplt.xticks(range(2, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()\n```\n\n## Clustered Data Points For Different K Values\n\nPlease see @fig-differentk for different clusters based on different values of K\n\n```{python}\n#| layout-nrows: 4\n#| label: fig-differentk\n#| fig-cap: Clustering Using Different K Values\n#| fig-subcap: \n#|    - \"Number of Cluster K = 1\"\n#|    - \"Number of Cluster K = 2\"\n#|    - \"Number of Cluster K = 3\"\n#|    - \"Number of Cluster K = 4\"\n#|    - \"Elbow Method\"\n\n\n\n\n# Create a range of values for k\nk_range = range(1, 5)\n \n# Initialize an empty list to \n# store the inertia values for each k\ninertia_values = []\n \n# Fit and plot the data for each k value\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, \\\n                    init='k-means++', random_state=42)\n    y_kmeans = kmeans.fit_predict(X)\n    inertia_values.append(kmeans.inertia_)\n    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans)\n    plt.scatter(kmeans.cluster_centers_[:, 0],\\\n                kmeans.cluster_centers_[:, 1], \\\n                s=100, c='red')\n    #plt.title('K-means clustering (k={})'.format(k))\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.show()\n \n# Plot the inertia values for each k\nplt.plot(k_range, inertia_values, 'bo-')\n#plt.title('Elbow Method')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Inertia')\nplt.show()\n```\n\n# Loading Dataset\n\n```{python}\ndf = pd.read_csv('DATA/Mall_Customers.csv')\n```\n\n## Metadata of the Dataset\n\n```{python}\n# first few rows\ndf.head()\n```\n\n```{python}\n# size of datset\ndf.shape\n```\n\n```{python}\n# summary about dataset\ndf.info()\n```\n\n```{python}\ndf.isnull().sum()\n```\n\n```{python}\ndf.describe()\n```\n\n```{python}\ndf.describe().T\n```\n\n# Exploratory Analysis\n\n```{python}\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 15)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n```\n\n```{python}\nsns.pairplot(df, vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue = \"Gender\")\nplt.show()\n```\n\n# Clustering Based on Age and Spending Score\n\n```{python}\nsns.set_style('whitegrid')\nplt.figure(1 , figsize = (10 , 5))\nplt.title('Scatter plot of Age v/s Spending Score', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, s = 100)\nplt.show()\n```\n\n## Deciding `K` Value\n\n```{python}\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 15):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\n```\n\n```{python}\nplt.figure(1 , figsize = (10,5))\nplt.plot(np.arange(1 , 15) , inertia , 'o')\nplt.plot(np.arange(1 , 15) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n```\n\n-   The KMeans algorithm clusters data by trying to separate samples in n groups of equal variances, minimizing a criterion known as `inertia`, or `within-cluster sum-of-squares Inertia`, or the `within-cluster sum of squares criterion`, can be recognized as a measure of how internally coherent clusters are.\n\n-   The k-means algorithm divides a set of `N` samples into `K` disjoint clusters `C`, each described by the mean `j` of the samples in the cluster. The means are commonly called the cluster centroids.\n\n-   The K-means algorithm aims to choose centroids that **minimize** the inertia, or within-cluster sum of squared criterion.\n\n## Applying KMeans for `K = 4`\n\n```{python}\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n```\n\n```{python}\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n```\n\n```{python}\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n```\n\n## Applying KMeans for `K = 5`\n\n```{python}\nalgorithm = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111 , algorithm='elkan'))\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n```\n\n```{python}\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n```\n\n```{python}\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n```\n\n# Clustering Based on Annual Income and Spending Score\n\n```{python}\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X2)\n    inertia.append(algorithm.inertia_)\n```\n\n```{python}\nplt.figure(1 , figsize = (10,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n```\n\n```{python}\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X2)\nlabels2 = algorithm.labels_\ncentroids2 = algorithm.cluster_centers_\n```\n\n```{python}\nh = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n```\n\n```{python}\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \n            s = 100 )\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\nplt.show()\n```\n\n# Clustering Age, Annual Income, and Spending Score\n\n```{python}\nX3 = df[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, \n                        tol=0.0001, random_state= 111, algorithm='elkan'))\n    algorithm.fit(X3)\n    inertia.append(algorithm.inertia_)\n```\n\n```{python}\nplt.figure(1 , figsize = (10,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n```\n\n```{python}\nalgorithm = (KMeans(n_clusters = 6 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X3)\nlabels3 = algorithm.labels_\ncentroids3 = algorithm.cluster_centers_\n\ny_kmeans = algorithm.fit_predict(X3)\ndf['cluster'] = pd.DataFrame(y_kmeans)\ndf.head()\n```\n\n```{python}\nimport plotly as py\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter3d(\n    x= df['Age'],\n    y= df['Spending Score (1-100)'],\n    z= df['Annual Income (k$)'],\n    mode='markers',\n     marker=dict(\n        color = df['cluster'], \n        size= 10,\n        line=dict(\n            color= df['cluster'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndata = [trace1]\nlayout = go.Layout(\n    title= 'Clusters wrt Age, Income and Spending Scores',\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Spending Score'),\n            zaxis = dict(title  = 'Annual Income')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)\n```\n\n# Conclusion\n\n```{python}\n#| include: false\n#| eval: false\nTo learn more about different clustering techniques, please go through this [link](https://www.kaggle.com/code/marcinrutecki/clustering-methods-comprehensive-study)\n\n# Project Link \n\nhttps://www.datacamp.com/tutorial/k-means-clustering-python\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"chap10_kmeansclustering.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.55","theme":"united","light":"flatly","linkcolor":"orangered","mainfont":"Georgia","title":"<center> Chapter # 10 <br> K-means Clustering"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}