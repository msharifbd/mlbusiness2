[
  {
    "objectID": "syllabus.html#books",
    "href": "syllabus.html#books",
    "title": "Syllabus",
    "section": "Books",
    "text": "Books"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "syllabus.html#classroom-etiquette",
    "href": "syllabus.html#classroom-etiquette",
    "title": "Syllabus",
    "section": "Classroom Etiquette",
    "text": "Classroom Etiquette"
  },
  {
    "objectID": "syllabus.html#modules",
    "href": "syllabus.html#modules",
    "title": "Syllabus",
    "section": "Modules",
    "text": "Modules\n\n01. Basics of R, Python, Tableau, and Power BI\n\n\n02. Working with Data\n\n\n03. Data Visualization\n\n\n04. Working with Databases\n\n\n05. Regression\n\n\n06. Machine Learning\n\n\n07. Textual Analysis"
  },
  {
    "objectID": "recent-developments.html",
    "href": "recent-developments.html",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow AI/ML will Transform Business?\n\n\n\n\n\nAI’s vast capabilities are already transforming business and changing the nature of many jobs\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "python_eda.html",
    "href": "python_eda.html",
    "title": "\nExploratory Data Analysis in Python",
    "section": "",
    "text": "The objective of this document is to introduce the necessary functions from pandas library in Python for data manipulation and matplotlib and seaborn libraries for data visualization. There are basically six functions - select(), filter(), mutate(), arrange(), group_by(), and summarize() - from dplyr package of tidyverse ecosystem that are very much necessary for data manipulation. These six functions can be used for 80% of data manipulation problems. In this document, we will compare the above six functions from dplyr with the equivalent pandas functions. Additionally, this handout also compares some other Python packages, particularly plotnine library that can be used to apply ggplot in Python.",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#changing-the-types-of-the-variables",
    "href": "python_eda.html#changing-the-types-of-the-variables",
    "title": "\nExploratory Data Analysis in Python",
    "section": "6.1 Changing the Types of the Variables",
    "text": "6.1 Changing the Types of the Variables\n      There are several types of data in Python as it is in R. Table Table 1 lists the data types in python.\n\n\n\n\nTable 1: Types of Data in Python\n\n\n\n\n\n\nPython Data Type\nData Nature\n\n\n\n\nfloat64\nReal Numbers\n\n\ncategory\ncateogries\n\n\ndatetime64\nDate Times\n\n\nint64\nIntegers\n\n\nbool\nTrue or False\n\n\nstring\nText\n\n\n\n\n\n\n\n\n\n\n\n# Changing the DATE variable from object to date\nproduct['DATE'] = pd.to_datetime(product['DATE']) \nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  int64         \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  int64         \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  int64         \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(5), object(4)\nmemory usage: 1.6+ MB\n\n\n\n# converting integer to object\nproduct.INVOICENO = product.INVOICENO.astype(str) \nproduct[['MONTH', 'PRODUCTID']] = product[['MONTH', 'PRODUCTID']].astype(str) \nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#select-equivalent-in-python---accessing-columns",
    "href": "python_eda.html#select-equivalent-in-python---accessing-columns",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.1 select () Equivalent in Python - Accessing Columns",
    "text": "7.1 select () Equivalent in Python - Accessing Columns\n\nprod2 = product[['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE']]\nprod2.head()\n\n   YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0  2014      159.0       0.0        159\n1  2014      159.2       0.2        199\n2  2014      119.2       0.2        149\n3  2014      159.0       0.0        159\n4  2014      159.0       0.0        159\n\n\n\nproduct.loc[:,['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE']]\n\n       YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0      2014      159.0       0.0        159\n1      2014      159.2       0.2        199\n2      2014      119.2       0.2        149\n3      2014      159.0       0.0        159\n4      2014      159.0       0.0        159\n...     ...        ...       ...        ...\n14962  2016      139.0       0.0        139\n14963  2016      149.0       0.0        149\n14964  2016      125.3       0.3        179\n14965  2016      199.0       0.0        199\n14966  2016      125.1       0.1        139\n\n[14967 rows x 4 columns]\n\nproduct.loc[0:5,['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE']]\n\n   YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0  2014      159.0       0.0        159\n1  2014      159.2       0.2        199\n2  2014      119.2       0.2        149\n3  2014      159.0       0.0        159\n4  2014      159.0       0.0        159\n5  2014      159.0       0.0        159\n\n\n\nproduct.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n\n       YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0      2014      159.0       0.0        159\n1      2014      159.2       0.2        199\n2      2014      119.2       0.2        149\n3      2014      159.0       0.0        159\n4      2014      159.0       0.0        159\n...     ...        ...       ...        ...\n14962  2016      139.0       0.0        139\n14963  2016      149.0       0.0        149\n14964  2016      125.3       0.3        179\n14965  2016      199.0       0.0        199\n14966  2016      125.1       0.1        139\n\n[14967 rows x 4 columns]\n\n\n\n# Regular Expression (Regex)\nproduct.filter(regex = \"PRICE$\") # Ends with Price \n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\nproduct.filter(regex = \"^SIZE\")  # Starts with SIZE\n\n       SIZE_US SIZE_EUROPE  SIZE_UK\n0         11.0          44     10.5\n1         11.5       44-45     11.0\n2          9.5       42-43      9.0\n3          9.5          40      7.5\n4          9.0       39-40      7.0\n...        ...         ...      ...\n14962      9.5       42-43      9.0\n14963     12.0       42-43     10.0\n14964     10.5       43-44     10.0\n14965      9.5          40      7.5\n14966      6.5          37      4.5\n\n[14967 rows x 3 columns]\n\nproduct.filter(regex = \"PRICE\")  # Contains the word Price \n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\n\n\nproduct.select_dtypes('object')\n\n      INVOICENO         COUNTRY PRODUCTID  SHOP  GENDER SIZE_EUROPE MONTH\n0         52389  United Kingdom      2152   UK2    Male          44     1\n1         52390   United States      2230  US15    Male       44-45     1\n2         52391          Canada      2160  CAN7    Male       42-43     1\n3         52392   United States      2234   US6  Female          40     1\n4         52393  United Kingdom      2222   UK4  Female       39-40     1\n...         ...             ...       ...   ...     ...         ...   ...\n14962     65773  United Kingdom      2154   UK2    Male       42-43    12\n14963     65774   United States      2181  US12  Female       42-43    12\n14964     65775          Canada      2203  CAN6    Male       43-44    12\n14965     65776         Germany      2231  GER1  Female          40    12\n14966     65777         Germany      2156  GER1  Female          37    12\n\n[14967 rows x 7 columns]\n\nproduct.select_dtypes('int')\n\n       UNITPRICE  YEAR\n0            159  2014\n1            199  2014\n2            149  2014\n3            159  2014\n4            159  2014\n...          ...   ...\n14962        139  2016\n14963        149  2016\n14964        179  2016\n14965        199  2016\n14966        139  2016\n\n[14967 rows x 2 columns]\n\n\n\nproduct.loc[:,product.columns.str.startswith('SIZE')]\n\n       SIZE_US SIZE_EUROPE  SIZE_UK\n0         11.0          44     10.5\n1         11.5       44-45     11.0\n2          9.5       42-43      9.0\n3          9.5          40      7.5\n4          9.0       39-40      7.0\n...        ...         ...      ...\n14962      9.5       42-43      9.0\n14963     12.0       42-43     10.0\n14964     10.5       43-44     10.0\n14965      9.5          40      7.5\n14966      6.5          37      4.5\n\n[14967 rows x 3 columns]\n\nproduct.loc[:,product.columns.str.contains('PRICE')]\n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\nproduct.loc[:,product.columns.str.endswith('PRICE')]\n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\n\n\n# Dropping some columns \nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB\n\nproduct.drop(columns = ['SIZE_EUROPE', 'SIZE_UK'], axis = 1)\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n0         52389 2014-01-01  United Kingdom  ...  2014     1     159.0\n1         52390 2014-01-01   United States  ...  2014     1     159.2\n2         52391 2014-01-01          Canada  ...  2014     1     119.2\n3         52392 2014-01-01   United States  ...  2014     1     159.0\n4         52393 2014-01-01  United Kingdom  ...  2014     1     159.0\n...         ...        ...             ...  ...   ...   ...       ...\n14962     65773 2016-12-31  United Kingdom  ...  2016    12     139.0\n14963     65774 2016-12-31   United States  ...  2016    12     149.0\n14964     65775 2016-12-31          Canada  ...  2016    12     125.3\n14965     65776 2016-12-31         Germany  ...  2016    12     199.0\n14966     65777 2016-12-31         Germany  ...  2016    12     125.1\n\n[14967 rows x 12 columns]\n\nproduct.drop(columns = ['SIZE_EUROPE', 'SIZE_UK'], axis = 1) \\\n    .pipe(lambda x: x.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 12 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   INVOICENO  14967 non-null  object        \n 1   DATE       14967 non-null  datetime64[ns]\n 2   COUNTRY    14967 non-null  object        \n 3   PRODUCTID  14967 non-null  object        \n 4   SHOP       14967 non-null  object        \n 5   GENDER     14967 non-null  object        \n 6   SIZE_US    14967 non-null  float64       \n 7   UNITPRICE  14967 non-null  int64         \n 8   DISCOUNT   14967 non-null  float64       \n 9   YEAR       14967 non-null  int64         \n 10  MONTH      14967 non-null  object        \n 11  SALEPRICE  14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(3), int64(2), object(6)\nmemory usage: 1.4+ MB\n\n\n\n7.1.1 Rearranging Columns\n\n# Sorting Alphabetically\nproduct.reindex(sorted(product.columns), axis = 1)\n\n              COUNTRY       DATE  DISCOUNT  ... SIZE_US UNITPRICE  YEAR\n0      United Kingdom 2014-01-01       0.0  ...    11.0       159  2014\n1       United States 2014-01-01       0.2  ...    11.5       199  2014\n2              Canada 2014-01-01       0.2  ...     9.5       149  2014\n3       United States 2014-01-01       0.0  ...     9.5       159  2014\n4      United Kingdom 2014-01-01       0.0  ...     9.0       159  2014\n...               ...        ...       ...  ...     ...       ...   ...\n14962  United Kingdom 2016-12-31       0.0  ...     9.5       139  2016\n14963   United States 2016-12-31       0.0  ...    12.0       149  2016\n14964          Canada 2016-12-31       0.3  ...    10.5       179  2016\n14965         Germany 2016-12-31       0.0  ...     9.5       199  2016\n14966         Germany 2016-12-31       0.1  ...     6.5       139  2016\n\n[14967 rows x 14 columns]\n\n# Sorting As You Want (ASY)\nproduct.columns.to_list()\n\n['INVOICENO', 'DATE', 'COUNTRY', 'PRODUCTID', 'SHOP', 'GENDER', 'SIZE_US', 'SIZE_EUROPE', 'SIZE_UK', 'UNITPRICE', 'DISCOUNT', 'YEAR', 'MONTH', 'SALEPRICE']\n\ncol_first = ['YEAR','MONTH']\ncol_rest = product.columns.difference(col_first, sort=False).to_list()\nproduct2 = product[col_first + col_rest]\nproduct2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   YEAR         14967 non-null  int64         \n 1   MONTH        14967 non-null  object        \n 2   INVOICENO    14967 non-null  object        \n 3   DATE         14967 non-null  datetime64[ns]\n 4   COUNTRY      14967 non-null  object        \n 5   PRODUCTID    14967 non-null  object        \n 6   SHOP         14967 non-null  object        \n 7   GENDER       14967 non-null  object        \n 8   SIZE_US      14967 non-null  float64       \n 9   SIZE_EUROPE  14967 non-null  object        \n 10  SIZE_UK      14967 non-null  float64       \n 11  UNITPRICE    14967 non-null  int64         \n 12  DISCOUNT     14967 non-null  float64       \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#filter-equivalent-in-python---accessing-rows",
    "href": "python_eda.html#filter-equivalent-in-python---accessing-rows",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.2 filter () Equivalent in Python - Accessing Rows",
    "text": "7.2 filter () Equivalent in Python - Accessing Rows\n\nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB\n\nproduct.COUNTRY.value_counts()\n\nCOUNTRY\nUnited States     5886\nGermany           4392\nCanada            2952\nUnited Kingdom    1737\nName: count, dtype: int64\n\nproduct['YEAR'].unique()\n\narray([2014, 2015, 2016])\n\nproduct['YEAR'].value_counts()\n\nYEAR\n2016    7366\n2015    4848\n2014    2753\nName: count, dtype: int64\n\n\n\nproduct.query('COUNTRY == \"United States\"')\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n8         52397 2014-01-02  United States  ...  2014     1     139.0\n10        52399 2014-01-02  United States  ...  2014     1     129.0\n...         ...        ...            ...  ...   ...   ...       ...\n14956     65767 2016-12-31  United States  ...  2016    12     139.0\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n\n[5886 rows x 14 columns]\n\nproduct.query('COUNTRY == \"United States\" | COUNTRY == \"Canada\"')\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n2         52391 2014-01-01         Canada  ...  2014     1     119.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n7         52396 2014-01-02         Canada  ...  2014     1     169.0\n...         ...        ...            ...  ...   ...   ...       ...\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n14964     65775 2016-12-31         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\nproduct.query(\"COUNTRY in ['United States', 'Canada']\")\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n2         52391 2014-01-01         Canada  ...  2014     1     119.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n7         52396 2014-01-02         Canada  ...  2014     1     169.0\n...         ...        ...            ...  ...   ...   ...       ...\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n14964     65775 2016-12-31         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\nproduct.query(\"COUNTRY == 'United States' & YEAR == 2016\")\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n7610      59206 2016-01-02  United States  ...  2016     1     132.3\n7613      59209 2016-01-02  United States  ...  2016     1     127.2\n7617      59213 2016-01-02  United States  ...  2016     1     125.3\n7618      59214 2016-01-02  United States  ...  2016     1     151.2\n7619      59214 2016-01-02  United States  ...  2016     1     151.2\n...         ...        ...            ...  ...   ...   ...       ...\n14956     65767 2016-12-31  United States  ...  2016    12     139.0\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n\n[2935 rows x 14 columns]\n\nproduct.query(\"COUNTRY == 'United States' & YEAR in [2015,2016]\")\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n2753      54725 2015-01-01  United States  ...  2015     1     179.0\n2754      54726 2015-01-01  United States  ...  2015     1     169.0\n2755      54727 2015-01-01  United States  ...  2015     1     116.1\n2761      54733 2015-01-02  United States  ...  2015     1     179.0\n2766      54738 2015-01-02  United States  ...  2015     1     199.0\n...         ...        ...            ...  ...   ...   ...       ...\n14956     65767 2016-12-31  United States  ...  2016    12     139.0\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n\n[4859 rows x 14 columns]\n\n\n\nproduct.loc[(product['COUNTRY'] == \"United States\")]\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n8         52397 2014-01-02  United States  ...  2014     1     139.0\n10        52399 2014-01-02  United States  ...  2014     1     129.0\n...         ...        ...            ...  ...   ...   ...       ...\n14956     65767 2016-12-31  United States  ...  2016    12     139.0\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n\n[5886 rows x 14 columns]\n\nproduct.loc[product['COUNTRY'].isin([\"United States\", \"Canada\"])]\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n2         52391 2014-01-01         Canada  ...  2014     1     119.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n7         52396 2014-01-02         Canada  ...  2014     1     169.0\n...         ...        ...            ...  ...   ...   ...       ...\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n14964     65775 2016-12-31         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\nproduct.loc[product['COUNTRY'] \\\n  .isin([\"United States\", \"Canada\"]) & (product['YEAR'] == 2014)]\n\n     INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1        52390 2014-01-01  United States  ...  2014     1     159.2\n2        52391 2014-01-01         Canada  ...  2014     1     119.2\n3        52392 2014-01-01  United States  ...  2014     1     159.0\n5        52394 2014-01-01  United States  ...  2014     1     159.0\n7        52396 2014-01-02         Canada  ...  2014     1     169.0\n...        ...        ...            ...  ...   ...   ...       ...\n2739     54713 2014-12-30  United States  ...  2014    12     189.0\n2745     54718 2014-12-31         Canada  ...  2014    12     151.2\n2746     54719 2014-12-31  United States  ...  2014    12     199.0\n2748     54721 2014-12-31         Canada  ...  2014    12      74.5\n2749     54722 2014-12-31  United States  ...  2014    12     118.3\n\n[1649 rows x 14 columns]\n\nproduct.loc[(product['COUNTRY'] == \"United States\") & (product[\"YEAR\"] == 2014)]\n\n     INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1        52390 2014-01-01  United States  ...  2014     1     159.2\n3        52392 2014-01-01  United States  ...  2014     1     159.0\n5        52394 2014-01-01  United States  ...  2014     1     159.0\n8        52397 2014-01-02  United States  ...  2014     1     139.0\n10       52399 2014-01-02  United States  ...  2014     1     129.0\n...        ...        ...            ...  ...   ...   ...       ...\n2731     54705 2014-12-29  United States  ...  2014    12     179.0\n2734     54708 2014-12-30  United States  ...  2014    12     159.0\n2739     54713 2014-12-30  United States  ...  2014    12     189.0\n2746     54719 2014-12-31  United States  ...  2014    12     199.0\n2749     54722 2014-12-31  United States  ...  2014    12     118.3\n\n[1027 rows x 14 columns]\n\n\n\n7.2.1 loc[] Function can be used both for Slicing (selecting Rows) and Selecting Columns\n\nproduct.loc[\n  product['COUNTRY'] == 'United States',\n  ['COUNTRY', \"UNITPRICE\", \"SALEPRICE\"]]\n\n             COUNTRY  UNITPRICE  SALEPRICE\n1      United States        199      159.2\n3      United States        159      159.0\n5      United States        159      159.0\n8      United States        139      139.0\n10     United States        129      129.0\n...              ...        ...        ...\n14956  United States        139      139.0\n14959  United States        149      119.2\n14960  United States        189      189.0\n14961  United States        129      129.0\n14963  United States        149      149.0\n\n[5886 rows x 3 columns]",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#arrange-equivalent-in-python---sorting-or-arranging-rows",
    "href": "python_eda.html#arrange-equivalent-in-python---sorting-or-arranging-rows",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.3 arrange () Equivalent in Python - Sorting or Arranging Rows",
    "text": "7.3 arrange () Equivalent in Python - Sorting or Arranging Rows\n\nproduct.sort_values(by = ['MONTH'])\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n0         52389 2014-01-01  United Kingdom  ...  2014     1     159.0\n2862      54831 2015-01-13         Germany  ...  2015     1     169.0\n2863      54832 2015-01-13         Germany  ...  2015     1     125.1\n2864      54833 2015-01-13   United States  ...  2015     1     135.2\n2865      54834 2015-01-13         Germany  ...  2015     1     149.0\n...         ...        ...             ...  ...   ...   ...       ...\n13225     64202 2016-09-28          Canada  ...  2016     9     189.0\n13224     64201 2016-09-28         Germany  ...  2016     9     149.0\n13223     64200 2016-09-28          Canada  ...  2016     9     199.0\n13235     64210 2016-09-29   United States  ...  2016     9     135.2\n12749     63775 2016-09-08         Germany  ...  2016     9     139.0\n\n[14967 rows x 14 columns]\n\nproduct.sort_values(by = ['MONTH'], ascending = False)\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n13162     64145 2016-09-26  United Kingdom  ...  2016     9      90.3\n5946      57674 2015-09-12   United States  ...  2015     9     159.0\n5944      57672 2015-09-12          Canada  ...  2015     9     179.0\n5943      57671 2015-09-12         Germany  ...  2015     9     159.0\n5942      57670 2015-09-12          Canada  ...  2015     9     179.0\n...         ...        ...             ...  ...   ...   ...       ...\n3037      54995 2015-01-29   United States  ...  2015     1     103.2\n3038      54996 2015-01-30          Canada  ...  2015     1     161.1\n3039      54997 2015-01-30          Canada  ...  2015     1      84.5\n3040      54997 2015-01-30          Canada  ...  2015     1     169.0\n0         52389 2014-01-01  United Kingdom  ...  2014     1     159.0\n\n[14967 rows x 14 columns]\n\nproduct.sort_values(by = ['MONTH', 'SALEPRICE'])\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n33        52414 2014-01-05         Germany  ...  2014     1      64.5\n177       52533 2014-01-25          Canada  ...  2014     1      64.5\n185       52539 2014-01-26  United Kingdom  ...  2014     1      64.5\n194       52548 2014-01-27  United Kingdom  ...  2014     1      64.5\n2762      54734 2015-01-02         Germany  ...  2015     1      64.5\n...         ...        ...             ...  ...   ...   ...       ...\n13245     64219 2016-09-29  United Kingdom  ...  2016     9     199.0\n13246     64220 2016-09-29   United States  ...  2016     9     199.0\n13248     64222 2016-09-29   United States  ...  2016     9     199.0\n13251     64224 2016-09-29         Germany  ...  2016     9     199.0\n13272     64244 2016-09-30   United States  ...  2016     9     199.0\n\n[14967 rows x 14 columns]",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#rename-equivalent-in-python---renaming-column-names",
    "href": "python_eda.html#rename-equivalent-in-python---renaming-column-names",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.4 rename () Equivalent in Python - Renaming Column Names",
    "text": "7.4 rename () Equivalent in Python - Renaming Column Names\n      We already did some renaming of the columns using str. function. Here we use rename () function to change the name of the columns.\n\nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB\n\nproduct.rename( columns = \n  {'SIZE_(EUROPE)': 'SIZE_EUROPE',\n   'SIZE_(UK)' : 'SIZE_UK'}) \\\n   .pipe(lambda x: x.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#mutate-equivalent-in-python---creating-new-columns-variables",
    "href": "python_eda.html#mutate-equivalent-in-python---creating-new-columns-variables",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.5 mutate () Equivalent in Python - Creating New Columns (Variables)",
    "text": "7.5 mutate () Equivalent in Python - Creating New Columns (Variables)\n\nproduct['NECOLUMN'] = 5\nproduct.head()\n\n  INVOICENO       DATE         COUNTRY  ... MONTH SALEPRICE NECOLUMN\n0     52389 2014-01-01  United Kingdom  ...     1     159.0        5\n1     52390 2014-01-01   United States  ...     1     159.2        5\n2     52391 2014-01-01          Canada  ...     1     119.2        5\n3     52392 2014-01-01   United States  ...     1     159.0        5\n4     52393 2014-01-01  United Kingdom  ...     1     159.0        5\n\n[5 rows x 15 columns]\n\nproduct.drop(columns = ['NECOLUMN'], axis = 1, inplace = True) \n\n\nproduct['SALEPRICE2'] = product['UNITPRICE']*(1-product['DISCOUNT'])\nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \n 14  SALEPRICE2   14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(5), int64(2), object(7)\nmemory usage: 1.7+ MB\n\n\n\n# Using the assign () function \nproduct[['PRODUCTID', 'UNITPRICE', 'DISCOUNT']] \\\n .assign(SALEPRICE3 = lambda x: x.UNITPRICE*(1-x.DISCOUNT)) \\\n .head(5)\n\n  PRODUCTID  UNITPRICE  DISCOUNT  SALEPRICE3\n0      2152        159       0.0       159.0\n1      2230        199       0.2       159.2\n2      2160        149       0.2       119.2\n3      2234        159       0.0       159.0\n4      2222        159       0.0       159.0",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#group_by-and-summarize-equivalent-in-python---summarizing-data",
    "href": "python_eda.html#group_by-and-summarize-equivalent-in-python---summarizing-data",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.6 group_by () and summarize () Equivalent in Python - Summarizing Data",
    "text": "7.6 group_by () and summarize () Equivalent in Python - Summarizing Data\n      Figure Figure 1 presents the split-apply-combine principle in group_by () and summarize () functions.\n\n\n\n\n\n\n\n\nFigure 1: Split Apply and Combine Principle\n\n\n\n\n\n\nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \n 14  SALEPRICE2   14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(5), int64(2), object(7)\nmemory usage: 1.7+ MB\n\nproduct.groupby(['COUNTRY']) ['UNITPRICE'].mean()\n\nCOUNTRY\nCanada            164.691057\nGermany           164.163934\nUnited Kingdom    165.614853\nUnited States     163.490316\nName: UNITPRICE, dtype: float64\n\nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']].mean()\n\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(np.mean)\n\n&lt;string&gt;:3: FutureWarning: The provided callable &lt;function mean at 0x00000199A8E536A0&gt; is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(\"mean\")\n\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\n       \nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(AVG_UNITPRICE = (\"UNITPRICE\", \"mean\"), \n            AVG_LISTPRICE = (\"SALEPRICE\", \"mean\"))\n\n                AVG_UNITPRICE  AVG_LISTPRICE\nCOUNTRY                                     \nCanada             164.691057     144.228963\nGermany            164.163934     143.574658\nUnited Kingdom     165.614853     145.505872\nUnited States      163.490316     143.727421\n\n            \n            \nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(AVG_UNITPRICE = (\"UNITPRICE\", \"mean\"), \n            AVG_LISTPRICE = (\"SALEPRICE\", \"mean\"),\n            TOTALN = (\"SALEPRICE\", \"size\"), # size function for n\n            TOTALOBS = (\"SALEPRICE\", \"count\") # count function for n\n            )\n\n                AVG_UNITPRICE  AVG_LISTPRICE  TOTALN  TOTALOBS\nCOUNTRY                                                       \nCanada             164.691057     144.228963    2952      2952\nGermany            164.163934     143.574658    4392      4392\nUnited Kingdom     165.614853     145.505872    1737      1737\nUnited States      163.490316     143.727421    5886      5886\n\n# Combining Several Pandas Functions together           \nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(AVG_UNITPRICE = (\"UNITPRICE\", \"mean\"), \n            AVG_LISTPRICE = (\"SALEPRICE\", \"mean\"),\n            TOTALN = (\"SALEPRICE\", \"size\"), # size function for n\n            TOTALOBS = (\"SALEPRICE\", \"count\") # count function for n\n            ) \\\n       .sort_values(by = ['TOTALOBS'], ascending = False) \\\n       .reset_index() \\\n       .query ('COUNTRY == \"United States\"')\n\n         COUNTRY  AVG_UNITPRICE  AVG_LISTPRICE  TOTALN  TOTALOBS\n0  United States     163.490316     143.727421    5886      5886",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#summary-statistics-in-python",
    "href": "python_eda.html#summary-statistics-in-python",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.7 Summary Statistics in Python",
    "text": "7.7 Summary Statistics in Python\n\n# Summary Statistics in Python \nproduct.GENDER.value_counts()\n\nGENDER\nMale      8919\nFemale    6048\nName: count, dtype: int64\n\n# Encoding a Categorical Variables \nproduct['SEX'] = product['GENDER'].map({\n  'Male':1,\n  'Female':0\n})\n\n\n# Defining a Function \ndef percentile(n):\n    def percentile_(x):\n        return x.quantile(n)\n    percentile_.__name__ = 'percentile_{:02.0f}'.format(n*100)\n    return percentile_\n  \nproduct [['SALEPRICE', 'UNITPRICE', 'SEX']] \\\n     .agg([\"count\",\"mean\", \"std\", \"median\", percentile(0.25), percentile(0.75)]) \\\n     .transpose () \\\n     .reset_index() \\\n     .rename(columns = {'index': \"Variables\",\n                        'percentile_25': 'P25',\n                        'percentile_75': 'P75',\n                        'count': 'n',\n                        'mean' : 'Mean',\n                        'median' : 'Median',\n                        'std': 'Std'\n                       }) \\\n      .round(3) # rounding to two decimal places \n\n   Variables        n     Mean     Std  Median    P25    P75\n0  SALEPRICE  14967.0  143.988  35.181   149.0  125.1  169.0\n1  UNITPRICE  14967.0  164.171  22.941   159.0  149.0  179.0\n2        SEX  14967.0    0.596   0.491     1.0    0.0    1.0",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#bar-chart",
    "href": "python_eda.html#bar-chart",
    "title": "\nExploratory Data Analysis in Python",
    "section": "9.1 Bar Chart",
    "text": "9.1 Bar Chart\n\nbar_r = product.filter (['COUNTRY']) \\\n       .value_counts() \\\n       .reset_index() \\\n       .rename (columns = {'count':'n'}) \\\n       .sort_values (by = ['n'])\n\nplot = (ggplot(data = bar_r, \n  mapping = aes(x = 'COUNTRY', y = 'n'))+\n  geom_bar (fill = \"pink\", stat = \"identity\")+\n  labs (x = 'Country',\n  y = 'Number of Observations'\n  #,title = 'Total Observations of Countries'\n  )\n)\nplot.draw(True)\n\n\n\n\nTotal Observations of Countries",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#line-chart",
    "href": "python_eda.html#line-chart",
    "title": "\nExploratory Data Analysis in Python",
    "section": "9.2 Line Chart",
    "text": "9.2 Line Chart\n\nplot = (ggplot(product, aes(x = 'SIZE_US', y= 'UNITPRICE', color = 'GENDER'))+\n facet_wrap('COUNTRY')+\n geom_smooth(se = False, method = 'lm')+\n labs(x = \"Shoe Size (US)\", y = \"Price\")+\n theme (legend_position = \"top\")\n)\nplot.draw(True)\n\n\n\n\nRelations between Shoe Size and Sale Price in Different Countries\n\n\n\n\n\nmonth_sales = product['MONTH'] \\\n    .value_counts(sort = False) \\\n    .reset_index(name = 'SALES') \\\n    .rename (columns = {'index' : 'MONTH'})\n\nmonth_sales['MONTH'] = pd.to_numeric(month_sales['MONTH']) \n\nplot = (ggplot(month_sales, aes (\"MONTH\", \"SALES\"))\n + geom_point(color = 'blue')\n + labs(x = \"Month\", y = \"Total Sales\"\n   #,title = \"SALES IN DIFFERENT MONTHS\"\n   )\n)\nplot.draw(True)\n\n\n\n\nSales of Shoe in Different Months",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#footnotes",
    "href": "python_eda.html#footnotes",
    "title": "\nExploratory Data Analysis in Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.techrepublic.com/article/why-data-scientist-is-the-most-promising-job-of-2019/↩︎",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "posts/how-ai-transform-business.html",
    "href": "posts/how-ai-transform-business.html",
    "title": "How AI/ML will Transform Business?",
    "section": "",
    "text": "How Artificial Intelligence Will Transform Businesses\n\n\n\n Back to top"
  },
  {
    "objectID": "moduels.html",
    "href": "moduels.html",
    "title": "Modules & Slides",
    "section": "",
    "text": "The following Modules (Chapters) are suggested for the course -\n\nChapter # 01 - Introduction to Machine Learning (ML) & Concepts\nChapter # 02 - Introduction to R and Python and Exploratory Data Analysis (EDA)\nChapter # 03 - KNN Algorithm and Fundamental Concepts in ML Modeling\nChapter # 04 - Logistic Regression Algorithm\nChapter # 05 - Discriminant Analysis\nChapter # 06 - Naive Bayes and Support Vector Machine (SVM)\nChapter # 07 - Linear Regression\nChapter # 08 - Decision Tree Analysis\nChapter # 09 - Dimension Reduction and Principal Component Analysis\nChapter # 10 - K- Means Clustering",
    "crumbs": [
      "Modules & Slides"
    ]
  },
  {
    "objectID": "moduels.html#modules",
    "href": "moduels.html#modules",
    "title": "Modules & Slides",
    "section": "",
    "text": "The following Modules (Chapters) are suggested for the course -\n\nChapter # 01 - Introduction to Machine Learning (ML) & Concepts\nChapter # 02 - Introduction to R and Python and Exploratory Data Analysis (EDA)\nChapter # 03 - KNN Algorithm and Fundamental Concepts in ML Modeling\nChapter # 04 - Logistic Regression Algorithm\nChapter # 05 - Discriminant Analysis\nChapter # 06 - Naive Bayes and Support Vector Machine (SVM)\nChapter # 07 - Linear Regression\nChapter # 08 - Decision Tree Analysis\nChapter # 09 - Dimension Reduction and Principal Component Analysis\nChapter # 10 - K- Means Clustering",
    "crumbs": [
      "Modules & Slides"
    ]
  },
  {
    "objectID": "moduels.html#slides",
    "href": "moduels.html#slides",
    "title": "Modules & Slides",
    "section": "Slides",
    "text": "Slides\nThe slides for the course are available here.",
    "crumbs": [
      "Modules & Slides"
    ]
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "mlbusiness2/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/period/test_constructors.html",
    "href": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/period/test_constructors.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "mlbusiness2/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "chap10_kmeansclustering.html",
    "href": "chap10_kmeansclustering.html",
    "title": "\nChapter # 10  K-means Clustering",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Modules & Slides",
      "Clustering",
      "<center> Chapter # 10 <br> K-means Clustering"
    ]
  },
  {
    "objectID": "chap08_decision_tree.html",
    "href": "chap08_decision_tree.html",
    "title": "\nChapter # 08  Decision Tree",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 08 <br> Decision Tree"
    ]
  },
  {
    "objectID": "chap06_svm.html",
    "href": "chap06_svm.html",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "",
    "text": "Support Vector Machines (SVMs in short) are machine learning algorithms that are used for classification and regression purposes. SVMs are one of the powerful machine learning algorithms for classification, regression and outlier detection purposes. An SVM classifier builds a model that assigns new data points to one of the given categories. Thus, it can be viewed as a non-probabilistic binary linear classifier.\nThe original SVM algorithm was developed by Vladimir N Vapnik and Alexey Ya. Chervonenkis in 1963. At that time, the algorithm was in early stages. The only possibility is to draw hyperplanes for linear classifier. In 1992, Bernhard E. Boser, Isabelle M Guyon and Vladimir N Vapnik suggested a way to create non-linear classifiers by applying the kernel trick to maximum-margin hyperplanes. The current standard was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.\nSVMs can be used for linear classification purposes. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using the kernel trick. It enable us to implicitly map the inputs into high dimensional feature spaces.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#hyperplane",
    "href": "chap06_svm.html#hyperplane",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Hyperplane",
    "text": "Hyperplane\nA hyperplane is a decision boundary which separates between given set of data points having different class labels. The SVM classifier separates data points using a hyperplane with the maximum amount of margin. This hyperplane is known as the maximum margin hyperplane and the linear classifier it defines is known as the maximum margin classifier.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#support-vectors",
    "href": "chap06_svm.html#support-vectors",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Support Vectors",
    "text": "Support Vectors\nSupport vectors are the sample data points, which are closest to the hyperplane. These data points will define the separating line or hyperplane better by calculating margins.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#margin",
    "href": "chap06_svm.html#margin",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Margin",
    "text": "Margin\nA margin is a separation gap between the two lines on the closest data points. It is calculated as the perpendicular distance from the line to support vectors or closest data points. In SVMs, we try to maximize this separation gap so that we get maximum margin.\nThe following diagram illustrates these concepts visually.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#margin-in-svm",
    "href": "chap06_svm.html#margin-in-svm",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Margin in SVM",
    "text": "Margin in SVM\n\n\n\nMargin in SVM",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#svm-under-the-hood",
    "href": "chap06_svm.html#svm-under-the-hood",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "SVM Under the hood",
    "text": "SVM Under the hood\nIn SVMs, our main objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum margin hyperplane in the following 2 step process –\n\nGenerate hyperplanes which segregates the classes in the best possible way. There are many hyperplanes that might classify the data. We should look for the best hyperplane that represents the largest separation, or margin, between the two classes.\nSo, we choose the hyperplane so that distance from it to the support vectors on each side is maximized. If such a hyperplane exists, it is known as the maximum margin hyperplane and the linear classifier it defines is known as a maximum margin classifier.\n\nThe following diagram illustrates the concept of maximum margin and maximum margin hyperplane in a clear manner.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#maximum-margin-hyperplane",
    "href": "chap06_svm.html#maximum-margin-hyperplane",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Maximum margin hyperplane",
    "text": "Maximum margin hyperplane\n\n\n\nMaximum margin hyperplane",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#problem-with-dispersed-datasets",
    "href": "chap06_svm.html#problem-with-dispersed-datasets",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Problem with dispersed datasets",
    "text": "Problem with dispersed datasets\nSometimes, the sample data points are so dispersed that it is not possible to separate them using a linear hyperplane. In such a situation, SVMs use a kernel trick to transform the input space to a higher dimensional space as shown in the diagram below. It uses a mapping function to transform the 2-D input space into the 3-D input space. Now, we can easily segregate the data points using linear separation.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#kernel-trick---transformation-of-input-space-to-higher-dimensional-space",
    "href": "chap06_svm.html#kernel-trick---transformation-of-input-space-to-higher-dimensional-space",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Kernel trick - transformation of input space to higher dimensional space",
    "text": "Kernel trick - transformation of input space to higher dimensional space\n\n\n\nKernel trick\n\n\nAnother Example of the Kernel Trick -\n\n\n\nKernel trick",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#kernel-function",
    "href": "chap06_svm.html#kernel-function",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Kernel function",
    "text": "Kernel function\n\n\n\nKernel function\n\n\nIn the context of SVMs, there are 4 popular kernels – Linear kernel,Polynomial kernel,Radial Basis Function (RBF) kernel (also called Gaussian kernel) and Sigmoid kernel. These are described below -\n\nLinear kernel\nIn linear kernel, the kernel function takes the form of a linear function as follows-\nlinear kernel : K(xi , xj ) = xiT xj\nLinear kernel is used when the data is linearly separable. It means that data can be separated using a single line. It is one of the most common kernels to be used. It is mostly used when there are large number of features in a dataset. Linear kernel is often used for text classification purposes.\nTraining with a linear kernel is usually faster, because we only need to optimize the C regularization parameter. When training with other kernels, we also need to optimize the γ parameter. So, performing a grid search will usually take more time.\nLinear kernel can be visualized with the following figure.\n\nLinear Kernel\n\n\n\nLinear Kernel\n\n\n\n\n\nPolynomial Kernel\nPolynomial kernel represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables. The polynomial kernel looks not only at the given features of input samples to determine their similarity, but also combinations of the input samples.\nFor degree-d polynomials, the polynomial kernel is defined as follows –\nPolynomial kernel : K(xi , xj ) = (γxiT xj + r)d , γ &gt; 0\nPolynomial kernel is very popular in Natural Language Processing. The most common degree is d = 2 (quadratic), since larger degrees tend to overfit on NLP problems. It can be visualized with the following diagram.\n\nPolynomial Kernel\n\n\n\nPolynomial Kernel\n\n\n\n\n\nRadial Basis Function Kernel\nRadial basis function kernel is a general purpose kernel. It is used when we have no prior knowledge about the data. The RBF kernel on two samples x and y is defined by the following equation –\n\nRadial Basis Function kernel\n\n\n\nRBK Kernel\n\n\nThe following diagram demonstrates the SVM classification with rbf kernel.\n\n\n\nSVM Classification with rbf kernel\n\n\n\nSVM Classification with rbf kernel\n\n\n\n\nSigmoid kernel\nSigmoid kernel has its origin in neural networks. We can use it as the proxy for neural networks. Sigmoid kernel is given by the following equation –\nsigmoid kernel : k (x, y) = tanh(αxTy + c)\nSigmoid kernel can be visualized with the following diagram-\n\nSigmoid kernel\n\n\n\nSigmoid kernel",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#attribute-information",
    "href": "chap06_svm.html#attribute-information",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Attribute Information:",
    "text": "Attribute Information:\nEach candidate is described by 8 continuous variables, and a single class variable. The first four are simple statistics obtained from the integrated pulse profile. The remaining four variables are similarly obtained from the DM-SNR curve . These are summarized below:\n\nMean of the integrated profile.\nStandard deviation of the integrated profile.\nExcess kurtosis of the integrated profile.\nSkewness of the integrated profile.\nMean of the DM-SNR curve.\nStandard deviation of the DM-SNR curve.\nExcess kurtosis of the DM-SNR curve.\nSkewness of the DM-SNR curve.\nClass",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#explore-missing-values-in-variables",
    "href": "chap06_svm.html#explore-missing-values-in-variables",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Explore missing values in variables",
    "text": "Explore missing values in variables\n\n# check for missing values in variables\ndf.isnull().sum()\n\nIP Mean            0\nIP Sd              0\nIP Kurtosis        0\nIP Skewness        0\nDM-SNR Mean        0\nDM-SNR Sd          0\nDM-SNR Kurtosis    0\nDM-SNR Skewness    0\ntarget_class       0\ndtype: int64\n\n\nWe can see that there are no missing values in the dataset.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#summary-of-numerical-variables",
    "href": "chap06_svm.html#summary-of-numerical-variables",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Summary of numerical variables",
    "text": "Summary of numerical variables\n\nThere are 9 numerical variables in the dataset.\n8 are continuous variables and 1 is discrete variable.\nThe discrete variable is target_class variable. It is also the target variable.\nThere are no missing values in the dataset.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#outliers-in-numerical-variables",
    "href": "chap06_svm.html#outliers-in-numerical-variables",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Outliers in numerical variables",
    "text": "Outliers in numerical variables\n\n# view summary statistics in numerical variables\nround(df.describe().transpose(),2)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nIP Mean\n17898.0\n111.08\n25.65\n5.81\n100.93\n115.08\n127.09\n192.62\n\n\nIP Sd\n17898.0\n46.55\n6.84\n24.77\n42.38\n46.95\n51.02\n98.78\n\n\nIP Kurtosis\n17898.0\n0.48\n1.06\n-1.88\n0.03\n0.22\n0.47\n8.07\n\n\nIP Skewness\n17898.0\n1.77\n6.17\n-1.79\n-0.19\n0.20\n0.93\n68.10\n\n\nDM-SNR Mean\n17898.0\n12.61\n29.47\n0.21\n1.92\n2.80\n5.46\n223.39\n\n\nDM-SNR Sd\n17898.0\n26.33\n19.47\n7.37\n14.44\n18.46\n28.43\n110.64\n\n\nDM-SNR Kurtosis\n17898.0\n8.30\n4.51\n-3.14\n5.78\n8.43\n10.70\n34.54\n\n\nDM-SNR Skewness\n17898.0\n104.86\n106.51\n-1.98\n34.96\n83.06\n139.31\n1191.00\n\n\ntarget_class\n17898.0\n0.09\n0.29\n0.00\n0.00\n0.00\n0.00\n1.00\n\n\n\n\n\n\n\nOn closer inspection, we can suspect that all the continuous variables may contain outliers.\nWe will draw boxplots to visualise outliers in the above variables.\n\n# draw boxplots to visualize outliers\nplt.figure(figsize=(24,20))\n\n\nplt.subplot(4, 2, 1)\nfig = df.boxplot(column='IP Mean')\nfig.set_title('')\nfig.set_ylabel('IP Mean')\n\n\nplt.subplot(4, 2, 2)\nfig = df.boxplot(column='IP Sd')\nfig.set_title('')\nfig.set_ylabel('IP Sd')\n\n\nplt.subplot(4, 2, 3)\nfig = df.boxplot(column='IP Kurtosis')\nfig.set_title('')\nfig.set_ylabel('IP Kurtosis')\n\n\nplt.subplot(4, 2, 4)\nfig = df.boxplot(column='IP Skewness')\nfig.set_title('')\nfig.set_ylabel('IP Skewness')\n\n\nplt.subplot(4, 2, 5)\nfig = df.boxplot(column='DM-SNR Mean')\nfig.set_title('')\nfig.set_ylabel('DM-SNR Mean')\n\n\nplt.subplot(4, 2, 6)\nfig = df.boxplot(column='DM-SNR Sd')\nfig.set_title('')\nfig.set_ylabel('DM-SNR Sd')\n\n\nplt.subplot(4, 2, 7)\nfig = df.boxplot(column='DM-SNR Kurtosis')\nfig.set_title('')\nfig.set_ylabel('DM-SNR Kurtosis')\n\n\nplt.subplot(4, 2, 8)\nfig = df.boxplot(column='DM-SNR Skewness')\nfig.set_title('')\nfig.set_ylabel('DM-SNR Skewness')\n\nText(0, 0.5, 'DM-SNR Skewness')\n\n\n\n\n\n\n\n\n\nThe above boxplots confirm that there are lot of outliers in these variables.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#handle-outliers-with-svms",
    "href": "chap06_svm.html#handle-outliers-with-svms",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Handle outliers with SVMs",
    "text": "Handle outliers with SVMs\nThere are 2 variants of SVMs. They are hard-margin variant of SVM and soft-margin variant of SVM.\nThe hard-margin variant of SVM does not deal with outliers. In this case, we want to find the hyperplane with maximum margin such that every training point is correctly classified with margin at least 1. This technique does not handle outliers well.\nAnother version of SVM is called soft-margin variant of SVM. In this case, we can have a few points incorrectly classified or classified with a margin less than 1. But for every such point, we have to pay a penalty in the form of C parameter, which controls the outliers. Low C implies we are allowing more outliers and high C implies less outliers.\nThe message is that since the dataset contains outliers, so the value of C should be high while training the model.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#check-the-distribution-of-variables",
    "href": "chap06_svm.html#check-the-distribution-of-variables",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Check the distribution of variables",
    "text": "Check the distribution of variables\nNow, We will plot the histograms to check distributions to find out if they are normal or skewed.\n\n# plot histogram to check distribution\nplt.figure(figsize=(24,20))\n\n\nplt.subplot(4, 2, 1)\nfig = df['IP Mean'].hist(bins=20)\nfig.set_xlabel('IP Mean')\nfig.set_ylabel('Number of pulsar stars')\n\n\nplt.subplot(4, 2, 2)\nfig = df['IP Sd'].hist(bins=20)\nfig.set_xlabel('IP Sd')\nfig.set_ylabel('Number of pulsar stars')\n\n\nplt.subplot(4, 2, 3)\nfig = df['IP Kurtosis'].hist(bins=20)\nfig.set_xlabel('IP Kurtosis')\nfig.set_ylabel('Number of pulsar stars')\n\n\n\nplt.subplot(4, 2, 4)\nfig = df['IP Skewness'].hist(bins=20)\nfig.set_xlabel('IP Skewness')\nfig.set_ylabel('Number of pulsar stars')\n\n\n\nplt.subplot(4, 2, 5)\nfig = df['DM-SNR Mean'].hist(bins=20)\nfig.set_xlabel('DM-SNR Mean')\nfig.set_ylabel('Number of pulsar stars')\n\n\n\nplt.subplot(4, 2, 6)\nfig = df['DM-SNR Sd'].hist(bins=20)\nfig.set_xlabel('DM-SNR Sd')\nfig.set_ylabel('Number of pulsar stars')\n\n\n\nplt.subplot(4, 2, 7)\nfig = df['DM-SNR Kurtosis'].hist(bins=20)\nfig.set_xlabel('DM-SNR Kurtosis')\nfig.set_ylabel('Number of pulsar stars')\n\n\nplt.subplot(4, 2, 8)\nfig = df['DM-SNR Skewness'].hist(bins=20)\nfig.set_xlabel('DM-SNR Skewness')\nfig.set_ylabel('Number of pulsar stars')\n\nText(0, 0.5, 'Number of pulsar stars')\n\n\n\n\n\n\n\n\n\nWe can see that all the 8 continuous variables are skewed.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#run-svm-with-rbf-kernel-and-c100.0",
    "href": "chap06_svm.html#run-svm-with-rbf-kernel-and-c100.0",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Run SVM with rbf kernel and C=100.0",
    "text": "Run SVM with rbf kernel and C=100.0\nWe have seen that there are outliers in our dataset. So, we should increase the value of C as higher C means fewer outliers. So, we will run SVM with kernel=rbf and C=100.0.\n\n# instantiate classifier with rbf kernel and C=100\nsvc=SVC(C=100.0) \n\n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score with rbf kernel and C=100.0 : 0.9832\n\n\nWe can see that we obtain a higher accuracy with C=100.0 as higher C means less outliers.\nNow, We will further increase the value of C=1000.0 and check accuracy.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#run-svm-with-rbf-kernel-and-c1000.0",
    "href": "chap06_svm.html#run-svm-with-rbf-kernel-and-c1000.0",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Run SVM with rbf kernel and C=1000.0",
    "text": "Run SVM with rbf kernel and C=1000.0\n\n# instantiate classifier with rbf kernel and C=1000\nsvc=SVC(C=1000.0) \n\n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with rbf kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score with rbf kernel and C=1000.0 : 0.9816\n\n\nIn this case, we can see that the accuracy had decreased with C=1000.0",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#run-svm-with-linear-kernel-and-c1.0",
    "href": "chap06_svm.html#run-svm-with-linear-kernel-and-c1.0",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Run SVM with linear kernel and C=1.0",
    "text": "Run SVM with linear kernel and C=1.0\n\n# instantiate classifier with linear kernel and C=1.0\nlinear_svc=SVC(kernel='linear', C=1.0) \n\n\n# fit classifier to training set\nlinear_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred_test=linear_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))\n\nModel accuracy score with linear kernel and C=1.0 : 0.9830",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#run-svm-with-linear-kernel-and-c100.0",
    "href": "chap06_svm.html#run-svm-with-linear-kernel-and-c100.0",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Run SVM with linear kernel and C=100.0",
    "text": "Run SVM with linear kernel and C=100.0\n\n# instantiate classifier with linear kernel and C=100.0\nlinear_svc100=SVC(kernel='linear', C=100.0) \n\n\n# fit classifier to training set\nlinear_svc100.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=linear_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score with linear kernel and C=100.0 : 0.9832",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#run-svm-with-linear-kernel-and-c1000.0",
    "href": "chap06_svm.html#run-svm-with-linear-kernel-and-c1000.0",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Run SVM with linear kernel and C=1000.0",
    "text": "Run SVM with linear kernel and C=1000.0\n\n# instantiate classifier with linear kernel and C=1000.0\nlinear_svc1000=SVC(kernel='linear', C=1000.0) \n\n\n# fit classifier to training set\nlinear_svc1000.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=linear_svc1000.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score with linear kernel and C=1000.0 : 0.9832\n\n\nWe can see that we can obtain higher accuracy with C=100.0 and C=1000.0 as compared to C=1.0.\nHere, y_test are the true class labels and y_pred are the predicted class labels in the test-set.\n\nCompare the train-set and test-set accuracy\nNow, I will compare the train-set and test-set accuracy to check for overfitting.\n\ny_pred_train = linear_svc.predict(X_train)\n\ny_pred_train\n\narray([0, 0, 1, ..., 0, 0, 0], shape=(14318,))\n\n\n\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\nTraining-set accuracy score: 0.9783\n\n\nWe can see that the training set and test-set accuracy are very much comparable.\n\n\nCheck for overfitting and underfitting\n\n# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(linear_svc.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(linear_svc.score(X_test, y_test)))\n\nTraining set score: 0.9783\nTest set score: 0.9830\n\n\nThe training-set accuracy score is 0.9783 while the test-set accuracy to be 0.9830. These two values are quite comparable. So, there is no question of overfitting.\n\n\nCompare model accuracy with null accuracy\nSo, the model accuracy is 0.9832. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\nSo, we should first check the class distribution in the test set.\n\n# check class distribution in test set\n\ny_test.value_counts()\n\ntarget_class\n0    3306\n1     274\nName: count, dtype: int64\n\n\nWe can see that the occurences of most frequent class 0 is 3306. So, we can calculate null accuracy by dividing 3306 by total number of occurences.\n\n# check null accuracy score\n\nnull_accuracy = (3306/(3306+274))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))\n\nNull accuracy score: 0.9235\n\n\nWe can see that our model accuracy score is 0.9830 but null accuracy score is 0.9235. So, we can conclude that our SVM classifier is doing a very good job in predicting the class labels.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#run-svm-with-polynomial-kernel-and-c1.0",
    "href": "chap06_svm.html#run-svm-with-polynomial-kernel-and-c1.0",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Run SVM with polynomial kernel and C=1.0",
    "text": "Run SVM with polynomial kernel and C=1.0\n\n# instantiate classifier with polynomial kernel and C=1.0\npoly_svc=SVC(kernel='poly', C=1.0) \n\n\n# fit classifier to training set\npoly_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=poly_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score with polynomial kernel and C=1.0 : 0.9807\n\n\n### Run SVM with polynomial kernel and C=100.0\n\n# instantiate classifier with polynomial kernel and C=100.0\npoly_svc100=SVC(kernel='poly', C=100.0) \n\n\n# fit classifier to training set\npoly_svc100.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=poly_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score with polynomial kernel and C=1.0 : 0.9824\n\n\nPolynomial kernel gives poor performance. It may be overfitting the training set.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#run-svm-with-sigmoid-kernel-and-c1.0",
    "href": "chap06_svm.html#run-svm-with-sigmoid-kernel-and-c1.0",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Run SVM with sigmoid kernel and C=1.0",
    "text": "Run SVM with sigmoid kernel and C=1.0\n\n# instantiate classifier with sigmoid kernel and C=1.0\nsigmoid_svc=SVC(kernel='sigmoid', C=1.0) \n\n\n# fit classifier to training set\nsigmoid_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score with sigmoid kernel and C=1.0 : 0.8858",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#run-svm-with-sigmoid-kernel-and-c100.0",
    "href": "chap06_svm.html#run-svm-with-sigmoid-kernel-and-c100.0",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Run SVM with sigmoid kernel and C=100.0",
    "text": "Run SVM with sigmoid kernel and C=100.0\n\n# instantiate classifier with sigmoid kernel and C=100.0\nsigmoid_svc100=SVC(kernel='sigmoid', C=100.0) \n\n\n# fit classifier to training set\nsigmoid_svc100.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score with sigmoid kernel and C=100.0 : 0.8855\n\n\nWe can see that sigmoid kernel is also performing poorly just like with polynomial kernel.\n\nComments\nWe get maximum accuracy with rbf and linear kernel with C=100.0. and the accuracy is 0.9832. Based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\nBut, this is not true. Here, we have an imbalanced dataset. The problem is that accuracy is an inadequate measure for quantifying predictive performance in the imbalanced dataset problem.\nSo, we must explore alternative metrices that provide better guidance in selecting models. In particular, we would like to know the underlying distribution of values and the type of errors our classifer is making.\nOne such metric to analyze the model performance in imbalanced classes problem is Confusion matrix.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#classification-report",
    "href": "chap06_svm.html#classification-report",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Classification Report",
    "text": "Classification Report\nClassification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. We have described these terms in later.\nWe can print a classification report as follows:-\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_test))\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      3306\n           1       0.93      0.84      0.88       274\n\n    accuracy                           0.98      3580\n   macro avg       0.96      0.92      0.94      3580\nweighted avg       0.98      0.98      0.98      3580\n\n\n\n\nClassification accuracy\n\nTP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]\n\n\n# print classification accuracy\n\nclassification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n\nClassification accuracy : 0.9830\n\n\n\n\nClassification error\n\n# print classification error\n\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))\n\nClassification error : 0.0170\n\n\n\n\nPrecision\nPrecision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP).\nSo, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\nMathematically, precision can be defined as the ratio of TP to (TP + FP).\n\n# print precision score\n\nprecision = TP / float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))\n\nPrecision : 0.9949\n\n\n\n\nRecall\nRecall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall is also called Sensitivity.\nRecall identifies the proportion of correctly predicted actual positives.\nMathematically, recall can be defined as the ratio of TP to (TP + FN).\n\nrecall = TP / float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))\n\nRecall or Sensitivity : 0.9868\n\n\n\n\nTrue Positive Rate\nTrue Positive Rate is synonymous with Recall.\n\ntrue_positive_rate = TP / float(TP + FN)\n\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n\nTrue Positive Rate : 0.9868\n\n\n\n\nFalse Positive Rate\n\nfalse_positive_rate = FP / float(FP + TN)\n\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n\nFalse Positive Rate : 0.0688\n\n\n\n\nSpecificity\n\nspecificity = TN / (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))\n\nSpecificity : 0.9312\n\n\n\n\nf1-score\nf1-score is the weighted harmonic mean of precision and recall. The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy.\n\n\nSupport\nSupport is the actual number of occurrences of the class in our dataset.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#stratified-k-fold-cross-validation-with-shuffle-split-with-linear-kernel",
    "href": "chap06_svm.html#stratified-k-fold-cross-validation-with-shuffle-split-with-linear-kernel",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Stratified k-Fold Cross Validation with shuffle split with linear kernel",
    "text": "Stratified k-Fold Cross Validation with shuffle split with linear kernel\n\nfrom sklearn.model_selection import KFold\n\n\nkfold=KFold(n_splits=5, shuffle=True, random_state=0)\n\n\nlinear_svc=SVC(kernel='linear')\n\n\nlinear_scores = cross_val_score(linear_svc, X, y, cv=kfold)\n\n\n# print cross-validation scores with linear kernel\n\nprint('Stratified cross-validation scores with linear kernel:{}'.format(linear_scores))\n\nStratified cross-validation scores with linear kernel:[0.98296089 0.97458101 0.97988827 0.97876502 0.97848561]\n\n\n\n# print average cross-validation score with linear kernel\n\nprint('Average stratified cross-validation score with linear kernel:{:.4f}'.format(linear_scores.mean()))\n\nAverage stratified cross-validation score with linear kernel:0.9789",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap06_svm.html#stratified-k-fold-cross-validation-with-shuffle-split-with-rbf-kernel",
    "href": "chap06_svm.html#stratified-k-fold-cross-validation-with-shuffle-split-with-rbf-kernel",
    "title": "\nChapter # 06 (Part B)  Support Vector Machine (SVM)",
    "section": "Stratified k-Fold Cross Validation with shuffle split with rbf kernel",
    "text": "Stratified k-Fold Cross Validation with shuffle split with rbf kernel\n\nrbf_svc=SVC(kernel='rbf')\n\n\nrbf_scores = cross_val_score(rbf_svc, X, y, cv=kfold)\n\n\n# print cross-validation scores with rbf kernel\n\nprint('Stratified Cross-validation scores with rbf kernel:{}'.format(rbf_scores))\n\nStratified Cross-validation scores with rbf kernel:[0.97849162 0.97011173 0.97318436 0.9709416  0.96982397]\n\n\n\n# print average cross-validation score with rbf kernel\n\nprint('Average stratified cross-validation score with rbf kernel:{:.4f}'.format(rbf_scores.mean()))\n\nAverage stratified cross-validation score with rbf kernel:0.9725\n\n\n\nComments\nI obtain higher average stratified k-fold cross-validation score of 0.9789 with linear kernel but the model accuracy is 0.9832. So, stratified cross-validation technique does not help to improve the model performance.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part B) <br> Support Vector Machine (SVM)"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#assumptions-of-linear-discriminant-analysis",
    "href": "chap05_discriminant.html#assumptions-of-linear-discriminant-analysis",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "2 Assumptions of Linear Discriminant Analysis",
    "text": "2 Assumptions of Linear Discriminant Analysis\nDiscriminant analysis assumes that:\n\nThe data is normally distributed.\nMeans of each class are specific to that class.\nAll classes have a common covariance matrix.\n\nIf these assumptions are realized, DA generates a linear decision boundary.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#loading-python-packages",
    "href": "chap05_discriminant.html#loading-python-packages",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "3 Loading Python Packages",
    "text": "3 Loading Python Packages\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Python version that is used for this code file is {}'.format(sys.version))\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\nprint ('The Panda version that is used for this code file is {}'.format(pd.__version__))\nprint ('The Numpy version that is used for this code file is {}'.format(np.__version__))\n\nThe Python version that is used for this code file is 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\nThe Scikit-learn version that is used for this code file is 1.6.0\nThe Panda version that is used for this code file is 2.2.3\nThe Numpy version that is used for this code file is 2.2.1",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#working-directory",
    "href": "chap05_discriminant.html#working-directory",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "4 Working Directory",
    "text": "4 Working Directory\n\nimport os\nos.getcwd()\n\n\nfor x in os.listdir():\n  print (x)",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#importing-datasets",
    "href": "chap05_discriminant.html#importing-datasets",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "5 Importing Datasets",
    "text": "5 Importing Datasets\n\nfrom sklearn import datasets\ndataset = datasets.load_wine()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#metadata-of-the-imported-dataset",
    "href": "chap05_discriminant.html#metadata-of-the-imported-dataset",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "6 Metadata of the Imported Dataset",
    "text": "6 Metadata of the Imported Dataset\n\ndataset.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])\n\n\n\ndataset['data']\n\narray([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n        1.065e+03],\n       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n        1.050e+03],\n       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n        1.185e+03],\n       ...,\n       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n        8.350e+02],\n       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n        8.400e+02],\n       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n        5.600e+02]], shape=(178, 13))\n\n\n\ndataset['target']\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n\n\n\ndataset['target_names']\n\narray(['class_0', 'class_1', 'class_2'], dtype='&lt;U7')\n\n\n\n# Creating Data frame from the array \ndata = pd.DataFrame(dataset['data'], columns = dataset['feature_names'])\ndata.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n\n\n\n\n\n\n\n\n# Feature Vector \nfeatures_df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n# Target Vector \ntarget_df = pd.Categorical.from_codes(dataset.target, dataset.target_names)\n\n\ntarget_df\n\n['class_0', 'class_0', 'class_0', 'class_0', 'class_0', ..., 'class_2', 'class_2', 'class_2', 'class_2', 'class_2']\nLength: 178\nCategories (3, object): ['class_0', 'class_1', 'class_2']\n\n\n\n# Joining the above two datasets \ndf = features_df.join(pd.Series(target_df, name = 'class'))\ndf.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nclass\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\nclass_0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\nclass_0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\nclass_0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\nclass_0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\nclass_0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype   \n---  ------                        --------------  -----   \n 0   alcohol                       178 non-null    float64 \n 1   malic_acid                    178 non-null    float64 \n 2   ash                           178 non-null    float64 \n 3   alcalinity_of_ash             178 non-null    float64 \n 4   magnesium                     178 non-null    float64 \n 5   total_phenols                 178 non-null    float64 \n 6   flavanoids                    178 non-null    float64 \n 7   nonflavanoid_phenols          178 non-null    float64 \n 8   proanthocyanins               178 non-null    float64 \n 9   color_intensity               178 non-null    float64 \n 10  hue                           178 non-null    float64 \n 11  od280/od315_of_diluted_wines  178 non-null    float64 \n 12  proline                       178 non-null    float64 \n 13  class                         178 non-null    category\ndtypes: category(1), float64(13)\nmemory usage: 18.5 KB\n\n\n\ndf.columns\nnum_features= dataset.feature_names\nnum_features\n# Looping functions \ndf.groupby('class')[num_features].mean().transpose()\n\n\n\n\n\n\n\nclass\nclass_0\nclass_1\nclass_2\n\n\n\n\nalcohol\n13.744746\n12.278732\n13.153750\n\n\nmalic_acid\n2.010678\n1.932676\n3.333750\n\n\nash\n2.455593\n2.244789\n2.437083\n\n\nalcalinity_of_ash\n17.037288\n20.238028\n21.416667\n\n\nmagnesium\n106.338983\n94.549296\n99.312500\n\n\ntotal_phenols\n2.840169\n2.258873\n1.678750\n\n\nflavanoids\n2.982373\n2.080845\n0.781458\n\n\nnonflavanoid_phenols\n0.290000\n0.363662\n0.447500\n\n\nproanthocyanins\n1.899322\n1.630282\n1.153542\n\n\ncolor_intensity\n5.528305\n3.086620\n7.396250\n\n\nhue\n1.062034\n1.056282\n0.682708\n\n\nod280/od315_of_diluted_wines\n3.157797\n2.785352\n1.683542\n\n\nproline\n1115.711864\n519.507042\n629.895833",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#analysis-of-variance-anova",
    "href": "chap05_discriminant.html#analysis-of-variance-anova",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "7 Analysis of Variance (ANOVA)",
    "text": "7 Analysis of Variance (ANOVA)\nOne-way ANOVA (also known as “Analysis of Variance”) is a test that is used to find out whether there exists a statistically significant difference between the mean values of more than one group.\nSee Figure 1 to know when you should use which correlation\n\n\n\n\n\n\nFigure 1: When You Should Use Which Correlation?\n\n\n\nA one-way ANOVA has the below given null and alternative hypotheses:\nH0 (Null hypothesis): μ1 = μ2 = μ3 = … = μk (It implies that the means of all the population are equal)\nH1 (Alternate hypothesis): It states that there will be at least one population mean that differs from the rest\n\ndataset.target_names\nalc_class0 = df[df['class']=='class_0']['alcohol']\ntype(alc_class0)\nalc_class1 = df[df['class']=='class_1']['alcohol']\nalc_class2 = df[df['class']=='class_2']['alcohol']\n\n\nfrom scipy.stats import f_oneway\nf_oneway(alc_class0,alc_class1,alc_class2)\n\nF_onewayResult(statistic=np.float64(135.07762424279912), pvalue=np.float64(3.319503795619639e-36))\n\n\nThe F statistic is 135.0776 and p-value is 0.000. Since the p-value is less than 0.05, we reject Null Hypothesis (H0). The findings imply that there exists a difference between three groups for the variable alcohol.\nSo, where does the difference come from? We can perform Post Hoc Analysis to check where does the differences come from\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\ntukey = pairwise_tukeyhsd(endog=df['alcohol'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey.summary()\n\n\nMultiple Comparison of Means - Tukey HSD, FWER=0.05\n\n\ngroup1\ngroup2\nmeandiff\np-adj\nlower\nupper\nreject\n\n\nclass_0\nclass_1\n-1.466\n0.0\n-1.6792\n-1.2528\nTrue\n\n\nclass_0\nclass_2\n-0.591\n0.0\n-0.8262\n-0.3558\nTrue\n\n\nclass_1\nclass_2\n0.875\n0.0\n0.6489\n1.1011\nTrue\n\n\n\n\n\nSee Figure 2 for the sources of differences of target variable and alcohol\n\ntukey.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Where Does the Difference Come From between Target variable and variable alcohol?\n\n\n\n\n\n\n7.1 Using ANOVA for Feature Selection\n\nlist(dataset.target_names)\n\n[np.str_('class_0'), np.str_('class_1'), np.str_('class_2')]\n\n\n\ntukey_malic = pairwise_tukeyhsd(endog=df['malic_acid'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey_malic.summary()\n\n\nMultiple Comparison of Means - Tukey HSD, FWER=0.05\n\n\ngroup1\ngroup2\nmeandiff\np-adj\nlower\nupper\nreject\n\n\nclass_0\nclass_1\n-0.078\n0.8855\n-0.4703\n0.3143\nFalse\n\n\nclass_0\nclass_2\n1.3231\n0.0\n0.8902\n1.7559\nTrue\n\n\nclass_1\nclass_2\n1.4011\n0.0\n0.9849\n1.8172\nTrue\n\n\n\n\n\nSee Figure 3 for the difference between target variable and malic_acid\n\ntukey_malic.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Where Does the Difference Come From between Target variable and variable malic_acid?\n\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ndf.rename(columns = {\n  'od280/od315_of_diluted_wines': 'diluted_wines'}, inplace = True)",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#linear-discriminant-analysis",
    "href": "chap05_discriminant.html#linear-discriminant-analysis",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "8 Linear Discriminant Analysis",
    "text": "8 Linear Discriminant Analysis\n\nX = dataset.data\ny = dataset.target\ntarget_names = dataset.target_names\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nlda = LinearDiscriminantAnalysis(n_components = 2)\n\n\nX_r2 = lda.fit(X,y).transform(X)\n\n\nX_r2[0:10,]\n\narray([[-4.70024401,  1.97913835],\n       [-4.30195811,  1.17041286],\n       [-3.42071952,  1.42910139],\n       [-4.20575366,  4.00287148],\n       [-1.50998168,  0.4512239 ],\n       [-4.51868934,  3.21313756],\n       [-4.52737794,  3.26912179],\n       [-4.14834781,  3.10411765],\n       [-3.86082876,  1.95338263],\n       [-3.36662444,  1.67864327]])\n\n\n\nlda.explained_variance_ratio_\n\narray([0.68747889, 0.31252111])\n\n\n\n8.1 Plotting the Dataset\n\nplt.figure(figsize = (15,8))\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target,cmap = 'gnuplot', alpha = 0.7)\nplt.xlabel('DF1')\nplt.ylabel('DF2')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.2 Distribution of LDA Components\n\ndf_lda = pd.DataFrame(zip(X_r2[:,0], X_r2[:,1],y), columns = [\"ld1\", \"ld2\", \"class\"])\nsns.set(rc={'figure.figsize':(12,8)})\nplt.subplot(2,1,1)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld1')\nplt.subplot(2,1,2)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld2')\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#using-lda-to-solve-classification-problem",
    "href": "chap05_discriminant.html#using-lda-to-solve-classification-problem",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "9 Using LDA to Solve Classification Problem",
    "text": "9 Using LDA to Solve Classification Problem\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 2024)\n\n\n9.1 Training the Model\n\nlda_model = LinearDiscriminantAnalysis(n_components = 2)\nlda_model.fit(X_train, y_train)\n\nLinearDiscriminantAnalysis(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearDiscriminantAnalysis?Documentation for LinearDiscriminantAnalysisiFittedLinearDiscriminantAnalysis(n_components=2) \n\n\n\n\n9.2 Testing the Model\n\ny_pred = lda_model.predict(X_test)\n\n\n\n9.3 Checking Model Accuracy\n\nfrom sklearn.metrics import accuracy_score\nprint (\"The Accuracy of LDA Model is %0.2f%%.\" % (accuracy_score(y_test, y_pred)*100))\n\nThe Accuracy of LDA Model is 100.00%.\n\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#cross-validation",
    "href": "chap05_discriminant.html#cross-validation",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "10 Cross Validation",
    "text": "10 Cross Validation\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 50, random_state = 1)\nscores = cross_val_score(lda_model, X,y, scoring = \"accuracy\", cv = cv, n_jobs = -1)\nprint(np.mean(scores))",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#lda-vs-pca-visualization-difference",
    "href": "chap05_discriminant.html#lda-vs-pca-visualization-difference",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "11 LDA vs PCA (Visualization Difference)",
    "text": "11 LDA vs PCA (Visualization Difference)\n\n11.1 PCA Model\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_pca = pca.fit(X).transform(X)\n\n\nfrom pylab import *\nsubplot(2,1,1)\ntitle (\"PCA\")\nplt.scatter(X_pca[:,0], X_pca[:,1], c = dataset.target, cmap = \"gnuplot\")\nsubplot(2,1,2)\ntitle (\"LDA\")\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target, cmap = \"gnuplot\")\nplt.show()\n\n\n\n\n\n\n\n\nBoth algorithms have successfully reduced the components but created different clusters because both have reduced the components based on different principles.\nNow let’s also visualize and compare the distributions of each of the algorithms on their respective components. Here we will visualize the distribution of the first component of each algorithm (LDA-1 and PCA-1).\n\n# creating dataframs\ndf=pd.DataFrame(zip(X_pca[:,0],X_r2[:,0],y),columns=[\"pc1\",\"ld1\",\"class\"])\n# plotting the lda1\nplt.subplot(2,1,1)\nsns.boxplot(x='class', y='ld1', data=df)\n# plotting pca1\nplt.subplot(2,1,2)\nsns.boxplot(x='class', y='pc1', data=df)\nplt.show()\n\n\n\n\n\n\n\n\nThere is a slight difference in the distribution of both of the algorithms. For example, the PCA result shows outliers only at the first target variable, whereas the LDA result contains outliers for every target variable.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#variance-covariance-matrix",
    "href": "chap05_discriminant.html#variance-covariance-matrix",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "12 Variance Covariance Matrix",
    "text": "12 Variance Covariance Matrix\nTo calculate the covariance matrix in Python using NumPy, you can import NumPy as np, create or load your data as a NumPy array, subtract the mean of each column from the data, transpose the array, multiply the transposed array and original array, divide the multiplied array by the number of observations, and print the array. Alternatively, you can use the np.cov function which takes the data array as an input and returns the covariance matrix as an output.\nTo learn more about variance-covariance matrix.\nTo learn more about eigenvalues and eigenvectors.\n\nA = [45, 37, 42, 35, 39]\nB = [38, 31, 26, 28, 33]\nC = [10, 15, 17, 21, 12]\n\ndata = np.array([A, B, C])\n\ncov_matrix = np.cov(data, bias=True)\nprint(cov_matrix)\n\n[[ 12.64   7.68  -9.6 ]\n [  7.68  17.36 -13.8 ]\n [ -9.6  -13.8   14.8 ]]\n\n\n\nnp.var(A)\n\nnp.float64(12.64)\n\n\n\nnp.var(C)\n\nnp.float64(14.8)\n\n\n\n12.1 Eigenvalues and Eigenvector for Variance-covariance Matrix\n\n# eigendecomposition\nfrom numpy.linalg import eig\n\n\n# calculate eigendecomposition\nvalues, vectors = eig(cov_matrix)\n# Eigenvalues \nprint(values)\n\n[36.22111819  6.98906964  1.58981217]\n\n\n\n# Eigenvectors \nprint(vectors)\n\n[[-0.45932764 -0.83268027  0.30929225]\n [-0.63870049  0.55159313  0.53647618]\n [ 0.61731661 -0.04887322  0.78519527]]",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#regularized-discriminant-analysis",
    "href": "chap05_discriminant.html#regularized-discriminant-analysis",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "13 Regularized Discriminant Analysis",
    "text": "13 Regularized Discriminant Analysis\nSince regularization techniques have been highly successful in the solution of ill-posed and poorly-posed inverse problems so to mitigate this problem the most reliable way is to use the regularization technique.\n\nA poorly posed problem occurs when the number of parameters to be estimated is comparable to the number of observations.\nSimilarly,ill-posed if that number exceeds the sample size.\n\nIn these cases the parameter estimates can be highly unstable, giving rise to high variance. Regularization would help to improve the estimates by shifting them away from their sample-based values towards values that are more physically valid; this would be achieved by applying shrinkage to each class.\nWhile regularization reduces the variance associated with the sample-based estimate, it may also increase bias. This process known as bias-variance trade-off is generally controlled by one or more degree-of-belief parameters that determine how strongly biasing towards “plausible” values of population parameters takes place.\nWhenever the sample size is not significantly greater than the dimension of measurement space for any class, Quantitative discriminant analysis (QDA) is ill-posed. Typically, regularization is applied to a discriminant analysis by replacing the individual class sample covariance matrices with the average weights assigned to the eigenvalues.\nThis applies a considerable degree of regularization by substantially reducing the number of parameters to be estimated. The regularization parameter () which is added to the equation of QDA and LDA takes a value between 0 to 1. It controls the degree of shrinkage of the individual class covariance matrix estimates toward the pooled estimate. Values between these limits represent degrees of regularization.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,confusion_matrix\nfrom imblearn.over_sampling import SMOTE # To install module, run this line of code - pip install imblearn\nfrom sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n\n\n# Reading a new dataset \ndf = pd.read_csv('DATA/healthcare-dataset-stroke-data.csv')\nprint(\"Records = \", df.shape[0], \"\\nFeatures = \", df.shape[1])\n\nRecords =  5110 \nFeatures =  12\n\n\n\ndf.sample(5)\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n3837\n27854\nFemale\n23.0\n0\n0\nNo\nPrivate\nRural\n96.28\n31.1\nnever smoked\n0\n\n\n3969\n33185\nMale\n59.0\n0\n0\nNo\nGovt_job\nUrban\n83.60\n27.5\nformerly smoked\n0\n\n\n3924\n39958\nMale\n18.0\n0\n0\nNo\nPrivate\nRural\n118.93\n22.4\nnever smoked\n0\n\n\n732\n31308\nFemale\n49.0\n0\n0\nYes\nPrivate\nUrban\n114.50\n35.9\nformerly smoked\n0\n\n\n794\n21688\nFemale\n42.0\n0\n0\nYes\nPrivate\nRural\n88.31\n24.0\nsmokes\n0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5110 entries, 0 to 5109\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 5110 non-null   int64  \n 1   gender             5110 non-null   object \n 2   age                5110 non-null   float64\n 3   hypertension       5110 non-null   int64  \n 4   heart_disease      5110 non-null   int64  \n 5   ever_married       5110 non-null   object \n 6   work_type          5110 non-null   object \n 7   Residence_type     5110 non-null   object \n 8   avg_glucose_level  5110 non-null   float64\n 9   bmi                4909 non-null   float64\n 10  smoking_status     5110 non-null   object \n 11  stroke             5110 non-null   int64  \ndtypes: float64(3), int64(4), object(5)\nmemory usage: 479.2+ KB\n\n\n\n# Missing Values \n(df.isnull().sum()/len(df)*100)\n\nid                   0.000000\ngender               0.000000\nage                  0.000000\nhypertension         0.000000\nheart_disease        0.000000\never_married         0.000000\nwork_type            0.000000\nResidence_type       0.000000\navg_glucose_level    0.000000\nbmi                  3.933464\nsmoking_status       0.000000\nstroke               0.000000\ndtype: float64\n\n\n\n# Dropping the Missing Observations\ndf.dropna(axis = 0, inplace = True)\ndf.shape\n\n(4909, 12)\n\n\n\n# Creating the Dummies \ndf_pre = pd.get_dummies(df, drop_first = True)\ndf_pre.sample(5)\n\n\n\n\n\n\n\n\nid\nage\nhypertension\nheart_disease\navg_glucose_level\nbmi\nstroke\ngender_Male\ngender_Other\never_married_Yes\nwork_type_Never_worked\nwork_type_Private\nwork_type_Self-employed\nwork_type_children\nResidence_type_Urban\nsmoking_status_formerly smoked\nsmoking_status_never smoked\nsmoking_status_smokes\n\n\n\n\n2600\n44112\n51.0\n0\n0\n219.92\n33.5\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\nFalse\n\n\n4691\n25878\n55.0\n0\n0\n97.68\n47.1\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1389\n68235\n12.0\n0\n0\n86.00\n20.1\n0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n4597\n40842\n29.0\n0\n0\n108.14\n25.1\n0\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n768\n59521\n33.0\n0\n0\n74.88\n31.6\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\n# Training and Testing the Split \nX = df_pre.drop(['stroke'], axis = 1)\ny = df_pre['stroke']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 25)\n\n\n# Building the LDA\nLDA = LinearDiscriminantAnalysis()\nLDA.fit_transform(X_train, y_train)\nX_test['predictions'] = LDA.predict(X_test)\nConfusionMatrixDisplay.from_predictions(y_test, X_test['predictions'])\nplt.show()\n\n\n\n\n\n\n\n\n\ntn, fp, fn, tp = confusion_matrix(list(y_test), list(X_test['predictions']), labels=[0, 1]).ravel()\n\n\nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\nprint(\"Precision score\",precision_score(y_test,X_test['predictions']))\n\nTrue Positive : 6\nTrue Negative : 1397\nFalse Positive : 13\nFalse Negative : 57\nPrecision score 0.3157894736842105\n\n\nIt has only 32% precision rate, which is very poor performance.\n\nprint(\"Accuracy Score\",accuracy_score(y_test,X_test['predictions']))\n\nAccuracy Score 0.9524779361846571\n\n\nThe accuracy is approximately 95%, but the precision is 32%.\n\n13.1 Cross Validation of the Dataset\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n\n#Define method to evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=50, random_state=1)\n\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n\n0.23822585829203477\n\n\n\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n\n0.9466250593260561\n\n\nEven after cross validation, the precision is about 24% and the accuracy is 95% approximately. There is no significant improvement of the metrics of the model.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#regularizing-and-shrinking-the-lda",
    "href": "chap05_discriminant.html#regularizing-and-shrinking-the-lda",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "14 Regularizing and Shrinking the LDA",
    "text": "14 Regularizing and Shrinking the LDA\n\ndf_pre['stroke'].value_counts()\n\nstroke\n0    4700\n1     209\nName: count, dtype: int64\n\n\nAs observed by the value count of the dependent variable the data is imbalanced as the quantity of 1’s is approx 4% of the total dependent variable. So, it needs to be balanced for the learner to be a good predictor.\n\n14.1 Balancing the Dependent Variable\nThere are two ways by which the data can be synthesized: one by oversampling and the second, by undersampling. In this scenario, oversampling is better which will synthesize the lesser category linear interpolation.\n\noversample = SMOTE()\nX_smote, y_smote = oversample.fit_resample(X, y)\nXs_train, Xs_test, ys_train, ys_test = train_test_split(X_smote, y_smote, test_size=0.30, random_state=42)\n\nThe imbalance is mitigated by using the Synthetic Minority Oversampling Technique (SMOTE) but this will not help much we also need to regularize the leaner by using the GridSearchCV which will find the best parameters for the learner and add a penalty to the solver which will shrink the eigenvalue i.e regularization.\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\ngrid = dict()\ngrid['solver'] = ['eigen','lsqr']\ngrid['shrinkage'] = ['auto',0.2,1,0.3,0.5]\nsearch = GridSearchCV(LDA, grid, scoring='precision', cv=cv, n_jobs=-1)\nresults = search.fit(Xs_train, ys_train)\nprint('Precision: %.3f' % results.best_score_)\nprint('Configuration:',results.best_params_)\n\nPrecision: 0.879\nConfiguration: {'shrinkage': 'auto', 'solver': 'eigen'}\n\n\nThe precision score jumped right from 35% to 87% with the help of regularization and shrinkage of the learner and the best solver for the Linear Discriminant Analysis is eigen and the shrinkage method is auto which uses the Ledoit-Wolf lemma for finding the shrinkage penalty.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#building-the-regularized-discriminant-analysis-rda",
    "href": "chap05_discriminant.html#building-the-regularized-discriminant-analysis-rda",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "15 Building the Regularized Discriminant Analysis (RDA)",
    "text": "15 Building the Regularized Discriminant Analysis (RDA)\n\n# Build the RDA\nLDA_final=LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')\nLDA_final.fit_transform(Xs_train,ys_train)\nXs_test['predictions']=LDA_final.predict(Xs_test)\nConfusionMatrixDisplay.from_predictions(ys_test, Xs_test['predictions'])\nplt.show()\n \ntn, fp, fn, tp = confusion_matrix(list(ys_test), list(Xs_test['predictions']), labels=[0, 1]).ravel()\n \nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\n\n\n\n\n\n\n\n\nTrue Positive : 1259\nTrue Negative : 1235\nFalse Positive : 172\nFalse Negative : 154\n\n\n\nprint(\"Precision score\",np.round(precision_score(ys_test,Xs_test['predictions']),3))\n\nPrecision score 0.88\n\n\n\nprint(\"Accuracy score\",np.round(accuracy_score(ys_test,Xs_test['predictions']),3))\n\nAccuracy score 0.884",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#conclusion",
    "href": "chap05_discriminant.html#conclusion",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "16 Conclusion",
    "text": "16 Conclusion",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap03_slide.html#introduction",
    "href": "chap03_slide.html#introduction",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#introduction-contd",
    "href": "chap03_slide.html#introduction-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Introduction (Cont’d)",
    "text": "Introduction (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#what-is-knn-algorithm",
    "href": "chap03_slide.html#what-is-knn-algorithm",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "What is KNN Algorithm?",
    "text": "What is KNN Algorithm?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#how-does-knn-learn",
    "href": "chap03_slide.html#how-does-knn-learn",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "How Does KNN Learn?",
    "text": "How Does KNN Learn?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#how-does-knn-learn-contd",
    "href": "chap03_slide.html#how-does-knn-learn-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "How Does KNN Learn? (Cont’d)",
    "text": "How Does KNN Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#how-does-knn-learn-contd-1",
    "href": "chap03_slide.html#how-does-knn-learn-contd-1",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "How Does KNN Learn? (Cont’d)",
    "text": "How Does KNN Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#what-happens-if-the-vote-is-tied",
    "href": "chap03_slide.html#what-happens-if-the-vote-is-tied",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "What Happens if the Vote is Tied?",
    "text": "What Happens if the Vote is Tied?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#fyi",
    "href": "chap03_slide.html#fyi",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "FYI",
    "text": "FYI",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#building-knn-algorithm",
    "href": "chap03_slide.html#building-knn-algorithm",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Building KNN Algorithm",
    "text": "Building KNN Algorithm",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#defining-the-task",
    "href": "chap03_slide.html#defining-the-task",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Defining the Task",
    "text": "Defining the Task",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#define-the-learner",
    "href": "chap03_slide.html#define-the-learner",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Define the Learner",
    "text": "Define the Learner",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#train-the-model",
    "href": "chap03_slide.html#train-the-model",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Train the Model",
    "text": "Train the Model",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#making-predictions-using-our-model-and-evaluating-performance-of-the-model",
    "href": "chap03_slide.html#making-predictions-using-our-model-and-evaluating-performance-of-the-model",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Making Predictions using our Model and Evaluating Performance of the Model",
    "text": "Making Predictions using our Model and Evaluating Performance of the Model",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#bias-variance-trade-off-two-sources-of-model-error",
    "href": "chap03_slide.html#bias-variance-trade-off-two-sources-of-model-error",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Bias-Variance Trade Off – Two Sources of Model Error",
    "text": "Bias-Variance Trade Off – Two Sources of Model Error",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#bias-variance-trade-off-two-sources-of-model-error-contd",
    "href": "chap03_slide.html#bias-variance-trade-off-two-sources-of-model-error-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Bias-Variance Trade Off – Two Sources of Model Error (Cont’d)",
    "text": "Bias-Variance Trade Off – Two Sources of Model Error (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#holdout-cross-validation-cv",
    "href": "chap03_slide.html#holdout-cross-validation-cv",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Holdout Cross-validation (CV)",
    "text": "Holdout Cross-validation (CV)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#holdout-cross-validation-cv-contd",
    "href": "chap03_slide.html#holdout-cross-validation-cv-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Holdout Cross-validation (CV) (Cont’d)",
    "text": "Holdout Cross-validation (CV) (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#k-fold-cv",
    "href": "chap03_slide.html#k-fold-cv",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "K-fold CV",
    "text": "K-fold CV",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#k-fold-cv-contd",
    "href": "chap03_slide.html#k-fold-cv-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "K-fold CV (Cont’d)",
    "text": "K-fold CV (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#leave-one-out-loo-cv",
    "href": "chap03_slide.html#leave-one-out-loo-cv",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Leave-One-Out (LOO) CV",
    "text": "Leave-One-Out (LOO) CV",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#parameters-vs-hyperparameters",
    "href": "chap03_slide.html#parameters-vs-hyperparameters",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Parameters vs Hyperparameters",
    "text": "Parameters vs Hyperparameters",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#parameters-vs-hyperparameters-contd",
    "href": "chap03_slide.html#parameters-vs-hyperparameters-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Parameters vs Hyperparameters (Cont’d)",
    "text": "Parameters vs Hyperparameters (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#questions-or-queries",
    "href": "chap03_slide.html#questions-or-queries",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Questions or Queries",
    "text": "Questions or Queries\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nQueries",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap02_RpythonEDA.html",
    "href": "chap02_RpythonEDA.html",
    "title": "\nChapter # 02  An Introduction to R and Python",
    "section": "",
    "text": "Please see the Appendix to learn about R and Python\n\n\n\n Back to top",
    "crumbs": [
      "Modules & Slides",
      "Introduction",
      "<center> Chapter # 02 <br> An Introduction to R and Python"
    ]
  },
  {
    "objectID": "chap01_intro.html",
    "href": "chap01_intro.html",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "",
    "text": "To get the slides of this chapter, please see the “slide” dropdown menu on “Modules & Slides” tab on the menu bar of this website.\n\n\n\n Back to top",
    "crumbs": [
      "Modules & Slides",
      "Introduction",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The website is for the course BSAN405 - Machine Learning in Business - in College of Business & Analytics in Southern Illinois University Carbondale (SIUC). If you have questions or queries, please email the intructor.\n\n\n\n Back to top"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Books are not required for the course. All of the books are suggested.\n\nMachine Learning with R, the tidyverse, and mlr by Hefin I. Rhys. The book is available on Manning Publications website. The hard copy of the book is not required. Online copy is fine.\n\n\n\n\n\n\n\nPython Data Science Handbook: Essential Tools for Working with Data by Jake VanderPlas. The book is available on Github. The hard copy of the book is not required. Online copy is fine. This is the first edition of the book. The second edition of the book is also available. If you are interested, you can buy one from Amazon.\n\n\n\n\n\n\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron. The book is available on Amazon. The hard copy of the book is not required. A pdf copy of the book is available from me on request.\n\n\n\n\n\n\n\nSupplemental Reading\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund is an excellent book to learn about the basics of R. The online version of 2nd Edition of the book is available free.\n\n\n\n\n\n\n\nTo learn about the mathematics underlying many Machine Learning (ML) algorithms, Mathematics for Machine Learning by Marc Deisenroth, A Aldo Faisal, and Cheng Ong can be used -\n\n\n\n\n\n\n\nTo learn about the Ethics in Machine Learning, Fairness and Machine Learning by Solon Barocas, Mortiz Hardt, and Arvind Narayanan can be used. The book is not needed to be purchased. An online version of the book is available.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "chap01_slide.html#introduction",
    "href": "chap01_slide.html#introduction",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#one-caveat",
    "href": "chap01_slide.html#one-caveat",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "One Caveat",
    "text": "One Caveat",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#what-is-machine-learning-ml",
    "href": "chap01_slide.html#what-is-machine-learning-ml",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "What is Machine Learning (ML)",
    "text": "What is Machine Learning (ML)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#artificial-intelligence-al-vs-machine-learning-ml",
    "href": "chap01_slide.html#artificial-intelligence-al-vs-machine-learning-ml",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Artificial Intelligence (AL) vs Machine Learning (ML)",
    "text": "Artificial Intelligence (AL) vs Machine Learning (ML)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#artificial-intelligence-ai-vs-machine-learning-mlcontd",
    "href": "chap01_slide.html#artificial-intelligence-ai-vs-machine-learning-mlcontd",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Artificial Intelligence (AI) vs Machine Learning (ML)(Cont’d)",
    "text": "Artificial Intelligence (AI) vs Machine Learning (ML)(Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#model-vs-algorithm",
    "href": "chap01_slide.html#model-vs-algorithm",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Model vs Algorithm",
    "text": "Model vs Algorithm",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#model-vs-algorithm-contd",
    "href": "chap01_slide.html#model-vs-algorithm-contd",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Model vs Algorithm (Cont’d)",
    "text": "Model vs Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#model-vs-algorithm-contd-1",
    "href": "chap01_slide.html#model-vs-algorithm-contd-1",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Model vs Algorithm (Cont’d)",
    "text": "Model vs Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#classes-of-machine-learning-algorithm",
    "href": "chap01_slide.html#classes-of-machine-learning-algorithm",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Classes of Machine Learning Algorithm",
    "text": "Classes of Machine Learning Algorithm",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#classes-of-machine-learning-algorithm-contd",
    "href": "chap01_slide.html#classes-of-machine-learning-algorithm-contd",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Classes of Machine Learning Algorithm (Cont’d)",
    "text": "Classes of Machine Learning Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#classes-of-machine-learning-algorithm-contd-1",
    "href": "chap01_slide.html#classes-of-machine-learning-algorithm-contd-1",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Classes of Machine Learning Algorithm (Cont’d)",
    "text": "Classes of Machine Learning Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#classes-of-machine-learning-algorithm-contd-2",
    "href": "chap01_slide.html#classes-of-machine-learning-algorithm-contd-2",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Classes of Machine Learning Algorithm (Cont’d)",
    "text": "Classes of Machine Learning Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#artificial-intelligence-ai-machine-learning-ml-and-deep-learning-dl",
    "href": "chap01_slide.html#artificial-intelligence-ai-machine-learning-ml-and-deep-learning-dl",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Artificial Intelligence (AI), Machine learning (ML), and Deep learning (DL)",
    "text": "Artificial Intelligence (AI), Machine learning (ML), and Deep learning (DL)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#ethics-in-machine-learning",
    "href": "chap01_slide.html#ethics-in-machine-learning",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Ethics in Machine Learning",
    "text": "Ethics in Machine Learning",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#python-vs.-r-for-machine-learning",
    "href": "chap01_slide.html#python-vs.-r-for-machine-learning",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Python VS. R for Machine Learning",
    "text": "Python VS. R for Machine Learning",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#what-will-we-learn-from-bsan405",
    "href": "chap01_slide.html#what-will-we-learn-from-bsan405",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "What will We Learn from BSAN405",
    "text": "What will We Learn from BSAN405",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#questions-or-queries",
    "href": "chap01_slide.html#questions-or-queries",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Questions or Queries",
    "text": "Questions or Queries\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nQueries",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap03_knn.html",
    "href": "chap03_knn.html",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "",
    "text": "The data set for this lecture practice is available in this link.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#introduction",
    "href": "chap03_knn.html#introduction",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "",
    "text": "The data set for this lecture practice is available in this link.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#assigning-column-names",
    "href": "chap03_knn.html#assigning-column-names",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "5.1 Assigning Column Names",
    "text": "5.1 Assigning Column Names\n\ncol_names = ['Id', 'Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape', 'Marginal_Adhesion','Single_Epithelial_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class']\n\ndf.columns = col_names\n\ndf.columns\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 699 entries, 0 to 698\nData columns (total 11 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Id                           699 non-null    int64 \n 1   Clump_thickness              699 non-null    int64 \n 2   Uniformity_Cell_Size         699 non-null    int64 \n 3   Uniformity_Cell_Shape        699 non-null    int64 \n 4   Marginal_Adhesion            699 non-null    int64 \n 5   Single_Epithelial_Cell_Size  699 non-null    int64 \n 6   Bare_Nuclei                  699 non-null    object\n 7   Bland_Chromatin              699 non-null    int64 \n 8   Normal_Nucleoli              699 non-null    int64 \n 9   Mitoses                      699 non-null    int64 \n 10  Class                        699 non-null    int64 \ndtypes: int64(10), object(1)\nmemory usage: 60.2+ KB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nId\nClump_thickness\nUniformity_Cell_Size\nUniformity_Cell_Shape\nMarginal_Adhesion\nSingle_Epithelial_Cell_Size\nBare_Nuclei\nBland_Chromatin\nNormal_Nucleoli\nMitoses\nClass\n\n\n\n\n0\n1000025\n5\n1\n1\n1\n2\n1\n3\n1\n1\n2\n\n\n1\n1002945\n5\n4\n4\n5\n7\n10\n3\n2\n1\n2\n\n\n2\n1015425\n3\n1\n1\n1\n2\n2\n3\n1\n1\n2\n\n\n3\n1016277\n6\n8\n8\n1\n3\n4\n3\n7\n1\n2\n\n\n4\n1017023\n4\n1\n1\n3\n2\n1\n3\n1\n1\n2",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#dropping-redundant-columns",
    "href": "chap03_knn.html#dropping-redundant-columns",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "5.2 Dropping Redundant Columns",
    "text": "5.2 Dropping Redundant Columns\n\ndf.drop('Id', axis=1, inplace=True)\ndf.info()\ndf.dtypes\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 699 entries, 0 to 698\nData columns (total 10 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Clump_thickness              699 non-null    int64 \n 1   Uniformity_Cell_Size         699 non-null    int64 \n 2   Uniformity_Cell_Shape        699 non-null    int64 \n 3   Marginal_Adhesion            699 non-null    int64 \n 4   Single_Epithelial_Cell_Size  699 non-null    int64 \n 5   Bare_Nuclei                  699 non-null    object\n 6   Bland_Chromatin              699 non-null    int64 \n 7   Normal_Nucleoli              699 non-null    int64 \n 8   Mitoses                      699 non-null    int64 \n 9   Class                        699 non-null    int64 \ndtypes: int64(9), object(1)\nmemory usage: 54.7+ KB\n\n\nClump_thickness                 int64\nUniformity_Cell_Size            int64\nUniformity_Cell_Shape           int64\nMarginal_Adhesion               int64\nSingle_Epithelial_Cell_Size     int64\nBare_Nuclei                    object\nBland_Chromatin                 int64\nNormal_Nucleoli                 int64\nMitoses                         int64\nClass                           int64\ndtype: object\n\n\n\ndf['Class'].value_counts()\n\nClass\n2    458\n4    241\nName: count, dtype: int64",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#changing-data-types-of-variables",
    "href": "chap03_knn.html#changing-data-types-of-variables",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "5.3 Changing Data Types of Variables",
    "text": "5.3 Changing Data Types of Variables\n\ndf['Bare_Nuclei'] = pd.to_numeric(df['Bare_Nuclei'], errors='coerce')\ndf.dtypes\n\nClump_thickness                  int64\nUniformity_Cell_Size             int64\nUniformity_Cell_Shape            int64\nMarginal_Adhesion                int64\nSingle_Epithelial_Cell_Size      int64\nBare_Nuclei                    float64\nBland_Chromatin                  int64\nNormal_Nucleoli                  int64\nMitoses                          int64\nClass                            int64\ndtype: object",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#checking-missing-observations-in-the-dataset",
    "href": "chap03_knn.html#checking-missing-observations-in-the-dataset",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "5.4 Checking Missing Observations in the Dataset",
    "text": "5.4 Checking Missing Observations in the Dataset\n\ndf.isnull().sum() # Checking missing values in variables\ndf.isna().sum() # Checking missing values in the dataframe \n\nClump_thickness                 0\nUniformity_Cell_Size            0\nUniformity_Cell_Shape           0\nMarginal_Adhesion               0\nSingle_Epithelial_Cell_Size     0\nBare_Nuclei                    16\nBland_Chromatin                 0\nNormal_Nucleoli                 0\nMitoses                         0\nClass                           0\ndtype: int64\n\n\n\ndf['Bare_Nuclei'].value_counts()\n\nBare_Nuclei\n1.0     402\n10.0    132\n2.0      30\n5.0      30\n3.0      28\n8.0      21\n4.0      19\n9.0       9\n7.0       8\n6.0       4\nName: count, dtype: int64\n\n\n\ndf['Bare_Nuclei'].unique()\n\narray([ 1., 10.,  2.,  4.,  3.,  9.,  7., nan,  5.,  8.,  6.])",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#multivariate-plots",
    "href": "chap03_knn.html#multivariate-plots",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "7.1 Multivariate Plots",
    "text": "7.1 Multivariate Plots\n\ncorrelation = df.corr()\ncorrelation['Class'].sort_values(ascending=False)\n\nClass                          1.000000\nBare_Nuclei                    0.822696\nUniformity_Cell_Shape          0.818934\nUniformity_Cell_Size           0.817904\nBland_Chromatin                0.756616\nClump_thickness                0.716001\nNormal_Nucleoli                0.712244\nMarginal_Adhesion              0.696800\nSingle_Epithelial_Cell_Size    0.682785\nMitoses                        0.423170\nName: Class, dtype: float64\n\n\nInterpretation: The correlation coefficient ranges from -1 to +1.\nWhen it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is a strong positive correlation between Class and Bare_Nuclei, Class and Uniformity_Cell_Shape, Class and Uniformity_Cell_Size.\nWhen it is close to -1, it means that there is a strong negative correlation. When it is close to 0, it means that there is no correlation.\nWe can see that all the variables are positively correlated with Class variable. Some variables are strongly positive correlated while some variables are negatively correlated.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#discover-pattern-and-relationship",
    "href": "chap03_knn.html#discover-pattern-and-relationship",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "7.2 Discover Pattern and Relationship",
    "text": "7.2 Discover Pattern and Relationship\nAn important step in EDA is to discover patterns and relationships between variables in the dataset. I will use the seaborn heatmap to explore the patterns and relationships in the dataset.\n\nplt.figure(figsize=(10,8))\nplt.title('Correlation of Attributes with Class variable')\na = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white')\na.set_xticklabels(a.get_xticklabels(), rotation=90)\na.set_yticklabels(a.get_yticklabels(), rotation=30)           \nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#engineering-missing-values-in-variables",
    "href": "chap03_knn.html#engineering-missing-values-in-variables",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "10.1 Engineering Missing Values in Variables",
    "text": "10.1 Engineering Missing Values in Variables\n\n# check missing values in numerical variables in X_test\n\nX_test.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    3\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\n\n# print percentage of missing values in the numerical variables in training set\n\nfor col in X_train.columns:\n    if X_train[col].isnull().mean()&gt;0:\n        print(col, round(X_train[col].isnull().mean(),4))\n\nBare_Nuclei 0.0233\n\n\nAssumption I assume that the data are missing completely at random (MCAR). There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation. When there are outliers in the dataset, we should use median imputation. So, I will use median imputation because median imputation is robust to outliers.\nI will impute missing values with the appropriate statistical measures of the data, in this case median. Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting.\n\n# impute missing values in X_train and X_test with respective column median in X_train\n\nfor df in [X_train, X_test]:\n    for col in X_train.columns:\n        col_median=X_train[col].median()\n        df[col].fillna(col_median, inplace=True)  \n\n\n# check again missing values in numerical variables in X_train\n\nX_train.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    0\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\n\n# check missing values in numerical variables in X_test\n\nX_test.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    0\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\nWe now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called feature scaling. I will do it as follows.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#feature-selection",
    "href": "chap03_knn.html#feature-selection",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "10.2 Feature Selection",
    "text": "10.2 Feature Selection\nFeature selection involves identifying the features that have the greatest explanatory power to predict the target variables. Therere are many techniques that can be used for feature selection. When there are many variables, using feature selection is very much important. Otherwise, noises might be introduced in the model.\n\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\n\n\nchi_feature = SelectKBest(chi2, k = 4).fit(X_train, y_train)\nprint ('Score: ', chi_feature.scores_)\nprint ('Features: ', X_train.columns)\n\nScore:  [ 493.52822606 1144.54028237 1046.47720631  844.84992698  404.07603727\n 1364.87717094  532.42045225  951.992508    180.65784477]\nFeatures:  Index(['Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape',\n       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses'],\n      dtype='object')\n\n\n\nanova_feature = SelectKBest (f_classif, k = 4).fit(X_train, y_train)\nprint ('Scores: ', anova_feature.scores_)\nprint ('Features: ', X_train.columns)\n\nScores:  [ 545.20592252 1142.16232995 1093.33969603  582.15032766  518.143572\n 1022.11887635  716.06901174  593.48561929  124.1776932 ]\nFeatures:  Index(['Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape',\n       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses'],\n      dtype='object')",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#feature-scaling",
    "href": "chap03_knn.html#feature-scaling",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "10.3 Feature Scaling",
    "text": "10.3 Feature Scaling\n\ncols = X_train.columns\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n\n\nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])\nX_train.head()\n\n\n\n\n\n\n\n\nClump_thickness\nUniformity_Cell_Size\nUniformity_Cell_Shape\nMarginal_Adhesion\nSingle_Epithelial_Cell_Size\nBare_Nuclei\nBland_Chromatin\nNormal_Nucleoli\nMitoses\n\n\n\n\n0\n2.028383\n0.299506\n0.289573\n1.119077\n-0.546543\n1.858357\n-0.577774\n0.041241\n-0.324258\n\n\n1\n1.669451\n2.257680\n2.304569\n-0.622471\n3.106879\n1.297589\n-0.159953\n0.041241\n-0.324258\n\n\n2\n-1.202005\n-0.679581\n-0.717925\n0.074148\n-1.003220\n-0.104329\n-0.995595\n-0.608165\n-0.324258\n\n\n3\n-0.125209\n-0.026856\n-0.046260\n-0.622471\n-0.546543\n-0.665096\n-0.159953\n0.041241\n-0.324258\n\n\n4\n0.233723\n-0.353219\n-0.382092\n-0.274161\n-0.546543\n-0.665096\n-0.577774\n-0.283462\n-0.324258",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#compare-the-train-set-and-test-set-accuracy",
    "href": "chap03_knn.html#compare-the-train-set-and-test-set-accuracy",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "13.1 Compare the Train-set and Test-set Accuracy",
    "text": "13.1 Compare the Train-set and Test-set Accuracy\nNow, I will compare the train-set and test-set accuracy to check for overfitting.\n\ny_pred_train = knn.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\nTraining-set accuracy score: 0.9821",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#compare-model-accuracy-with-null-accuracy",
    "href": "chap03_knn.html#compare-model-accuracy-with-null-accuracy",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "14.1 Compare Model Accuracy with Null Accuracy",
    "text": "14.1 Compare Model Accuracy with Null Accuracy\nSo, the model accuracy is 0.9714. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\nSo, we should first check the class distribution in the test set.\n\n# check class distribution in test set\n\ny_test.value_counts()\n\nClass\n2    85\n4    55\nName: count, dtype: int64\n\n\nWe can see that the occurences of most frequent class is 85. So, we can calculate null accuracy by dividing 85 by total number of occurences.\n\n# check null accuracy score\n\nnull_accuracy = (85/(85+55))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))\n\nNull accuracy score: 0.6071\n\n\nWe can see that our model accuracy score is 0.9714 but null accuracy score is 0.6071. So, we can conclude that our K Nearest Neighbors model is doing a very good job in predicting the class labels.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#rebuild-knn-classification-model-using-k6",
    "href": "chap03_knn.html#rebuild-knn-classification-model-using-k6",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "15.1 Rebuild KNN Classification Model Using K=6",
    "text": "15.1 Rebuild KNN Classification Model Using K=6\n\n# instantiate the model with k=6\nknn_6 = KNeighborsClassifier(n_neighbors=6)\n\n\n# fit the model to the training set\nknn_6.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_6 = knn_6.predict(X_test)\n\n\nprint('Model accuracy score with k=6 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_6)))\n\nModel accuracy score with k=6 : 0.9786",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#rebuild-knn-classification-model-using-k7",
    "href": "chap03_knn.html#rebuild-knn-classification-model-using-k7",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "15.2 Rebuild KNN Classification Model Using K=7",
    "text": "15.2 Rebuild KNN Classification Model Using K=7\n\n# instantiate the model with k=7\nknn_7 = KNeighborsClassifier(n_neighbors=7)\n\n\n# fit the model to the training set\nknn_7.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_7 = knn_7.predict(X_test)\n\n\nprint('Model accuracy score with k=7 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_7)))\n\nModel accuracy score with k=7 : 0.9786",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#rebuild-knn-classification-model-using-k8",
    "href": "chap03_knn.html#rebuild-knn-classification-model-using-k8",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "15.3 Rebuild KNN Classification Model Using K=8",
    "text": "15.3 Rebuild KNN Classification Model Using K=8\n\n# instantiate the model with k=8\nknn_8 = KNeighborsClassifier(n_neighbors=8)\n\n\n# fit the model to the training set\nknn_8.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_8 = knn_8.predict(X_test)\n\n\nprint('Model accuracy score with k=8 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_8)))\n\nModel accuracy score with k=8 : 0.9786",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#rebuild-knn-classification-model-using-k9",
    "href": "chap03_knn.html#rebuild-knn-classification-model-using-k9",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "15.4 Rebuild KNN Classification Model Using K=9",
    "text": "15.4 Rebuild KNN Classification Model Using K=9\n\n# instantiate the model with k=9\nknn_9 = KNeighborsClassifier(n_neighbors=9)\n\n\n# fit the model to the training set\nknn_9.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_9 = knn_9.predict(X_test)\n\n\nprint('Model accuracy score with k=9 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_9)))\n\nModel accuracy score with k=9 : 0.9714\n\n\nInterpretation: Our original model accuracy score with k=3 is 0.9714. Now, we can see that we get same accuracy score of 0.9714 with k=5. But, if we increase the value of k further, this would result in enhanced accuracy.\nWith k=6,7,8 we get accuracy score of 0.9786. So, it results in performance improvement.\nIf we increase k to 9, then accuracy decreases again to 0.9714.\nNow, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\nWe have another tool called Confusion matrix that comes to our rescue.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#classification-report",
    "href": "chap03_knn.html#classification-report",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.1 Classification Report",
    "text": "19.1 Classification Report\nClassification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. I have described these terms in later.\nWe can print a classification report as follows:-\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_7))\n\n              precision    recall  f1-score   support\n\n           2       0.99      0.98      0.98        85\n           4       0.96      0.98      0.97        55\n\n    accuracy                           0.98       140\n   macro avg       0.98      0.98      0.98       140\nweighted avg       0.98      0.98      0.98       140",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#classification-accuracy",
    "href": "chap03_knn.html#classification-accuracy",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.2 Classification Accuracy",
    "text": "19.2 Classification Accuracy\n\nTP = cm_7[0,0]\nTN = cm_7[1,1]\nFP = cm_7[0,1]\nFN = cm_7[1,0]\n\n\n# print classification accuracy\n\nclassification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n\nClassification accuracy : 0.9786",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#classification-error",
    "href": "chap03_knn.html#classification-error",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.3 Classification Error",
    "text": "19.3 Classification Error\n\n# print classification error\n\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))\n\nClassification error : 0.0214",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#precision",
    "href": "chap03_knn.html#precision",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.4 Precision",
    "text": "19.4 Precision\nPrecision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). Precision is a metric that tells us about the quality of positive predictions. So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class. Precision is a useful metric in cases where False Positive is a higher concern than False Negatives. Precision is important in music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.tt\nMathematically, precision can be defined as the ratio of TP to (TP + FP).\n\n# print precision score\n\nprecision = TP / float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))\n\nPrecision : 0.9765",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#recall",
    "href": "chap03_knn.html#recall",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.5 Recall",
    "text": "19.5 Recall\nRecall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall tells us about how well the model identifies true positives. Recall is also called Sensitivity. Recall identifies the proportion of correctly predicted actual positives. Mathematically, recall can be given as the ratio of TP to (TP + FN). Recall is a useful metric in cases where False Negative triumphs over False Positive. Recall is important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n\nrecall = TP / float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))\n\nRecall or Sensitivity : 0.9881",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#precision-vs-recall",
    "href": "chap03_knn.html#precision-vs-recall",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.6 Precision vs Recall",
    "text": "19.6 Precision vs Recall\nData scientists optimize their model to have higher precision or recall depending on the circumstances. A model with higher recall than precision often makes more positive predictions. A model like this comes with higher false positives and low false negatives. In scenarios like disease prediction, models should always be optimized for recall. False positives are better than false negatives in the healthcare industry.\nOn the other hand, a model with higher precision will have fewer false positives and more false negatives. If you were to build a bot detection machine learning model for an online store, you may want to optimize for higher precision, since banning legitimate users from the website will lead to a decline in sales.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#f1-score",
    "href": "chap03_knn.html#f1-score",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.7 f1-score",
    "text": "19.7 f1-score\nIn practice, when we try to increase the precision of our model, the recall goes down, and vice-versa. The F1-score captures both the trends in a single value:\n        f1-score = 2/((1/Recall) + (1/Precision))\nf1-score is the weighted harmonic mean of precision and recall, and so it gives a combined idea about these two metrics. It is maximum when Precision is equal to Recall. The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#support",
    "href": "chap03_knn.html#support",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.8 Support",
    "text": "19.8 Support\nSupport is the actual number of occurrences of the class in our dataset.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#true-positive-rate",
    "href": "chap03_knn.html#true-positive-rate",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.9 True Positive Rate",
    "text": "19.9 True Positive Rate\nTrue Positive Rate is synonymous with Recall.\n\ntrue_positive_rate = TP / float(TP + FN)\n\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n\nTrue Positive Rate : 0.9881",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#false-positive-rate",
    "href": "chap03_knn.html#false-positive-rate",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.10 False Positive Rate",
    "text": "19.10 False Positive Rate\n\nfalse_positive_rate = FP / float(FP + TN)\n\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n\nFalse Positive Rate : 0.0357",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#specificity-true-negative-rate",
    "href": "chap03_knn.html#specificity-true-negative-rate",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.11 Specificity (True Negative Rate)",
    "text": "19.11 Specificity (True Negative Rate)\n\nspecificity = TN / (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))\n\nSpecificity : 0.9643",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#adjusting-the-classification-threshold-level",
    "href": "chap03_knn.html#adjusting-the-classification-threshold-level",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.12 Adjusting the Classification Threshold Level",
    "text": "19.12 Adjusting the Classification Threshold Level\n\n# print the first 10 predicted probabilities of two classes- 2 and 4\n\ny_pred_prob = knn.predict_proba(X_test)[0:10]\n\ny_pred_prob\n\narray([[1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.21219604, 0.78780396],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.40998626, 0.59001374]])\n\n\nObservations In each row, the numbers sum to 1. There are 2 columns which correspond to 2 classes - 2 and 4.\n\nClass 2 - predicted probability that there is benign cancer.\nClass 4 - predicted probability that there is malignant cancer.\n\nImportance of predicted probabilities\nWe can rank the observations by probability of benign or malignant cancer.\n\npredict_proba process\n   * Predicts the probabilities\n\n   * Choose the class with the highest probability\nClassification threshold level\n   * There is a classification threshold level of 0.5.\n\n   * Class 4 - probability of malignant cancer is predicted if probability &gt; 0.5.\n\n   * Class 2 - probability of benign cancer is predicted if probability &lt; 0.5.\n\n\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, \\\ncolumns=['Prob of - benign cancer (2)', 'Prob of - malignant cancer (4)'])\n\ny_pred_prob_df\n\n\n\n\n\n\n\n\nProb of - benign cancer (2)\nProb of - malignant cancer (4)\n\n\n\n\n0\n1.000000\n0.000000\n\n\n1\n1.000000\n0.000000\n\n\n2\n0.212196\n0.787804\n\n\n3\n1.000000\n0.000000\n\n\n4\n0.000000\n1.000000\n\n\n5\n1.000000\n0.000000\n\n\n6\n0.000000\n1.000000\n\n\n7\n1.000000\n0.000000\n\n\n8\n0.000000\n1.000000\n\n\n9\n0.409986\n0.590014\n\n\n\n\n\n\n\n\n# print the first 10 predicted probabilities for class 4 - Probability of malignant cancer\n\nknn.predict_proba(X_test)[0:10, 1]\n\narray([0.        , 0.        , 0.78780396, 0.        , 1.        ,\n       0.        , 1.        , 0.        , 1.        , 0.59001374])\n\n\n\n# store the predicted probabilities for class 4 - Probability of malignant cancer\n\ny_pred_1 = knn.predict_proba(X_test)[:, 1]\n\n\n# plot histogram of predicted probabilities\n\n# adjust figure size\nplt.figure(figsize=(6,4))\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n# plot histogram with 10 bins\nplt.hist(y_pred_1, bins = 10)\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities of malignant cancer')\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n# set the title\nplt.xlabel('Predicted probabilities of malignant cancer')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nObservations\n\nWe can see that the above histogram is positively skewed.\nThe first column tell us that there are approximately 80 observations with 0 * * * * probability of malignant cancer.\nThere are few observations with probability &gt; 0.5.\nSo, these few observations predict that there will be malignant cancer.\n\nComments\n\nIn binary problems, the threshold of 0.5 is used by default to convert predicted probabilities into class predictions.\nThreshold can be adjusted to increase sensitivity or specificity.\nSensitivity and specificity have an inverse relationship. Increasing one would always decrease the other and vice versa.\nAdjusting the threshold level should be one of the last step you do in the model-building process.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#roc-curve",
    "href": "chap03_knn.html#roc-curve",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "20.1 ROC Curve",
    "text": "20.1 ROC Curve\nAnother tool to measure the classification model performance visually is ROC Curve. ROC Curve stands for Receiver Operating Characteristic Curve. An ROC Curve is a plot which shows the performance of a classification model at various classification threshold levels.\nThe ROC Curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold levels. True Positive Rate (TPR) is also called Recall. It is defined as the ratio of TP to (TP + FN). False Positive Rate (FPR) is defined as the ratio of FP to (FP + TN).\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positive. It will increase both True Positives (TP) and False Positives (FP).\n\n# plot ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_1, pos_label=4)\n\nplt.figure(figsize=(6,4))\n\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\n\nplt.title('ROC curve for Breast Cancer kNN classifier')\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\n\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()\n\n\n\n\n\n\n\n\nROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#roc-auc",
    "href": "chap03_knn.html#roc-auc",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "20.2 ROC AUC",
    "text": "20.2 ROC AUC\nROC AUC stands for Receiver Operating Characteristic - Area Under Curve. It is a technique to compare classifier performance. In this technique, we measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. So, ROC AUC is the percentage of the ROC plot that is underneath the curve.\n\n# compute ROC AUC\n\nfrom sklearn.metrics import roc_auc_score\n\nROC_AUC = roc_auc_score(y_test, y_pred_1)\n\nprint('ROC AUC : {:.4f}'.format(ROC_AUC))\n\nROC AUC : 0.9827\n\n\nInterpretation\n\nROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\nROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it is benign or malignant cancer.\n\n\nfrom sklearn.model_selection import cross_val_score\n\nCross_validated_ROC_AUC = \\\ncross_val_score(knn_7, X_train, y_train, cv=5,scoring='roc_auc').mean()\n\nprint('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))\n\nCross validated ROC AUC : 0.9910\n\n\nInterpretation\nOur Cross Validated ROC AUC is very close to 1. So, we can conclude that, the KNN classifier is indeed a very good model.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap04_logistic.html#loading-python-package",
    "href": "chap04_logistic.html#loading-python-package",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "2 Loading Python Package",
    "text": "2 Loading Python Package\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\n\nThe Scikit-learn version that is used for this code file is 1.6.0",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#loading-dataset",
    "href": "chap04_logistic.html#loading-dataset",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "3 Loading Dataset",
    "text": "3 Loading Dataset\n\n# Importing Training Dataset \ntrain_df = pd.read_csv(\"DATA/train.csv\")\n# Importing Testing Dataset\ntest_df = pd.read_csv(\"DATA/test.csv\")\n\n\n3.1 Metadata of the Dataset\n\nprint(\"The total number of rows and columns in the dataset is {} and {} respectively.\".format(train_df.shape[0],train_df.shape[1]))\nprint (\"\\nThe names and the types of the variables of the dataset:\")\ntrain_df.info()\ntrain_df.head()\nprint(\"\\nThe types of the variables in the dataset are:\")\ntrain_df.dtypes\n\nThe total number of rows and columns in the dataset is 891 and 12 respectively.\n\nThe names and the types of the variables of the dataset:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\nThe types of the variables in the dataset are:\n\n\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\n\nfor x in train_df.columns:\n  print  (x)\ntrain_df['Pclass'].value_counts(sort = True, ascending = True)\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\nPclass\n2    184\n1    216\n3    491\nName: count, dtype: int64\n\n\n\ntrain_df['Sex'].value_counts()\n\nSex\nmale      577\nfemale    314\nName: count, dtype: int64",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#data-quality-missing-value-assessment",
    "href": "chap04_logistic.html#data-quality-missing-value-assessment",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "4 Data Quality & Missing Value Assessment",
    "text": "4 Data Quality & Missing Value Assessment\n\n# Missing value in Training Dataset \ntrain_df.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\n# Missing value in Testing Dataset \ntest_df.isnull().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\n\n4.1 Age - Missing Value\n\nprint ('The percent of missing \"Age\" record in the training dataset is %0.2f%%' %(train_df['Age'].isnull().sum()/train_df.shape[0]*100))\n\nThe percent of missing \"Age\" record in the training dataset is 19.87%\n\n\n\nprint ('The percent of missing \"Age\" record in testing dataset is %0.3f%%' %(test_df['Age'].isnull().sum()/test_df.shape[0]*100))\n\nThe percent of missing \"Age\" record in testing dataset is 20.574%\n\n\n\nplt.figure(figsize = (10,8))\ntrain_df['Age'].hist(bins = 15, density = True, color = 'teal', alpha = 0.60)\ntrain_df['Age'].plot(kind = 'density', color = 'teal', alpha = 0.60)\nplt.show()\n\n\n\n\n\n\n\n\nSince the variable Age is a little bit right skewed, using the mean to replace the missing observations might bias our results. Therefore, it is recommended that median be used to replace the missing observations.\n\nprint ('The mean of \"Age\" variable is %0.3f.' %(train_df['Age'].mean(skipna = True)))\n\nThe mean of \"Age\" variable is 29.699.\n\n\n\nprint ('The median of \"Age\" variable is %0.2f.' %(train_df['Age'].median(skipna = True)))\n\nThe median of \"Age\" variable is 28.00.\n\n\n\n\n4.2 Cabin - Missing Value\n\ntrain_df['Cabin'].value_counts()\n\nCabin\nG6             4\nC23 C25 C27    4\nB96 B98        4\nF2             3\nD              3\n              ..\nE17            1\nA24            1\nC50            1\nB42            1\nC148           1\nName: count, Length: 147, dtype: int64\n\n\n\nprint ('The percent of cabin variable missing value is %0.2f%%.' %(train_df['Cabin'].isnull().sum()/train_df.shape[0]*100))\n\nThe percent of cabin variable missing value is 77.10%.\n\n\n77% observations of the Cabin variable is missing. Therefore, it is better to prune the variable from the dataset. Moreover, the drop of the Cabin variable is justified because it has correlation with two other variables - Fare and Pclass.\n\n\n4.3 Embarked - Missing Value\n\ntrain_df['Embarked'].value_counts()\n\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\n\n# Percent of missing 'Embarked' Variable\nprint(\n    \"The percent of missing 'Embarked' records is %.2f%%\" %\n    (train_df['Embarked'].isnull().sum()/train_df.shape[0]*100)\n)\n\nThe percent of missing 'Embarked' records is 0.22%\n\n\nSince there are only 0.22% missing observation for Embarked, we can impute the missing values with the port where most people embarked.\n\nprint('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\nprint(train_df['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train_df, palette='Set2')\nplt.show()\n\nBoarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\nprint('The most common boarding port of embarkation is %s.' %train_df['Embarked'].value_counts().idxmax())\n\nThe most common boarding port of embarkation is S.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#final-adjustment-to-the-datasets-training-testing",
    "href": "chap04_logistic.html#final-adjustment-to-the-datasets-training-testing",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "5 Final Adjustment to the Datasets (Training & Testing)",
    "text": "5 Final Adjustment to the Datasets (Training & Testing)\nBased on the assessment of the missing values in the dataset, We will make the following changes to the data:\n\nThe missing value of Age variable will be imputed with 28 (median value Age)\nThe missing value of Embarked variable will be imputed with S (the most common boarding point)\nThere are many missing values for the variable Cabin; therefore, the variable will be dropped. Moreover, the drop will not affect the model as the variable is associated with two other variables - Pclass and Fare.\n\n\ntrain_data = train_df.copy()\ntrain_data['Age'].fillna(train_data['Age'].median(skipna = True), inplace = True)\ntrain_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace = True)\ntrain_data.drop(['Cabin'], axis = 1, inplace = True)\ntrain_data.isnull().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nEmbarked       0\ndtype: int64\n\n\n\ntrain_data.tail()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nEmbarked\n\n\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.00\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.00\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\n28.0\n1\n2\nW./C. 6607\n23.45\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.00\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.75\nQ\n\n\n\n\n\n\n\n\nplt.figure(figsize=(15,8))\n# Data with missing observations\nax = train_df['Age'].hist(bins = 15, density = True, stacked = True, color = 'teal', alpha = 0.6)\ntrain_df['Age'].plot(kind = 'density', color = 'teal')\n# Data without missing observations\nax = train_data['Age'].hist(bins = 15, density = True, stacked = True, color = 'orange', alpha = 0.6)\ntrain_data['Age'].plot(kind = 'density', color = 'orange')\nplt.xlim(-10,85)\nax.legend([\"Raw Age\", \"Adjusted Age\"])\nax.set(xlabel = 'Age')\nplt.show()\n\n\n\n\n\n\n\n\n\n5.1 Additional Variables\nThe variable SibSp means whether the passenger has sibling or spouse aboard and the variable Parch means whether the passenger has parents or children aboard. For the sake of simplicity and to account for multicollinearity, these two variables will be combined into a categorical variable: whether or not the individual was traveling alone.\n\n# Creating categorical variable for Traveling alone\ntrain_data['TravelAlone'] = np.where((train_data['SibSp']+train_data['Parch']) &gt; 0, 0, 1)\ntrain_data.drop('SibSp', axis = 1, inplace = True)\ntrain_data.drop('Parch', axis = 1, inplace = True)\n\nFor variables Pclass, Sex, and Embarked, categorical variables will be created\n\ntraining = pd.get_dummies(train_data, columns= [\"Pclass\", \"Embarked\", \"Sex\"])\ntraining.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Name         891 non-null    object \n 3   Age          891 non-null    float64\n 4   Ticket       891 non-null    object \n 5   Fare         891 non-null    float64\n 6   TravelAlone  891 non-null    int64  \n 7   Pclass_1     891 non-null    bool   \n 8   Pclass_2     891 non-null    bool   \n 9   Pclass_3     891 non-null    bool   \n 10  Embarked_C   891 non-null    bool   \n 11  Embarked_Q   891 non-null    bool   \n 12  Embarked_S   891 non-null    bool   \n 13  Sex_female   891 non-null    bool   \n 14  Sex_male     891 non-null    bool   \ndtypes: bool(8), float64(2), int64(3), object(2)\nmemory usage: 55.8+ KB\n\n\n\ntraining[['Pclass_1','Pclass_2','Pclass_3', 'Embarked_C','Embarked_Q','Embarked_S', 'Sex_female', 'Sex_male']].head()\n\n\n\n\n\n\n\n\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_female\nSex_male\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n3\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n4\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\ntraining.drop(['Sex_female', 'PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\ntraining.tail()\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n886\n0\n27.0\n13.00\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n887\n1\n19.0\n30.00\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n888\n0\n28.0\n23.45\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n889\n1\n26.0\n30.00\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n890\n0\n32.0\n7.75\n1\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\ntraining.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Survived     891 non-null    int64  \n 1   Age          891 non-null    float64\n 2   Fare         891 non-null    float64\n 3   TravelAlone  891 non-null    int64  \n 4   Pclass_1     891 non-null    bool   \n 5   Pclass_2     891 non-null    bool   \n 6   Pclass_3     891 non-null    bool   \n 7   Embarked_C   891 non-null    bool   \n 8   Embarked_Q   891 non-null    bool   \n 9   Embarked_S   891 non-null    bool   \n 10  Sex_male     891 non-null    bool   \ndtypes: bool(7), float64(2), int64(2)\nmemory usage: 34.1 KB\n\n\n\nfinal_train = training\nfinal_train.tail()\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n886\n0\n27.0\n13.00\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n887\n1\n19.0\n30.00\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n888\n0\n28.0\n23.45\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n889\n1\n26.0\n30.00\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n890\n0\n32.0\n7.75\n1\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\ntest_df.isna().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\nNow we apply the same changes in the testing dataset.\n\nWe will apply to same imputation for Age in the Test data as we did for my Training data (if missing, Age = 28).\nWe will also remove the Cabin variable from the test data, as we’ve decided not to include it in my analysis.\nThere were no missing values in the Embarked port variable.\nWe will add the dummy variables to finalize the test set.\nFinally, We will impute the 1 missing value for Fare with the median, 14.45.\n\n\nprint('The median value of \"Fare\" variable in testing dataset is %0.3f.' %(train_df['Fare'].median(skipna = True)))\n\nThe median value of \"Fare\" variable in testing dataset is 14.454.\n\n\n\ntest_data = test_df.copy()\ntest_data['Age'].fillna(test_data['Age'].median(skipna = True), inplace = True)\ntest_data['Fare'].fillna(test_data['Fare'].median(skipna = True), inplace = True)\ntest_data.drop(['Cabin'], axis  = 1, inplace = True)\n\n\n\n5.2 Creating New Variables\n\n# Creating new variable - TravelAlone\ntest_data['TravelAlone']= np.where(test_data['SibSp']+test_data['Parch']&gt;0,0,1)\ntest_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\ntest_data.sample(5)\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nTicket\nFare\nEmbarked\nTravelAlone\n\n\n\n\n66\n958\n3\nBurns, Miss. Mary Delia\nfemale\n18.0\n330963\n7.8792\nQ\n1\n\n\n221\n1113\n3\nReynolds, Mr. Harold J\nmale\n21.0\n342684\n8.0500\nS\n1\n\n\n85\n977\n3\nKhalil, Mr. Betros\nmale\n27.0\n2660\n14.4542\nC\n0\n\n\n353\n1245\n2\nHerman, Mr. Samuel\nmale\n49.0\n220845\n65.0000\nS\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\nSOTON/O.Q. 3101262\n7.2500\nS\n1\n\n\n\n\n\n\n\n\n5.2.1 Creating the Dummies for Categorical Variables\n\ntesting = pd.get_dummies(test_data, columns = ['Pclass', 'Sex', 'Embarked'])\ntesting.drop(['Sex_female','PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)\ntesting.tail()\n\n\n\n\n\n\n\n\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n413\n27.0\n8.0500\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n414\n39.0\n108.9000\n1\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n415\n38.5\n7.2500\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n416\n27.0\n8.0500\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n417\n27.0\n22.3583\n0\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\nfinal_test = testing\nfinal_test.head()\n\n\n\n\n\n\n\n\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n34.5\n7.8292\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n1\n47.0\n7.0000\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n2\n62.0\n9.6875\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n3\n27.0\n8.6625\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n4\n22.0\n12.2875\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#exploratory-data-analysis-eda",
    "href": "chap04_logistic.html#exploratory-data-analysis-eda",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "6 Exploratory Data Analysis (EDA)",
    "text": "6 Exploratory Data Analysis (EDA)\n\n6.1 Exploration of Age Variable\n\nfinal_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Survived     891 non-null    int64  \n 1   Age          891 non-null    float64\n 2   Fare         891 non-null    float64\n 3   TravelAlone  891 non-null    int64  \n 4   Pclass_1     891 non-null    bool   \n 5   Pclass_2     891 non-null    bool   \n 6   Pclass_3     891 non-null    bool   \n 7   Embarked_C   891 non-null    bool   \n 8   Embarked_Q   891 non-null    bool   \n 9   Embarked_S   891 non-null    bool   \n 10  Sex_male     891 non-null    bool   \ndtypes: bool(7), float64(2), int64(2)\nmemory usage: 34.1 KB\n\n\n\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Age'][final_train.Survived == 1], shade=True, color = 'darkturquoise')\nsns.kdeplot(final_train['Age'][final_train.Survived == 0], shade=True, color = 'lightcoral')\nax.legend(['Survived', 'Died']) # or you can use plt.legend(['Survived', 'Died])\nplt.title('Density Plot for Surviving Population and Deceased Population')\nplt.xlabel('Age') # or you can use ax.set(xlabel = 'Age')\nplt.xlim(-10,85)\nplt.show()\n\n\n\n\n\n\n\n\nThe age distribution for survivors and deceased is actually very similar. One notable difference is that, of the survivors, a larger proportion were children. The passengers evidently made an attempt to save children by giving them a place on the life rafts.\n\navg_age = final_train.groupby(['Survived']) ['Age'].mean()\navg_age.to_frame().reset_index()\n\n\n\n\n\n\n\n\nSurvived\nAge\n\n\n\n\n0\n0\n30.028233\n\n\n1\n1\n28.291433\n\n\n\n\n\n\n\n\nsns.boxplot(data = final_train, x  = 'Survived', y = 'Age', palette='Set2')\nplt.title(\"Comparison of Age of Passengers Conditioned on Survived\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating a Dummy Variable IsMinor\nfinal_train['IsMinor'] = np.where(final_train['Age'] &lt;= 16, 1, 0)\nfinal_test['IsMinor'] = np.where(final_test['Age'] &lt;= 16, 1, 0)\n\n\n\n6.2 Exploration of Fare Variable\n\ntrain_df.groupby(['Survived']) ['Fare'].mean().to_frame().reset_index()\n\n\n\n\n\n\n\n\nSurvived\nFare\n\n\n\n\n0\n0\n22.117887\n\n\n1\n1\n48.395408\n\n\n\n\n\n\n\n\nsns.boxplot(data = final_train, x = 'Survived', y = 'Fare', palette='Set2')\nplt.ylim(0, 100)\nplt.title('Comparison of Fare of Passengers Conditioned on Survived')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Fare'][final_train.Survived == 1],shade=True, color='darkturquoise')\nsns.kdeplot(final_train['Fare'][final_train.Survived==0], shade=True, color='lightcoral')\nax.legend(['Survived', 'Died'])\nax.set(xlabel= 'Fare')\nplt.xlim(-20,200)\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nplt.show()\n\n\n\n\n\n\n\n\nAs the distributions are clearly different for the fares of survivors vs. deceased, it’s likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class, which we’ll look at next.\n\n# Pair Plot of two continuous variables (Age and Fare)\nplt.figure(figsize=(15,8))\nsns.pairplot(data=train_data, hue='Survived', vars= ['Age', 'Fare'])\nplt.show()\n\n&lt;Figure size 1440x768 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n6.3 Exploration of PClass Variable\n\nsns.barplot(data = train_df, x = 'Pclass', y = 'Survived', color='darkturquoise')\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, first class passengers were more likely to survive.\n\n\n6.4 Exploration of Embarked Variable\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=train_df, color=\"teal\")\nplt.show()\n\n\n\n\n\n\n\n\nPassengers who boarded in Cherbourg, France, appear to have the highest survival rate. Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown. This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck). It’s also worth noting the size of the whiskers in these plots. Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest. The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker. It’s possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts.\n\n\n6.5 Exploration of TravelAlone Variable\n\nsns.barplot(x = 'TravelAlone', y = 'Survived', data=final_train, color=\"mediumturquoise\")\nplt.xlabel('Travel Alone')\nplt.show()\n\n\n\n\n\n\n\n\nIndividuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it’s likely that individuals traveling alone were likely male.\n\n\n6.6 Exploration of Gender Variable\n\nsns.barplot(x = 'Sex', y = 'Survived', data=train_df, color=\"aquamarine\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.7 Chi-square Test of Independence\n\n6.7.1 Chi-square Test of Independence betweeen Survived and Sex\n\npd.crosstab(train_df['Survived'], train_df['Sex'])\n\n\n\n\n\n\n\nSex\nfemale\nmale\n\n\nSurvived\n\n\n\n\n\n\n0\n81\n468\n\n\n1\n233\n109\n\n\n\n\n\n\n\n\n# Importing scipy package \nfrom scipy import stats\n\n\nstats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\n\nChi2ContingencyResult(statistic=np.float64(260.71702016732104), pvalue=np.float64(1.1973570627755645e-58), dof=1, expected_freq=array([[193.47474747, 355.52525253],\n       [120.52525253, 221.47474747]]))\n\n\n\nchi2_stat, p, dof, expected = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\nprint(f\"chi2 statistic:     {chi2_stat:.5g}\")\nprint(f\"p-value:            {p:.5g}\")\nprint(f\"degrees of freedom: {dof}\")\nprint(\"expected frequencies:\\n\",expected)\n\nchi2 statistic:     260.72\np-value:            1.1974e-58\ndegrees of freedom: 1\nexpected frequencies:\n [[193.47474747 355.52525253]\n [120.52525253 221.47474747]]\n\n\n\n\n6.7.2 Chi-square Test of Independence betweeen Survived and Pclass\n\nchi2_stat_2, p_2, dof_2, expected_2 = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Pclass']))\nprint(f\"chi2 statistic:     {chi2_stat_2:.5g}\")\nprint(f\"p-value:            {p_2:.5g}\")\nprint(f\"degrees of freedom: {dof_2}\")\nprint(\"expected frequencies:\\n\",expected_2)\n\nchi2 statistic:     102.89\np-value:            4.5493e-23\ndegrees of freedom: 2\nexpected frequencies:\n [[133.09090909 113.37373737 302.53535354]\n [ 82.90909091  70.62626263 188.46464646]]\n\n\n\n\n\n6.8 Post Hoc Analysis of Pclass\nThe explanation of Post Hoc analysis is given in this link.\n\npclass_cross = pd.crosstab(train_df['Survived'], train_df['Pclass'])\npclass_cross\n\n\n\n\n\n\n\nPclass\n1\n2\n3\n\n\nSurvived\n\n\n\n\n\n\n\n0\n80\n97\n372\n\n\n1\n136\n87\n119\n\n\n\n\n\n\n\n\nimport gc\nfrom itertools import combinations\nimport scipy.stats\nimport statsmodels.stats.multicomp as multi\nfrom statsmodels.stats.multitest import multipletests\n\n\np_vals_chi = []\npairs_of_class = list(combinations(train_df['Pclass'].unique(),2))\n\nfor each_pair in pairs_of_class:\n    each_df = train_df[(train_df['Pclass']==each_pair[0]) | (train_df['Pclass']==each_pair[1])]\n    p_vals_chi.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Pclass']))[1]\n         )\n\n\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of class',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of class'] = pairs_of_class\nbonferroni_results['original p value'] = p_vals_chi\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni = multipletests(p_vals_chi, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni[0]\nbonferroni_results.head()\n\n\n\n\n\n\n\n\npair of class\noriginal p value\ncorrected p value\nReject Null?\n\n\n\n\n0\n(3, 1)\n1.212338e-22\n3.637013e-22\nTrue\n\n\n1\n(3, 2)\n1.224965e-08\n3.674894e-08\nTrue\n\n\n2\n(1, 2)\n2.319805e-03\n6.959414e-03\nTrue\n\n\n\n\n\n\n\n\n\n6.9 Post Hoc Analysis of Embarked\n\n# Write code Here \np_vals_chi_embark = []\npairs_of_embark = list(combinations(train_data['Embarked'].unique(),2))\n\nfor each_pair in pairs_of_embark:\n    each_df = train_data[(train_data['Embarked']==each_pair[0]) | (train_data['Embarked']==each_pair[1])]\n    p_vals_chi_embark.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Embarked']))[1]\n         )\n\n\n#Write code Here\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of embark',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of embark'] = pairs_of_embark\nbonferroni_results['original p value'] = p_vals_chi_embark\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni_embark = multipletests(p_vals_chi_embark, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni_embark[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni_embark[0]\nbonferroni_results.head()\n\n\n\n\n\n\n\n\npair of embark\noriginal p value\ncorrected p value\nReject Null?\n\n\n\n\n0\n(S, C)\n5.537903e-07\n0.000002\nTrue\n\n\n1\n(S, Q)\n4.493936e-01\n1.000000\nFalse\n\n\n2\n(C, Q)\n2.475540e-02\n0.074266\nFalse",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#logistic-regression-results",
    "href": "chap04_logistic.html#logistic-regression-results",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "7 Logistic Regression & Results",
    "text": "7 Logistic Regression & Results\n\n7.1 Feature Selection\n\n7.1.1 Recursive Feature Selection (RFE)\nGiven an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\n# Our all training dataset \ntrain_df\ntrain_data # impute missing values \ntraining # create the dummies \nfinal_train\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\nIsMinor\n\n\n\n\n0\n0\n22.0\n7.2500\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n0\n\n\n1\n1\n38.0\n71.2833\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n0\n\n\n2\n1\n26.0\n7.9250\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n0\n\n\n3\n1\n35.0\n53.1000\n0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n0\n\n\n4\n0\n35.0\n8.0500\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n27.0\n13.0000\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\n0\n\n\n887\n1\n19.0\n30.0000\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n0\n\n\n888\n0\n28.0\n23.4500\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n0\n\n\n889\n1\n26.0\n30.0000\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n0\n\n\n890\n0\n32.0\n7.7500\n1\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n0\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ncols = ['Age', 'Fare', 'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX = final_train[cols] # features vector\ny = final_train['Survived'] # Target vector \n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\n\n\n# Build the model \nmodel = LogisticRegression()\n# Create the RFE model \nrfe = RFE(estimator=model, n_features_to_select=8)\nrfe = rfe.fit(X ,y)\ndir(rfe)\n\n['__abstractmethods__',\n '__annotations__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__firstlineno__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__sklearn_clone__',\n '__sklearn_tags__',\n '__static_attributes__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_abc_impl',\n '_build_request_for_signature',\n '_check_feature_names',\n '_check_n_features',\n '_doc_link_module',\n '_doc_link_template',\n '_doc_link_url_param_generator',\n '_estimator_type',\n '_fit',\n '_get_default_requests',\n '_get_doc_link',\n '_get_metadata_request',\n '_get_param_names',\n '_get_support_mask',\n '_get_tags',\n '_more_tags',\n '_parameter_constraints',\n '_repr_html_',\n '_repr_html_inner',\n '_repr_mimebundle_',\n '_sklearn_auto_wrap_output_keys',\n '_transform',\n '_validate_data',\n '_validate_params',\n 'classes_',\n 'decision_function',\n 'estimator',\n 'estimator_',\n 'feature_names_in_',\n 'fit',\n 'fit_transform',\n 'get_feature_names_out',\n 'get_metadata_routing',\n 'get_params',\n 'get_support',\n 'importance_getter',\n 'inverse_transform',\n 'n_features_',\n 'n_features_in_',\n 'n_features_to_select',\n 'predict',\n 'predict_log_proba',\n 'predict_proba',\n 'ranking_',\n 'score',\n 'set_output',\n 'set_params',\n 'step',\n 'support_',\n 'transform',\n 'verbose']\n\n\n\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))\n\nSelected features: ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_Q', 'Sex_male', 'IsMinor']\n\n\n\n# Getting the Regression Results \nimport statsmodels.api as sm\nlogit_model = sm.Logit(y,X.astype(float))\nresult = logit_model.fit()\nprint(result.summary2())\n\nOptimization terminated successfully.\n         Current function value: 0.448075\n         Iterations 6\n                         Results: Logit\n=================================================================\nModel:              Logit            Method:           MLE       \nDependent Variable: Survived         Pseudo R-squared: 0.327     \nDate:               2025-02-12 22:24 AIC:              816.4704  \nNo. Observations:   891              BIC:              859.6015  \nDf Model:           8                Log-Likelihood:   -399.24   \nDf Residuals:       882              LL-Null:          -593.33   \nConverged:          1.0000           LLR p-value:      6.3002e-79\nNo. Iterations:     6.0000           Scale:            1.0000    \n------------------------------------------------------------------\n              Coef.   Std.Err.     z      P&gt;|z|    [0.025   0.975]\n------------------------------------------------------------------\nAge          -0.0121    0.0057   -2.1381  0.0325  -0.0232  -0.0010\nFare          0.0015    0.0022    0.6878  0.4916  -0.0028   0.0058\nTravelAlone   0.2803    0.1936    1.4475  0.1477  -0.0992   0.6597\nPclass_1      2.1646    0.2838    7.6272  0.0000   1.6083   2.7208\nPclass_2      1.3948    0.2309    6.0399  0.0000   0.9422   1.8473\nEmbarked_C    0.6014    0.2332    2.5790  0.0099   0.1443   1.0584\nEmbarked_Q    0.6366    0.3139    2.0282  0.0425   0.0214   1.2517\nSex_male     -2.5527    0.1955  -13.0567  0.0000  -2.9359  -2.1695\nIsMinor       0.9987    0.2737    3.6488  0.0003   0.4623   1.5352\n=================================================================\n\n\n\n\ncols2 = ['Age',  'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX_alt = final_train[cols2] # features vector\nlogit_model2 = sm.Logit(y,sm.add_constant(X_alt.astype(float)))\nresult2 = logit_model2.fit()\nprint(result2.summary2())\n\nOptimization terminated successfully.\n         Current function value: 0.446363\n         Iterations 6\n                         Results: Logit\n=================================================================\nModel:              Logit            Method:           MLE       \nDependent Variable: Survived         Pseudo R-squared: 0.330     \nDate:               2025-02-12 22:24 AIC:              813.4192  \nNo. Observations:   891              BIC:              856.5503  \nDf Model:           8                Log-Likelihood:   -397.71   \nDf Residuals:       882              LL-Null:          -593.33   \nConverged:          1.0000           LLR p-value:      1.4027e-79\nNo. Iterations:     6.0000           Scale:            1.0000    \n------------------------------------------------------------------\n              Coef.   Std.Err.     z      P&gt;|z|    [0.025   0.975]\n------------------------------------------------------------------\nconst         0.6181    0.3319    1.8623  0.0626  -0.0324   1.2686\nAge          -0.0245    0.0091   -2.7055  0.0068  -0.0423  -0.0068\nTravelAlone   0.1475    0.1973    0.7476  0.4547  -0.2392   0.5343\nPclass_1      2.2826    0.2546    8.9666  0.0000   1.7837   2.7815\nPclass_2      1.3380    0.2340    5.7178  0.0000   0.8793   1.7966\nEmbarked_C    0.5518    0.2351    2.3471  0.0189   0.0910   1.0126\nEmbarked_Q    0.5323    0.3217    1.6548  0.0980  -0.0982   1.1627\nSex_male     -2.6141    0.1973  -13.2482  0.0000  -3.0009  -2.2274\nIsMinor       0.5957    0.3560    1.6735  0.0942  -0.1020   1.2935\n=================================================================\n\n\n\n\nfrom stargazer.stargazer import Stargazer\ntitanic_logit = Stargazer([result, result2])\ntitanic_logit\n\n\nDependent variable: Survived(1)(2)\n\n\nAge-0.012**-0.025***\n(0.006)(0.009)\nEmbarked_C0.601***0.552**\n(0.233)(0.235)\nEmbarked_Q0.637**0.532*\n(0.314)(0.322)\nFare0.002\n(0.002)\nIsMinor0.999***0.596*\n(0.274)(0.356)\nPclass_12.165***2.283***\n(0.284)(0.255)\nPclass_21.395***1.338***\n(0.231)(0.234)\nSex_male-2.553***-2.614***\n(0.196)(0.197)\nTravelAlone0.2800.148\n(0.194)(0.197)\nconst0.618*\n(0.332)\n\n\nObservations891891Pseudo R20.3270.330\nNote:*p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n# Interpreting the coefficients, which are the log odds; therefore, we need to convert them into odds ratio. \nnp.exp(-2.5527)\n\nnp.float64(0.0778711298546485)\n\n\n\nnp.exp(result.params) # Getting the Odds Ratio of all Features \n\nAge            0.987957\nFare           1.001519\nTravelAlone    1.323475\nPclass_1       8.710827\nPclass_2       4.033966\nEmbarked_C     1.824624\nEmbarked_Q     1.889992\nSex_male       0.077872\nIsMinor        2.714860\ndtype: float64\n\n\n\n\n7.1.2 Feature Ranking with Recursive Feature Elimination and Cross-validation (RFECV)\nRFECV performs RFE in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation\n\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(max_iter = 5000), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\nfor x in dir(rfecv):\n  print(x)\n\n_RFECV__metadata_request__fit\n__abstractmethods__\n__annotations__\n__class__\n__delattr__\n__dict__\n__dir__\n__doc__\n__eq__\n__firstlineno__\n__format__\n__ge__\n__getattribute__\n__getstate__\n__gt__\n__hash__\n__init__\n__init_subclass__\n__le__\n__lt__\n__module__\n__ne__\n__new__\n__reduce__\n__reduce_ex__\n__repr__\n__setattr__\n__setstate__\n__sizeof__\n__sklearn_clone__\n__sklearn_tags__\n__static_attributes__\n__str__\n__subclasshook__\n__weakref__\n_abc_impl\n_build_request_for_signature\n_check_feature_names\n_check_n_features\n_doc_link_module\n_doc_link_template\n_doc_link_url_param_generator\n_estimator_type\n_fit\n_get_default_requests\n_get_doc_link\n_get_metadata_request\n_get_param_names\n_get_scorer\n_get_support_mask\n_get_tags\n_more_tags\n_parameter_constraints\n_repr_html_\n_repr_html_inner\n_repr_mimebundle_\n_sklearn_auto_wrap_output_keys\n_transform\n_validate_data\n_validate_params\nclasses_\ncv\ncv_results_\ndecision_function\nestimator\nestimator_\nfeature_names_in_\nfit\nfit_transform\nget_feature_names_out\nget_metadata_routing\nget_params\nget_support\nimportance_getter\ninverse_transform\nmin_features_to_select\nn_features_\nn_features_in_\nn_jobs\npredict\npredict_log_proba\npredict_proba\nranking_\nscore\nscoring\nset_output\nset_params\nstep\nsupport_\ntransform\nverbose\n\n\n\n# To get the accuracy\nrfecv.cv_results_\n\n{'mean_test_score': array([0.78672909, 0.78672909, 0.78672909, 0.79009988, 0.78560549,\n        0.78561798, 0.78790262, 0.79574282, 0.79463171]),\n 'std_test_score': array([0.02859935, 0.02859935, 0.02859935, 0.02989151, 0.02604354,\n        0.02546772, 0.03370206, 0.02689652, 0.02691683]),\n 'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}\n\n\n\ntype(rfecv.cv_results_)\n\ndict\n\n\n\nrfecv.cv_results_\n\n{'mean_test_score': array([0.78672909, 0.78672909, 0.78672909, 0.79009988, 0.78560549,\n        0.78561798, 0.78790262, 0.79574282, 0.79463171]),\n 'std_test_score': array([0.02859935, 0.02859935, 0.02859935, 0.02989151, 0.02604354,\n        0.02546772, 0.03370206, 0.02689652, 0.02691683]),\n 'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}\n\n\n\nrfecv.n_features_\n\nnp.int64(8)\n\n\n\nlist(X.columns[rfecv.support_])\n\n['Age',\n 'TravelAlone',\n 'Pclass_1',\n 'Pclass_2',\n 'Embarked_C',\n 'Embarked_Q',\n 'Sex_male',\n 'IsMinor']\n\n\n\n# To check whether the RFE and RFECV generate the same features\nset(list(X.columns[rfecv.support_])) == set(list((X.columns[rfe.support_])))  \n\nTrue\n\n\n\nrfecv.cv_results_.keys()\n\ndict_keys(['mean_test_score', 'std_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'split5_test_score', 'split6_test_score', 'split7_test_score', 'split8_test_score', 'split9_test_score', 'n_features'])\n\n\n\ncrossVal_results = rfecv.cv_results_\ncrossVal_results\ndel crossVal_results['mean_test_score']\ndel crossVal_results['std_test_score']\ncrossVal_results\n\n{'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#correlation-matrix",
    "href": "chap04_logistic.html#correlation-matrix",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "8 Correlation Matrix",
    "text": "8 Correlation Matrix\n\nSelected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n                     'Embarked_S', 'Sex_male', 'IsMinor']\nX = final_train[Selected_features] # Recreated features vector \n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\") # for cmap = 'viridis' can also be used.\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(final_train[['Survived', 'Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_S', 'Sex_male', 'IsMinor']].corr(), annot = True, cmap = 'viridis')\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#model-evaluation-procedures",
    "href": "chap04_logistic.html#model-evaluation-procedures",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "9 Model Evaluation Procedures",
    "text": "9 Model Evaluation Procedures\n\n9.1 Model Evaluation Based on Train/Test Split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n\nX = final_train[Selected_features]\ny = final_train['Survived']\n\n\n# use train/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n\n\n# Check classification scores of Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test) # Prediction actual class\ny_pred_proba = logreg.predict_proba(X_test) [:,1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba)) \nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr)) \n\nTrain/Test split results:\nLogisticRegression accuracy is 0.810\nLogisticRegression log_loss is 0.446\nLogisticRegression auc is 0.847\n\n\n\n\n9.2 Confusion Matrix\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n\nConfusion matrix\n\n [[98 13]\n [21 47]]\n\nTrue Positives(TP) =  98\n\nTrue Negatives(TN) =  47\n\nFalse Positives(FP) =  13\n\nFalse Negatives(FN) =  21\n\n\n\n# Visualizing Confusion Matrix\nplt.figure(figsize=(6,4))\ncm_matrix = pd.DataFrame(data= cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.3 Classification Report\n\nprint (classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.82      0.88      0.85       111\n           1       0.78      0.69      0.73        68\n\n    accuracy                           0.81       179\n   macro avg       0.80      0.79      0.79       179\nweighted avg       0.81      0.81      0.81       179\n\n\n\n\n\n9.4 ROC-AUC Curve\n\nidx = np.min(np.where(tpr &gt; 0.95)) # index of the first threshold for which the sensibility (true positive rate (tpr)) &gt; 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall/sensitivity)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))\n\n\n\n\n\n\n\n\nUsing a threshold of 0.087 guarantees a sensitivity of 0.956 and a specificity of 0.225, i.e. a false positive rate of 77.48%.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#model-evaluation-based-on-k-fold-cross-validation-cross_val_score-function",
    "href": "chap04_logistic.html#model-evaluation-based-on-k-fold-cross-validation-cross_val_score-function",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "10 Model Evaluation Based on K-fold Cross-validation cross_val_score() Function",
    "text": "10 Model Evaluation Based on K-fold Cross-validation cross_val_score() Function\n\n# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression(max_iter=5000)\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())\n\nK-fold cross-validation results:\nLogisticRegression average accuracy is 0.796\nLogisticRegression average log_loss is 0.454\nLogisticRegression average auc is 0.850\n\n\n\n10.1 Model Evaluation Based on K-fold Cross-validation Using cross_validate() Function\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std())) \n\nK-fold cross-validation results:\nLogisticRegression average accuracy: 0.796 (+/-0.024)\nLogisticRegression average log_loss: 0.454 (+/-0.037)\nLogisticRegression average auc: 0.850 (+/-0.028)\n\n\nWhat happens when we add the feature Fare -\n\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"]\nX = final_train[cols]\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, final_train[cols], y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))\n\nK-fold cross-validation results:\nLogisticRegression average accuracy: 0.795 (+/-0.027)\nLogisticRegression average log_loss: 0.455 (+/-0.037)\nLogisticRegression average auc: 0.849 (+/-0.028)\n\n\nWe notice that the model is slightly deteriorated. The Fare variable does not carry any useful information. Its presence is just a noise for the logistic regression model.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#gridsearchcv-evaluating-using-multiple-scorers-simultaneously",
    "href": "chap04_logistic.html#gridsearchcv-evaluating-using-multiple-scorers-simultaneously",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "11 GridSearchCV Evaluating Using Multiple Scorers Simultaneously",
    "text": "11 GridSearchCV Evaluating Using Multiple Scorers Simultaneously\n\nfrom sklearn.model_selection import GridSearchCV\n\nX = final_train[Selected_features]\n\nparam_grid = {'C': np.arange(1e-05, 3, 0.1)}\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n\ngs = GridSearchCV(LogisticRegression(max_iter=5000), return_train_score=True,\n                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n\ngs.fit(X, y)\nresults = gs.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(gs.best_estimator_))\nprint(\"best params: \" + str(gs.best_params_))\nprint('best score:', gs.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, param_grid['C'].max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n\n====================\nbest params: LogisticRegression(C=np.float64(2.50001), max_iter=5000)\nbest params: {'C': np.float64(2.50001)}\nbest score: 0.8069662921348316\n====================\n\n\n\n\n\n\n\n\n\n\n11.1 GridSearchCV Evaluating Using Multiple scorers, RepeatedStratifiedKFold and pipeline for Preprocessing Simultaneously\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\n\n#Define simple model\n###############################################################################\nC = np.arange(1e-05, 5.5, 0.1)\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\nlog_reg = LogisticRegression(max_iter=5000)\n\n#Simple pre-processing estimators\n###############################################################################\nstd_scale = StandardScaler(with_mean=False, with_std=False)\n#std_scale = StandardScaler()\n\n#Defining the CV method: Using the Repeated Stratified K Fold\n###############################################################################\n\nn_folds=5\nn_repeats=5\n\nrskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)\n\n#Creating simple pipeline and defining the gridsearch\n###############################################################################\n\nlog_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])\n\nlog_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,\n              scoring=scoring, return_train_score=True,\n              param_grid=dict(clf__C=C), refit='Accuracy')\n\nlog_clf.fit(X, y)\nresults = log_clf.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(log_clf.best_estimator_))\nprint(\"best params: \" + str(log_clf.best_params_))\nprint('best score:', log_clf.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, C.max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_clf__C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n\n====================\nbest params: Pipeline(steps=[('scale', StandardScaler(with_mean=False, with_std=False)),\n                ('clf',\n                 LogisticRegression(C=np.float64(4.90001), max_iter=5000))])\nbest params: {'clf__C': np.float64(4.90001)}\nbest score: 0.7995505617977527\n====================",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#regularization",
    "href": "chap04_logistic.html#regularization",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "12 Regularization",
    "text": "12 Regularization\nRegularization is a method of preventing overfitting, which is a common problem in machine learning. Overfitting means that your model learns too much from the specific data you have, and fails to generalize well to new or unseen data. This can lead to poor predictions and low performance. Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables.\nSince the size of each coefficient depends on the scale of its corresponding variable, scaling the data is required so that the regularization penalizes each variable equally. The regularization strength is determined by C and as C increases, the regularization term becomes smaller (and for extremely large C values, it’s as if there is no regularization at all).\nIf the initial model is overfit (as in, it fits the training data too well), then adding a strong regularization term (with small C value) makes the model perform worse for the training data, but introducing such “noise” improves the model’s performance on unseen (or test) data.\nAn example with 1000 samples and 200 features shown below. As can be seen from the plot of accuracy over different values of C, if C is large (with very little regularization), there is a big gap between how the model performs on training data and test data. However, as C decreases, the model performs worse on training data but performs better on test data (test accuracy increases). However, when C becomes too small (or the regularization becomes too strong), the model begins performing worse again because now the regularization term completely dominates the objective function.\n\n# Necessary Python Packages \nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# make sample data\nX, y = make_classification(1000, 200, n_informative=195, random_state=2023)\n# split into train-test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2023)\n\n# normalize the data\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# train Logistic Regression models for different values of C\n# and collect train and test accuracies\nscores = {}\nfor C in (10**k for k in range(-6, 6)):\n    lr = LogisticRegression(C=C)\n    lr.fit(X_train, y_train)\n    scores[C] = {'train accuracy': lr.score(X_train, y_train), \n                 'test accuracy': lr.score(X_test, y_test)}\n\n# plot the accuracy scores for different values of C\npd.DataFrame.from_dict(scores, 'index').plot(logx=True, xlabel='C', ylabel='accuracy')\n\n\n\n\n\n\n\n\n\n12.1 Types of Regularization\n\n12.1.1 L1 regularization\nL1 regularization, also known as Lasso regularization, adds the sum of the absolute values of the model’s coefficients to the loss function. It encourages sparsity in the model by shrinking some coefficients to precisely zero. This has the effect of performing feature selection, as the model can effectively ignore irrelevant or less important features. L1 regularization is particularly useful when dealing with high-dimensional datasets with desired feature selection.\nMathematically, the L1 regularization term can be written as:\nL1 regularization = λ * Σ|wi|\nHere, λ is the regularization parameter that controls the strength of regularization, wi represents the individual model coefficients and the sum is taken over all coefficients.\n\n\n12.1.2 L2 regularization\nL2 regularization, also known as Ridge regularization, adds the sum of the squared values of the model’s coefficients to the loss function. Unlike L1 regularization, L2 regularization does not force the coefficients to be exactly zero but instead encourages them to be small. L2 regularization can prevent overfitting by spreading the influence of a single feature across multiple features. It is advantageous when there are correlations between the input features.\nMathematically, the L2 regularization term can be written as:\nL2 regularization = λ * Σ(wi^2)\nSimilar to L1 regularization, λ is the regularization parameter, and wi represents the model coefficients. The sum is taken over all coefficients, and the squares of the coefficients are summed.\nThe choice between L1 and L2 regularization depends on the specific problem and the characteristics of the data. For example, L1 regularization produces sparse models, which can be advantageous when feature selection is desired. L2 regularization, on the other hand, encourages small but non-zero coefficients and can be more suitable when there are strong correlations between features.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#conclusion",
    "href": "chap04_logistic.html#conclusion",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "13 Conclusion",
    "text": "13 Conclusion",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap06_naive_bayes.html",
    "href": "chap06_naive_bayes.html",
    "title": "\nChapter # 06 (Part A)  Naive Bayes",
    "section": "",
    "text": "The Naive Bayes Classifier technique is based on the so-called Bayesian theorem and is particularly suited when the dimensionality of the inputs is high. Despite its simplicity, Naive Bayes can often outperform more sophisticated classification methods.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part A) <br> Naive Bayes"
    ]
  },
  {
    "objectID": "chap06_naive_bayes.html#introduction",
    "href": "chap06_naive_bayes.html#introduction",
    "title": "\nChapter # 06 (Part A)  Naive Bayes",
    "section": "",
    "text": "The Naive Bayes Classifier technique is based on the so-called Bayesian theorem and is particularly suited when the dimensionality of the inputs is high. Despite its simplicity, Naive Bayes can often outperform more sophisticated classification methods.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part A) <br> Naive Bayes"
    ]
  },
  {
    "objectID": "chap06_naive_bayes.html#bayes-theorem",
    "href": "chap06_naive_bayes.html#bayes-theorem",
    "title": "\nChapter # 06 (Part A)  Naive Bayes",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nThe algorithm is based on the famous Bayes theorem named after Rev. Thomas Bayes. It works on conditional probability. Conditional probability is the probability that something will happen, given that something else has already occurred. Using the conditional probability, we can calculate the probability of an event using its prior knowledge.\nBayes’ theorem is stated mathematically as the following equation:\n\\[{\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)\\,P(A)}{P(B)}},}\\] where \\(A\\) and \\(B\\) are events and \\(P(B)\\neq{0}\\).\n\\(P(A\\mid B)\\) is a conditional probability: the likelihood of event \\(A\\) occurring given that \\(B\\) is true.\n\\(P(B\\mid A)\\) is also a conditional probability: the likelihood of event \\(B\\) occurring given that \\(A\\) is true.\n\\(P(A)\\) and \\(P(B)\\) are the probabilities of observing \\(A\\) and \\(B\\) independently of each other; this is known as the marginal probability.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part A) <br> Naive Bayes"
    ]
  },
  {
    "objectID": "chap06_naive_bayes.html#whats-naive-in-naive-bayes-and-why-is-it-a-superfast-algorithm",
    "href": "chap06_naive_bayes.html#whats-naive-in-naive-bayes-and-why-is-it-a-superfast-algorithm",
    "title": "\nChapter # 06 (Part A)  Naive Bayes",
    "section": "What’s Naive in Naive Bayes and why is it a superfast algorithm?",
    "text": "What’s Naive in Naive Bayes and why is it a superfast algorithm?\nIt is called naive Bayes or idiot Bayes because the calculation of the probabilities for each hypothesis are simplified to make their calculation tractable. Rather than attempting to calculate the values of each attribute value, they are assumed to be conditionally independent given the target value.\nThis is a very strong assumption that is most unlikely in real data, i.e. that the attributes do not interact. Nevertheless, the approach performs surprisingly well on data where this assumption does not hold.\nTraining is fast because only the probability of each class and the probability of each class given different input values need to be calculated. No coefficients need to be fitted by optimization procedures.\nThe class probabilities are simply the frequency of instances that belong to each class divided by the total number of instances. The conditional probabilities are the frequency of each attribute value for a given class value divided by the frequency of instances with that class value.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part A) <br> Naive Bayes"
    ]
  },
  {
    "objectID": "chap06_naive_bayes.html#data-analyzed-in-this-notebook",
    "href": "chap06_naive_bayes.html#data-analyzed-in-this-notebook",
    "title": "\nChapter # 06 (Part A)  Naive Bayes",
    "section": "Data analyzed in this notebook",
    "text": "Data analyzed in this notebook\nIn this notebook, we will show how to use Python scikit-learn’s Naive Bayes method to classify origin of wine based on physio-chemical analysis data. These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\nDetails can be found here.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part A) <br> Naive Bayes"
    ]
  },
  {
    "objectID": "chap06_naive_bayes.html#read-in-the-data-and-perform-basic-exploratory-analysis",
    "href": "chap06_naive_bayes.html#read-in-the-data-and-perform-basic-exploratory-analysis",
    "title": "\nChapter # 06 (Part A)  Naive Bayes",
    "section": "Read in the data and perform basic exploratory analysis",
    "text": "Read in the data and perform basic exploratory analysis\n\nData set\n\ndf = pd.read_csv('DATA/wine.data.csv')\ndf.head(10)\n\n\n\n\n\n\n\n\nWine\nAlcohol\nMalic.acid\nAsh\nAcl\nMg\nPhenols\nFlavanoids\nNonflavanoid.phenols\nProanth\nColor.int\nHue\nOD\nProline\n\n\n\n\n0\n1\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n\n\n1\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n\n\n2\n1\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n\n\n3\n1\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n\n\n4\n1\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\n\n\n5\n1\n14.20\n1.76\n2.45\n15.2\n112\n3.27\n3.39\n0.34\n1.97\n6.75\n1.05\n2.85\n1450\n\n\n6\n1\n14.39\n1.87\n2.45\n14.6\n96\n2.50\n2.52\n0.30\n1.98\n5.25\n1.02\n3.58\n1290\n\n\n7\n1\n14.06\n2.15\n2.61\n17.6\n121\n2.60\n2.51\n0.31\n1.25\n5.05\n1.06\n3.58\n1295\n\n\n8\n1\n14.83\n1.64\n2.17\n14.0\n97\n2.80\n2.98\n0.29\n1.98\n5.20\n1.08\n2.85\n1045\n\n\n9\n1\n13.86\n1.35\n2.27\n16.0\n98\n2.98\n3.15\n0.22\n1.85\n7.22\n1.01\n3.55\n1045\n\n\n\n\n\n\n\n\n\nBasic statistics of the features\n\ndf.iloc[:,1:].describe()\n\n\n\n\n\n\n\n\nAlcohol\nMalic.acid\nAsh\nAcl\nMg\nPhenols\nFlavanoids\nNonflavanoid.phenols\nProanth\nColor.int\nHue\nOD\nProline\n\n\n\n\ncount\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n\n\nmean\n13.000618\n2.336348\n2.366517\n19.494944\n99.741573\n2.295112\n2.029270\n0.361854\n1.590899\n5.058090\n0.957449\n2.611685\n746.893258\n\n\nstd\n0.811827\n1.117146\n0.274344\n3.339564\n14.282484\n0.625851\n0.998859\n0.124453\n0.572359\n2.318286\n0.228572\n0.709990\n314.907474\n\n\nmin\n11.030000\n0.740000\n1.360000\n10.600000\n70.000000\n0.980000\n0.340000\n0.130000\n0.410000\n1.280000\n0.480000\n1.270000\n278.000000\n\n\n25%\n12.362500\n1.602500\n2.210000\n17.200000\n88.000000\n1.742500\n1.205000\n0.270000\n1.250000\n3.220000\n0.782500\n1.937500\n500.500000\n\n\n50%\n13.050000\n1.865000\n2.360000\n19.500000\n98.000000\n2.355000\n2.135000\n0.340000\n1.555000\n4.690000\n0.965000\n2.780000\n673.500000\n\n\n75%\n13.677500\n3.082500\n2.557500\n21.500000\n107.000000\n2.800000\n2.875000\n0.437500\n1.950000\n6.200000\n1.120000\n3.170000\n985.000000\n\n\nmax\n14.830000\n5.800000\n3.230000\n30.000000\n162.000000\n3.880000\n5.080000\n0.660000\n3.580000\n13.000000\n1.710000\n4.000000\n1680.000000\n\n\n\n\n\n\n\n\n\nBoxplots by output labels/classes\n\nfor c in df.columns[1:]:\n    df.boxplot(c,by='Wine',figsize=(7,4),fontsize=14)\n    plt.title(\"{}\\n\".format(c),fontsize=16)\n    plt.xlabel(\"Wine Class\", fontsize=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt can be seen that some features classify the wine labels pretty clearly. For example, Alcalinity, Total Phenols, or Flavonoids produce boxplots with well-separated medians, which are clearly indicative of wine classes.\nBelow is an example of class seperation using two variables\n\nplt.figure(figsize=(10,6))\nplt.scatter(df['OD'],df['Flavanoids'],c=df['Wine'],edgecolors='k',alpha=0.8,s=100)\nplt.grid(True)\nplt.title(\"Scatter plot of two features showing the correlation and class seperation\",fontsize=15)\nplt.xlabel(\"OD280/OD315 of diluted wines\",fontsize=15)\nplt.ylabel(\"Flavanoids\",fontsize=15)\n\nText(0, 0.5, 'Flavanoids')\n\n\n\n\n\n\n\n\n\n\n\nAre the features independent? Plot co-variance matrix\nIt can be seen that there are some good amount of correlation between features i.e. they are not independent of each other, as assumed in Naive Bayes technique. However, we will still go ahead and apply yhe classifier to see its performance.\n\ndef correlation_matrix(df):\n    from matplotlib import pyplot as plt\n    from matplotlib import cm as cm\n\n    fig = plt.figure(figsize=(16,12))\n    ax1 = fig.add_subplot(111)\n    cmap = cm.get_cmap('jet', 30)\n    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n    ax1.grid(True)\n    plt.title('Wine data set features correlation',fontsize=15)\n    labels=df.columns\n    ax1.set_xticklabels(labels,fontsize=9)\n    ax1.set_yticklabels(labels,fontsize=9)\n    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n    fig.colorbar(cax, ticks=[0.1*i for i in range(-11,11)])\n    plt.show()\n\ncorrelation_matrix(df)\n\nC:\\Users\\mshar\\AppData\\Local\\Temp\\ipykernel_21364\\3096039824.py:7: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  cmap = cm.get_cmap('jet', 30)\nC:\\Users\\mshar\\AppData\\Local\\Temp\\ipykernel_21364\\3096039824.py:12: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax1.set_xticklabels(labels,fontsize=9)\nC:\\Users\\mshar\\AppData\\Local\\Temp\\ipykernel_21364\\3096039824.py:13: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax1.set_yticklabels(labels,fontsize=9)",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part A) <br> Naive Bayes"
    ]
  },
  {
    "objectID": "chap06_naive_bayes.html#naive-bayes-classification",
    "href": "chap06_naive_bayes.html#naive-bayes-classification",
    "title": "\nChapter # 06 (Part A)  Naive Bayes",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\nTest/train split\n\nfrom sklearn.model_selection import train_test_split\n\ntest_size=0.3 # Test-set fraction\n\n\nX = df.drop('Wine',axis=1)\ny = df['Wine']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n\n\nX_train.shape\n\n(124, 13)\n\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nAlcohol\nMalic.acid\nAsh\nAcl\nMg\nPhenols\nFlavanoids\nNonflavanoid.phenols\nProanth\nColor.int\nHue\nOD\nProline\n\n\n\n\n23\n12.85\n1.60\n2.52\n17.8\n95\n2.48\n2.37\n0.26\n1.46\n3.93\n1.09\n3.63\n1015\n\n\n156\n13.84\n4.12\n2.38\n19.5\n89\n1.80\n0.83\n0.48\n1.56\n9.01\n0.57\n1.64\n480\n\n\n154\n12.58\n1.29\n2.10\n20.0\n103\n1.48\n0.58\n0.53\n1.40\n7.60\n0.58\n1.55\n640\n\n\n63\n12.37\n1.13\n2.16\n19.0\n87\n3.50\n3.10\n0.19\n1.87\n4.45\n1.22\n2.87\n420\n\n\n109\n11.61\n1.35\n2.70\n20.0\n94\n2.74\n2.92\n0.29\n2.49\n2.65\n0.96\n3.26\n680\n\n\n\n\n\n\n\n\n\nClassification using GaussianNB\nGiven a class variable \\(y\\) and a dependent feature vector \\(x_1\\) through \\(x_n\\), Bayes’ theorem states the following relationship:\n\\[P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)} {P(x_1, \\dots, x_n)}\\] Using the naive independence assumption that \\[P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y),\\] for all \\(i\\), this relationship is simplified to \\[P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)} {P(x_1, \\dots, x_n)}\\]\nSince \\(P(x_1, \\dots, x_n)\\) is constant given the input, we can use the following classification rule: \\[P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\] \\[\\Downarrow\\] \\[\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\]\nand we can use Maximum A Posteriori (MAP) estimation to estimate \\(P(y)\\) and \\(P(x_i \\mid y)\\); the former is then the relative frequency of class \\(y\\) in the training set.\nGaussianNB () implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\n\\[ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}) \\]\nThe parameters \\(\\sigma_y\\) and \\(\\mu_y\\) are estimated using maximum likelihood.\n\nfrom sklearn.naive_bayes import GaussianNB\n\n\nnbc = GaussianNB()\n\n\nnbc.fit(X_train,y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNB?Documentation for GaussianNBiFittedGaussianNB() \n\n\n\n\nPrediction, classification report, and confusion matrix\n\ny_pred = nbc.predict(X_test)\nmislabel = np.sum((y_test!=y_pred))\nprint(\"Total number of mislabelled data points from {} test samples is {}\".format(len(y_test),mislabel))\n\nTotal number of mislabelled data points from 54 test samples is 3\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nprint(\"The classification report is as follows...\")\nprint(classification_report(y_pred,y_test))\n\nThe classification report is as follows...\n              precision    recall  f1-score   support\n\n           1       0.88      1.00      0.93        14\n           2       0.95      0.91      0.93        22\n           3       1.00      0.94      0.97        18\n\n    accuracy                           0.94        54\n   macro avg       0.94      0.95      0.94        54\nweighted avg       0.95      0.94      0.94        54\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\ncm = (confusion_matrix(y_test,y_pred))\ncmdf = pd.DataFrame(cm,index=['Class 1','Class 2',' Class 3'], columns=['Class 1','Class 2',' Class 3'])\nprint(\"The confusion matrix looks like following...\")\ncmdf\n\nThe confusion matrix looks like following...\n\n\n\n\n\n\n\n\n\nClass 1\nClass 2\nClass 3\n\n\n\n\nClass 1\n14\n2\n0\n\n\nClass 2\n0\n20\n1\n\n\nClass 3\n0\n0\n17\n\n\n\n\n\n\n\nThis showed that even in the presence of correlation among features, the Naive Bayes algorithm performed quite well and could separate the classes easily",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part A) <br> Naive Bayes"
    ]
  },
  {
    "objectID": "chap06_naive_bayes.html#conclusion",
    "href": "chap06_naive_bayes.html#conclusion",
    "title": "\nChapter # 06 (Part A)  Naive Bayes",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 06 (Part A) <br> Naive Bayes"
    ]
  },
  {
    "objectID": "chap07_regression.html",
    "href": "chap07_regression.html",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "",
    "text": "Multiple linear regression is used to model the relationship between a continuous response variable and continuous or categorical explanatory variables. Multiple linear regression allows to evaluate the relationship between two variables, while controlling for the effect (i.e., removing the effect) of other variables.\n\n\n\nimport os\n# Current Working Directory \nos.getcwd()\n\n\n# Files in Working Directory\nfor file in os.listdir():\n  print(file)",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap09_pca.html",
    "href": "chap09_pca.html",
    "title": "\nChapter # 09  Dimension Reduction & PCA",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Modules & Slides",
      "Unsupervised ML Algorithm",
      "<center> Chapter # 09 <br> Dimension Reduction & PCA"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nMachine Learning in Business",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/datetimes/test_indexing.html",
    "href": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/datetimes/test_indexing.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/period/test_indexing.html",
    "href": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/period/test_indexing.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "href": "mlbusiness2/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "Copyright (c) 2012-2023, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "other-materials.html",
    "href": "other-materials.html",
    "title": "Data & Other Materials",
    "section": "",
    "text": "Data used in different chapters of the course are available here."
  },
  {
    "objectID": "other-materials.html#data",
    "href": "other-materials.html#data",
    "title": "Data & Other Materials",
    "section": "",
    "text": "Data used in different chapters of the course are available here."
  },
  {
    "objectID": "other-materials.html#other-materials",
    "href": "other-materials.html#other-materials",
    "title": "Data & Other Materials",
    "section": "Other Materials",
    "text": "Other Materials\nOther materials of the course will be available here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The project files for the course are available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Readings",
    "section": "",
    "text": "The readings for the course are available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "r_eda.html",
    "href": "r_eda.html",
    "title": "\nExploratory Data Analysis in R",
    "section": "",
    "text": "The objective of this document is to introduce the necessary functions from tidyverse package for data manipulation and data visualization. There are basically six functions - select(), filter(), mutate(), arrange(), group_by(), and summarize() - from dplyr package of tidyverse ecosystem that are very much necessary for data manipulation. These six functions can be used for 80% of data manipulation problems. Additionally, this handout also introduces ggplot functions from tidyverse. ggplot is considered very effective for data visualization.",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#st-first-verb---select",
    "href": "r_eda.html#st-first-verb---select",
    "title": "\nExploratory Data Analysis in R",
    "section": "7.1 1st (First) verb - select ()",
    "text": "7.1 1st (First) verb - select ()\n      The select () function is used to select some columns from your data set. For example, if you want to select all variables except SIZE_EUROPE and SIZE_UK from your data set. Then you should write the following code (We created a new data set called product2)\n\nproduct2 &lt;- product %&gt;% \n  select(\n   -SIZE_EUROPE, - SIZE_UK  \n  )  # 1st Verb\nglimpse(product2)\n\nRows: 14,967\nColumns: 12\n$ INVOICE_NO &lt;chr&gt; \"52389\", \"52390\", \"52391\", \"52392\", \"52393\", \"52394\", \"5239…\n$ DATE       &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01…\n$ COUNTRY    &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United States…\n$ PRODUCT_ID &lt;chr&gt; \"2152\", \"2230\", \"2160\", \"2234\", \"2222\", \"2173\", \"2200\", \"22…\n$ SHOP       &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5\"…\n$ GENDER     &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Female…\n$ SIZE_US    &lt;chr&gt; \"11\", \"11.5\", \"9.5\", \"9.5\", \"9\", \"10.5\", \"9\", \"10\", \"10.5\",…\n$ UNIT_PRICE &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169,…\n$ DISCOUNT   &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1,…\n$ YEAR       &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,…\n$ MONTH      &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",…\n$ SALE_PRICE &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 139…",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#nd-second-verb---filter",
    "href": "r_eda.html#nd-second-verb---filter",
    "title": "\nExploratory Data Analysis in R",
    "section": "9.1 2nd (Second) verb - filter ()",
    "text": "9.1 2nd (Second) verb - filter ()\n      If we want to subset our dataset by rows, then filter () is used. For example - we want to create a data set that will include only observations for United States, then we should write the following code. The name of the dataset is given US.\n\nUS &lt;- product %&gt;% \n  filter(\n    COUNTRY == \"United States\"\n  )   # 2nd Verb\n\nglimpse(US)\n\nRows: 5,886\nColumns: 14\n$ INVOICE_NO  &lt;chr&gt; \"52390\", \"52392\", \"52394\", \"52397\", \"52399\", \"52399\", \"523…\n$ DATE        &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-02, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United…\n$ PRODUCT_ID  &lt;chr&gt; \"2230\", \"2234\", \"2173\", \"2191\", \"2197\", \"2213\", \"2206\", \"2…\n$ SHOP        &lt;chr&gt; \"US15\", \"US6\", \"US15\", \"US13\", \"US1\", \"US11\", \"US2\", \"US15…\n$ GENDER      &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Femal…\n$ SIZE_US     &lt;chr&gt; \"11.5\", \"9.5\", \"10.5\", \"10.5\", \"10\", \"9.5\", \"9.5\", \"8\", \"1…\n$ SIZE_EUROPE &lt;chr&gt; \"44-45\", \"40\", \"43-44\", \"43-44\", \"43\", \"40\", \"40\", \"41\", \"…\n$ SIZE_UK     &lt;dbl&gt; 11.0, 7.5, 10.0, 10.0, 9.5, 7.5, 7.5, 7.5, 10.5, 7.5, 9.5,…\n$ UNIT_PRICE  &lt;dbl&gt; 199, 159, 159, 139, 129, 169, 139, 139, 149, 159, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.2, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.5, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ SALE_PRICE  &lt;dbl&gt; 159.2, 159.0, 159.0, 139.0, 129.0, 152.1, 139.0, 139.0, 74…\n\nGermany &lt;- product %&gt;% \n  filter(\n    COUNTRY == \"Germany\" & YEAR %in% c ('2014', '2015')\n  ) \nglimpse(Germany)\n\nRows: 2,229\nColumns: 14\n$ INVOICE_NO  &lt;chr&gt; \"52395\", \"52401\", \"52401\", \"52408\", \"52409\", \"52414\", \"524…\n$ DATE        &lt;date&gt; 2014-01-02, 2014-01-03, 2014-01-03, 2014-01-04, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"Germany\", \"Germany\", \"Germany\", \"Germany\", \"Germany\", \"Ge…\n$ PRODUCT_ID  &lt;chr&gt; \"2200\", \"2235\", \"2197\", \"2206\", \"2157\", \"2235\", \"2239\", \"2…\n$ SHOP        &lt;chr&gt; \"GER2\", \"GER1\", \"GER1\", \"GER2\", \"GER2\", \"GER1\", \"GER2\", \"G…\n$ GENDER      &lt;chr&gt; \"Female\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Femal…\n$ SIZE_US     &lt;chr&gt; \"9\", \"10.5\", \"8.5\", \"8.5\", \"12\", \"9.5\", \"8.5\", \"10.5\", \"10…\n$ SIZE_EUROPE &lt;chr&gt; \"39-40\", \"43-44\", \"39\", \"41-42\", \"45\", \"42-43\", \"39\", \"43-…\n$ SIZE_UK     &lt;dbl&gt; 7.0, 10.0, 6.5, 8.0, 11.5, 9.0, 6.5, 10.0, 9.5, 8.0, 10.0,…\n$ UNIT_PRICE  &lt;dbl&gt; 179, 169, 179, 149, 149, 169, 129, 169, 199, 149, 169, 189…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.5, 0.2, 0.2, 0.2, 0.3, 0.5, 0.5, 0.0, 0.2, 0.5, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ SALE_PRICE  &lt;dbl&gt; 179.0, 84.5, 143.2, 119.2, 119.2, 118.3, 64.5, 84.5, 199.0…\n\ncount (Germany, YEAR)\n\n# A tibble: 2 × 2\n   YEAR     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2014   710\n2  2015  1519\n\n\nQUESTIONS - 1. Filter those observations that belong to United States and Germany and that are related to Male Gender.",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#rd-third-verb---summarize",
    "href": "r_eda.html#rd-third-verb---summarize",
    "title": "\nExploratory Data Analysis in R",
    "section": "10.1 3rd (Third) verb - summarize ()",
    "text": "10.1 3rd (Third) verb - summarize ()\n      The summarize () function is used to calculate different statistics such as mean, median, standard deviation, maximum, and minimum value. For example, we want to calculate the average price of all products -\n\nproduct %&gt;% \n  summarize(AVG_PRICE = mean(SALE_PRICE)) # 3rd Verb\n\n# A tibble: 1 × 1\n  AVG_PRICE\n      &lt;dbl&gt;\n1      144.",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#th-fourth-verb---group_by",
    "href": "r_eda.html#th-fourth-verb---group_by",
    "title": "\nExploratory Data Analysis in R",
    "section": "10.2 4th (Fourth) verb - group_by ()",
    "text": "10.2 4th (Fourth) verb - group_by ()\n      The group_by () function is very useful when it is used with summarize () function. For example, we want to know the average price for each country; then, we should write the following code -",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#th-fifth-verb---arrange",
    "href": "r_eda.html#th-fifth-verb---arrange",
    "title": "\nExploratory Data Analysis in R",
    "section": "10.3 5th (Fifth) verb - arrange ()",
    "text": "10.3 5th (Fifth) verb - arrange ()\n      The arrange ()function allows you to reorder your data set by one or more variables. For example, if you want to reorder the average price in difference countries, you need to execute the following code -\n\nproduct %&gt;% \n  group_by(COUNTRY) %&gt;% # 4th Verb \n  summarise(AVG_PRICE = mean(SALE_PRICE)) %&gt;% \n  arrange(AVG_PRICE) # 5th Verb\n\n# A tibble: 4 × 2\n  COUNTRY        AVG_PRICE\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Germany             144.\n2 United States       144.\n3 Canada              144.\n4 United Kingdom      146.\n\n\nQUESTIONS - 1. Calculate the average price for both Gender. Who pays greater price? 2. Calculate the average discount for both Gender. Who gets higher discount? 3. Calculate the average discount for each month. In which month highest discount is provided?",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#tidy-data",
    "href": "r_eda.html#tidy-data",
    "title": "\nExploratory Data Analysis in R",
    "section": "11.1 Tidy Data",
    "text": "11.1 Tidy Data\n      There are three interrelated rules which make a dataset tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\n\n\n\n\n\n\n\n\n# An example of a Tidy Dataset \ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n      Manipulating tidy data is easy. For example, for dataset table1, if we want to measure the rate of, we can do it easily.\n\ntable1 %&gt;% \n  mutate(rate = cases / population * 10000)\n\n# A tibble: 6 × 5\n  country      year  cases population  rate\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071 0.373\n2 Afghanistan  2000   2666   20595360 1.29 \n3 Brazil       1999  37737  172006362 2.19 \n4 Brazil       2000  80488  174504898 4.61 \n5 China        1999 212258 1272915272 1.67 \n6 China        2000 213766 1280428583 1.67",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#untidy-data",
    "href": "r_eda.html#untidy-data",
    "title": "\nExploratory Data Analysis in R",
    "section": "11.2 Untidy Data",
    "text": "11.2 Untidy Data\n      Untidy data violate the principle of the tidy data. Therefore, we need to apply analytics to transform it into tidy data. There are two important functions from tidyr that can be used to reshape data. The first one is - pivot_wider function and the second one is pivot_longer function. pivot_wider widens a LONG data whereas pivot_longer lengthens a WIDE data.\n\n\n\n\n\n\n\n\n\n\n# Some Built-in Untidy Datasets\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n\n11.2.1 pivot_longer()\n\ntable4a %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\ntable4b %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\n# A tibble: 6 × 3\n  country     year  population\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n\ntidy4a &lt;- table4a %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\ntidy4b &lt;- table4b %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\nleft_join(tidy4a, tidy4b,\n          by = c ('country', 'year'))\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\n11.2.2 pivot_wider()\n\ntable2 %&gt;%\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#identify-total-sales-by-gender-of-each-product-product_id",
    "href": "r_eda.html#identify-total-sales-by-gender-of-each-product-product_id",
    "title": "\nExploratory Data Analysis in R",
    "section": "11.3 Identify Total Sales by Gender of Each Product (PRODUCT_ID)",
    "text": "11.3 Identify Total Sales by Gender of Each Product (PRODUCT_ID)\n\n11.3.1 6th (Sixth) verb - mutate ()\n      The function mutate () is used to create new variables (columns). For example, we want to know the TOTAL_SALE, which is the sum of the sum of sale by gender; then, we should write the following code -\n\nproduct %&gt;%\n  count(PRODUCT_ID, GENDER) %&gt;%\n  arrange(-n) %&gt;% \n  pivot_wider(\n    names_from = GENDER,\n    values_from = n\n  ) %&gt;% \n  rename_all(toupper) %&gt;% \n  rename_at(vars(c(\"MALE\",\"FEMALE\")), ~paste0(.x,\"_SALE\")) %&gt;% \n  mutate(\n    TOTAL_SALE = MALE_SALE + FEMALE_SALE\n  ) %&gt;% # 6th Verb\n  arrange(-TOTAL_SALE)\n\n# A tibble: 96 × 4\n   PRODUCT_ID MALE_SALE FEMALE_SALE TOTAL_SALE\n   &lt;chr&gt;          &lt;int&gt;       &lt;int&gt;      &lt;int&gt;\n 1 2190             132          75        207\n 2 2226             123          81        204\n 3 2213             114          90        204\n 4 2192             135          66        201\n 5 2158             120          78        198\n 6 2172             114          75        189\n 7 2179             102          87        189\n 8 2239              87         102        189\n 9 2225             117          69        186\n10 2183             123          60        183\n# ℹ 86 more rows\n\n\nQUESTIONS - 1. Identify Total Sales by Gender of Each Shoe Size (SIZE_US)",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#create-a-bar-chart-of-sale-of-shoes-by-gender-of-different-sizes",
    "href": "r_eda.html#create-a-bar-chart-of-sale-of-shoes-by-gender-of-different-sizes",
    "title": "\nExploratory Data Analysis in R",
    "section": "12.1 Create a Bar Chart of Sale of Shoes by Gender of Different Sizes",
    "text": "12.1 Create a Bar Chart of Sale of Shoes by Gender of Different Sizes\n\nproduct %&gt;%\n  count(SIZE_US, GENDER) %&gt;%\n  pivot_wider(\n    names_from = \"GENDER\",\n    values_from = \"n\"\n  ) %&gt;%\n  rename_all(toupper) %&gt;%\n  replace(is.na(.),0) %&gt;%\n  mutate(\n    TOTAL_SALES = FEMALE + MALE\n  ) %&gt;%\n  pivot_longer(\n    cols = c(\"FEMALE\", \"MALE\"),\n    names_to = \"GENDER\",\n    values_to = \"GENDERSALES\"\n  )%&gt;%\n  ggplot(aes(x=reorder(SIZE_US,as.numeric(SIZE_US)), y= GENDERSALES, fill = GENDER))+\n  geom_bar(stat = \"identity\")+\n  labs(x = \"SHOE SIZE\",\n       y = \"TOTAL SALES\",\n       title = \"SALES OF DIFFERENT SIZES OF SHOE\")+\n  geom_text(aes(label = GENDERSALES),\n            position = position_stack(vjust = 0.5),\n            color = \"black\",\n            size = 2\n  )+\n  theme(legend.title = element_blank())",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#create-a-bar-chart-of-sale-of-shoes-by-gender-of-different-sizes-in-different-countries",
    "href": "r_eda.html#create-a-bar-chart-of-sale-of-shoes-by-gender-of-different-sizes-in-different-countries",
    "title": "\nExploratory Data Analysis in R",
    "section": "12.2 Create a Bar Chart of Sale of Shoes by Gender of Different Sizes in different countries",
    "text": "12.2 Create a Bar Chart of Sale of Shoes by Gender of Different Sizes in different countries\n\nproduct %&gt;%\n  count(SIZE_US, GENDER, COUNTRY) %&gt;%\n  ggplot(aes(x=reorder(SIZE_US,as.numeric(SIZE_US)), y= n, fill = GENDER))+\n  geom_bar(stat = \"identity\")+\n  labs(x = \"SHOE SIZE\",\n       y = \"TOTAL SALES\",\n       title = \"SALES OF DIFFERENT SIZES OF SHOE BY GENDER IN DIFFERENT COUNTRIES\"\n  )+\n  geom_text(\n    aes(label = n),\n    position = position_stack(vjust = 0.5),\n    color = \"black\",\n    size = 2\n  )+\n  facet_wrap(~ COUNTRY, nrow = 2, ncol = 2\n  )+\n  theme(legend.position=\"top\",\n        legend.title = element_blank())+\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#create-a-bar-chart-for-product-id-product_id-2190-sales-by-shoes-sizes",
    "href": "r_eda.html#create-a-bar-chart-for-product-id-product_id-2190-sales-by-shoes-sizes",
    "title": "\nExploratory Data Analysis in R",
    "section": "12.3 Create a Bar Chart for Product ID (PRODUCT_ID) 2190 Sales by shoes sizes",
    "text": "12.3 Create a Bar Chart for Product ID (PRODUCT_ID) 2190 Sales by shoes sizes\n\nproduct %&gt;% \n  filter(\n    PRODUCT_ID == \"2190\"\n  ) %&gt;% \n  count(SIZE_US) %&gt;% \n  mutate(SIZE_US = (str_c (\"SIZE_\", SIZE_US))) %&gt;% \n  ggplot(aes(x = reorder(SIZE_US,n), y = n))+\n  geom_bar(stat=\"identity\", color = \"blue\", fill = \"orange\")+\n  coord_flip()+\n  geom_text(aes(label = n), stat = \"identity\", hjust = -0.2)+ # Here also try to use vjust and take out coord_flip()\n  xlab(\"SHOE SIZE\")+\n  ylab(\"SALES (UNIT)\")+\n  ggtitle(\"DISTRIBUTION of SALES for PRODUCT ID 2190\")\n\n\n\n\n\n\n\n\nQUESTIONS - 1. Create a Bar Chart for Product ID (PRODUCT_ID) 2190 Sales by Gender of Different Shoes Sizes",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#relationship-between-shoe-size-and-price-in-different-gender-and-in-different-countries",
    "href": "r_eda.html#relationship-between-shoe-size-and-price-in-different-gender-and-in-different-countries",
    "title": "\nExploratory Data Analysis in R",
    "section": "13.1 Relationship between Shoe Size and Price in Different Gender and in Different Countries",
    "text": "13.1 Relationship between Shoe Size and Price in Different Gender and in Different Countries\n\nproduct %&gt;%\n  ggplot(\n    aes(x = as.numeric(SIZE_US), y = UNIT_PRICE, color = GENDER)\n  )+\n  geom_smooth(se = FALSE)+\n  xlab(\"SHOE SIZE (US)\")+\n  ylab(\"PRICE\")+\n  facet_wrap(~COUNTRY)+\n  theme(legend.title = element_blank())\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nQUESTIONS - 1. Do the Above Analyses for the Relationship between Shoe Size and Discount",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#footnotes",
    "href": "r_eda.html#footnotes",
    "title": "\nExploratory Data Analysis in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.techrepublic.com/article/why-data-scientist-is-the-most-promising-job-of-2019/↩︎",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "chap04_slide.html#introduction",
    "href": "chap04_slide.html#introduction",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#introduction-contd",
    "href": "chap04_slide.html#introduction-contd",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "Introduction (Cont’d)",
    "text": "Introduction (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#what-is-logistic-regression",
    "href": "chap04_slide.html#what-is-logistic-regression",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "What is LOGISTIC REGRESSION?",
    "text": "What is LOGISTIC REGRESSION?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#how-does-logistic-regression-learn",
    "href": "chap04_slide.html#how-does-logistic-regression-learn",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "How Does Logistic Regression Learn?",
    "text": "How Does Logistic Regression Learn?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#how-does-logistic-regression-learn-contd",
    "href": "chap04_slide.html#how-does-logistic-regression-learn-contd",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "How Does Logistic Regression Learn? (Cont’d)",
    "text": "How Does Logistic Regression Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#how-does-logistic-regression-learn-contd-1",
    "href": "chap04_slide.html#how-does-logistic-regression-learn-contd-1",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "How Does Logistic Regression Learn? (Cont’d)",
    "text": "How Does Logistic Regression Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#how-does-logistic-regression-learn-contd-2",
    "href": "chap04_slide.html#how-does-logistic-regression-learn-contd-2",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "How Does Logistic Regression Learn? (Cont’d)",
    "text": "How Does Logistic Regression Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#how-does-logistic-regression-learn-contd-3",
    "href": "chap04_slide.html#how-does-logistic-regression-learn-contd-3",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "How Does Logistic Regression Learn? (Cont’d)",
    "text": "How Does Logistic Regression Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#how-does-logistic-regression-learn-contd-4",
    "href": "chap04_slide.html#how-does-logistic-regression-learn-contd-4",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "How Does Logistic Regression Learn? (Cont’d)",
    "text": "How Does Logistic Regression Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#how-does-logistic-regression-learn-contd-5",
    "href": "chap04_slide.html#how-does-logistic-regression-learn-contd-5",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "How Does Logistic Regression Learn? (Cont’d)",
    "text": "How Does Logistic Regression Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_slide.html#questions-or-queries",
    "href": "chap04_slide.html#questions-or-queries",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "Questions or Queries",
    "text": "Questions or Queries\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nQueries",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap05_slide.html#introduction-contd",
    "href": "chap05_slide.html#introduction-contd",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Introduction (Cont’d)",
    "text": "Introduction (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#when-da-is-used",
    "href": "chap05_slide.html#when-da-is-used",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "When DA is Used?",
    "text": "When DA is Used?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#what-is-da",
    "href": "chap05_slide.html#what-is-da",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "What is DA?",
    "text": "What is DA?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#what-is-da-contd",
    "href": "chap05_slide.html#what-is-da-contd",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "What is DA (Cont’d)?",
    "text": "What is DA (Cont’d)?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#the-curse-of-dimensionality",
    "href": "chap05_slide.html#the-curse-of-dimensionality",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "The Curse of Dimensionality?",
    "text": "The Curse of Dimensionality?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#what-problem-does-curse-of-dimensionality-cause",
    "href": "chap05_slide.html#what-problem-does-curse-of-dimensionality-cause",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "What Problem Does Curse of Dimensionality Cause?",
    "text": "What Problem Does Curse of Dimensionality Cause?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#linear-discriminant-analysis-lda",
    "href": "chap05_slide.html#linear-discriminant-analysis-lda",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#linear-discriminant-analysis-lda-contd",
    "href": "chap05_slide.html#linear-discriminant-analysis-lda-contd",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Linear Discriminant Analysis (LDA) (Cont’d)",
    "text": "Linear Discriminant Analysis (LDA) (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#linear-discriminant-analysis-lda-contd-1",
    "href": "chap05_slide.html#linear-discriminant-analysis-lda-contd-1",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Linear Discriminant Analysis (LDA) (Cont’d)",
    "text": "Linear Discriminant Analysis (LDA) (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#linear-discriminant-analysis-lda-contd-2",
    "href": "chap05_slide.html#linear-discriminant-analysis-lda-contd-2",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Linear Discriminant Analysis (LDA) (Cont’d)",
    "text": "Linear Discriminant Analysis (LDA) (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#linear-discriminant-analysis-lda-contd-3",
    "href": "chap05_slide.html#linear-discriminant-analysis-lda-contd-3",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Linear Discriminant Analysis (LDA) (Cont’d)",
    "text": "Linear Discriminant Analysis (LDA) (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#quadratic-discriminant-analysis-qda",
    "href": "chap05_slide.html#quadratic-discriminant-analysis-qda",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#how-do-lda-and-qda-make-predictions",
    "href": "chap05_slide.html#how-do-lda-and-qda-make-predictions",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "How Do LDA and QDA Make Predictions?",
    "text": "How Do LDA and QDA Make Predictions?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#how-do-lda-and-qda-make-predictions-contd",
    "href": "chap05_slide.html#how-do-lda-and-qda-make-predictions-contd",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "How Do LDA and QDA Make Predictions (Cont’d)?",
    "text": "How Do LDA and QDA Make Predictions (Cont’d)?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#strength-and-weakness-of-lda-and-qda",
    "href": "chap05_slide.html#strength-and-weakness-of-lda-and-qda",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Strength and Weakness of LDA and QDA",
    "text": "Strength and Weakness of LDA and QDA",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#strength-and-weakness-of-lda-and-qda-contd",
    "href": "chap05_slide.html#strength-and-weakness-of-lda-and-qda-contd",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Strength and Weakness of LDA and QDA (Cont’d)",
    "text": "Strength and Weakness of LDA and QDA (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#questions-or-queries",
    "href": "chap05_slide.html#questions-or-queries",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Questions or Queries",
    "text": "Questions or Queries\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nQueries",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap05_slide.html#introduction",
    "href": "chap05_slide.html#introduction",
    "title": "\nChapter # 05  Discriminant Analysis (DA)",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 05 <br> Discriminant Analysis (DA)"
    ]
  },
  {
    "objectID": "chap07_regression.html#working-directory",
    "href": "chap07_regression.html#working-directory",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "",
    "text": "import os\n# Current Working Directory \nos.getcwd()\n\n\n# Files in Working Directory\nfor file in os.listdir():\n  print(file)",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#metadata-of-the-dataset",
    "href": "chap07_regression.html#metadata-of-the-dataset",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Metadata of the Dataset",
    "text": "Metadata of the Dataset\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4340 entries, 0 to 4339\nData columns (total 8 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   name           4340 non-null   object\n 1   year           4340 non-null   int64 \n 2   selling_price  4340 non-null   int64 \n 3   km_driven      4340 non-null   int64 \n 4   fuel           4340 non-null   object\n 5   seller_type    4340 non-null   object\n 6   transmission   4340 non-null   object\n 7   owner          4340 non-null   object\ndtypes: int64(3), object(5)\nmemory usage: 271.4+ KB\n\n\n\ndf.shape\n\n(4340, 8)\n\n\n\ndf.isna().sum()\n\nname             0\nyear             0\nselling_price    0\nkm_driven        0\nfuel             0\nseller_type      0\ntransmission     0\nowner            0\ndtype: int64",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#finding-outliers",
    "href": "chap07_regression.html#finding-outliers",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Finding Outliers",
    "text": "Finding Outliers\nAn outlier is a data point that differs significantly from other observations. They can cause the performance of the model to drop. Please look at Figure 1.\n\n\n\n\n\n\nFigure 1: How to Detect Outliers\n\n\n\n\nOutliers for Numeric Variables\n\nfor col in df.select_dtypes(exclude = 'object'):\n  sns.boxplot(data = df, x = col)\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutliers for Categorical Variables\n\nfor col in df.select_dtypes(include = 'object'):\n  sns.boxplot(data = df, x = col, y = 'selling_price')\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding the Outliers in the Dataset\nThis is based on the concept of quartiles, which divide a dataset into four equal parts. The IQR (InterQuartile Range rule) rule specifically focuses on the range of values within the middle 50% of the data and uses this range to identify potential outliers.\nWe have to find the minimum and maximum quantile values for each unique value in the categorical columns and filter the outlier samples which do not fit into the 25th and 75th percentile of our target column (Selling Price).\nOn the other hand, the outliers in numerical columns can be filtered by the 25th and 75th percentiles of the same column. We don’t need to filter out with respect to the target column.\n\noutliers_indexes = []\ntarget = 'selling_price'\n\nfor col in df.select_dtypes(include='object').columns:\n    for cat in df[col].unique():\n        df1 = df[df[col] == cat]\n        q1 = df1[target].quantile(0.25)\n        q3 = df1[target].quantile(0.75)\n        iqr = q3-q1\n        maximum = q3 + (1.5 * iqr)\n        minimum = q1 - (1.5 * iqr)\n        outlier_samples = df1[(df1[target] &lt; minimum) | (df1[target] &gt; maximum)]\n        outliers_indexes.extend(outlier_samples.index.tolist())\n        \n        \nfor col in df.select_dtypes(exclude='object').columns:\n    q1 = df[col].quantile(0.25)\n    q3 = df[col].quantile(0.75)\n    iqr = q3-q1\n    maximum = q3 + (1.5 * iqr)\n    minimum = q1 - (1.5 * iqr)\n    outlier_samples = df[(df[col] &lt; minimum) | (df[col] &gt; maximum)]\n    outliers_indexes.extend(outlier_samples.index.tolist())\n    \noutliers_indexes = list(set(outliers_indexes))\nprint('{} outliers were identified, whose indices are:\\n\\n{}'.format(len(outliers_indexes), outliers_indexes))\n\n579 outliers were identified, whose indices are:\n\n[4096, 4098, 2054, 12, 2062, 25, 27, 29, 30, 32, 33, 35, 36, 4132, 39, 40, 43, 44, 2097, 4147, 4153, 61, 4163, 69, 70, 4170, 2129, 88, 89, 2138, 4186, 2140, 4184, 94, 96, 101, 102, 103, 4200, 105, 2154, 4204, 4208, 2167, 2173, 2175, 2176, 2177, 4224, 2178, 4228, 2183, 4231, 137, 2187, 141, 2190, 2193, 149, 2198, 4253, 4254, 159, 4255, 163, 2212, 2216, 2223, 2224, 4273, 4274, 4275, 187, 2237, 2238, 191, 2239, 4286, 197, 2247, 2250, 204, 2256, 2257, 2258, 4304, 213, 4311, 4313, 218, 2266, 2269, 225, 227, 4325, 2278, 234, 2283, 236, 4331, 2295, 2299, 259, 263, 264, 2315, 269, 2318, 2323, 2333, 2337, 289, 291, 2342, 2348, 2350, 2351, 306, 2358, 2359, 315, 2364, 2366, 318, 320, 321, 324, 2373, 334, 337, 344, 345, 2394, 2401, 2402, 2404, 2406, 370, 372, 2425, 381, 2431, 391, 2440, 2441, 394, 399, 2447, 401, 2451, 410, 427, 431, 2482, 2495, 2499, 2503, 457, 461, 2511, 465, 2518, 2541, 2547, 502, 506, 2560, 2566, 2572, 525, 2574, 527, 2580, 534, 2582, 536, 537, 538, 539, 544, 2595, 548, 2597, 550, 551, 2598, 553, 554, 555, 2605, 563, 2612, 566, 2616, 572, 573, 574, 582, 585, 2635, 591, 592, 593, 2641, 601, 604, 2654, 2655, 610, 611, 612, 620, 623, 2672, 631, 2685, 2690, 644, 2696, 2699, 2704, 656, 660, 662, 665, 2715, 2718, 2737, 2738, 2739, 692, 2747, 2750, 711, 2760, 2766, 721, 2771, 2777, 729, 731, 2779, 2783, 738, 744, 747, 2796, 757, 2806, 2809, 2813, 770, 2819, 771, 2841, 2842, 796, 2847, 2848, 2850, 2851, 2855, 2858, 2865, 818, 820, 821, 828, 2878, 836, 838, 842, 849, 850, 851, 852, 2904, 2906, 2908, 2910, 2916, 2921, 878, 2929, 2942, 898, 899, 900, 2955, 908, 911, 2961, 2964, 917, 918, 919, 2972, 927, 930, 945, 957, 958, 959, 3007, 963, 968, 969, 975, 3026, 988, 997, 3046, 998, 1001, 3064, 3065, 3066, 1019, 1021, 1022, 1023, 1024, 3071, 1026, 3077, 1031, 1033, 1040, 1041, 1042, 3094, 1050, 1053, 1057, 1060, 1065, 1075, 1077, 1078, 1080, 1081, 1088, 3147, 1101, 1107, 1113, 1115, 1116, 3171, 1126, 3177, 1129, 1156, 3206, 3207, 3212, 1169, 3219, 1184, 3237, 3241, 1202, 3261, 1214, 1217, 3267, 3284, 3286, 3287, 1243, 1253, 3306, 3308, 3315, 3320, 3322, 1281, 1282, 1284, 3334, 1288, 1290, 1299, 3350, 1303, 3351, 1311, 1313, 3369, 3371, 3372, 1325, 1330, 3396, 1350, 3404, 1362, 3411, 1363, 3416, 3431, 1397, 3445, 3447, 3448, 1402, 1404, 3453, 1406, 3455, 3458, 1410, 3461, 1414, 1417, 3470, 1426, 3475, 1441, 1446, 3496, 1450, 1451, 1453, 3503, 1459, 1462, 1466, 3516, 3517, 3518, 1471, 1478, 1479, 1482, 3531, 3532, 3533, 3534, 1486, 3541, 1502, 3550, 1516, 3566, 3572, 3575, 3576, 1532, 3596, 3599, 1560, 3611, 3615, 1572, 3621, 3622, 3626, 1579, 3633, 1591, 1598, 3646, 3651, 3661, 3663, 1619, 1623, 1626, 3675, 3679, 1636, 3692, 1646, 1652, 3707, 1659, 1668, 1669, 3718, 3721, 1674, 3734, 1688, 3739, 1699, 1703, 1705, 1711, 3762, 3765, 1719, 1723, 1733, 3782, 3783, 3787, 3790, 3800, 3801, 1775, 1777, 1778, 1780, 3840, 3842, 1795, 3843, 3844, 1798, 1797, 3848, 3849, 3851, 3854, 3856, 3860, 3863, 3866, 1823, 3872, 3873, 3874, 3875, 1825, 1829, 1830, 1834, 1835, 1836, 3883, 3887, 3889, 3892, 1844, 1847, 3898, 3900, 3901, 1852, 3904, 3916, 3937, 3943, 3958, 1914, 1915, 3969, 1923, 1927, 3976, 3979, 3980, 3981, 1933, 3984, 1939, 1943, 3992, 3994, 3995, 4008, 1971, 1972, 4020, 1974, 1977, 1988, 4042, 4047, 1999, 4049, 4052, 4059, 2028, 2032, 2034, 4088, 2044]",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#bivariate-analysis",
    "href": "chap07_regression.html#bivariate-analysis",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\n\nsns.scatterplot(data = df, x = 'km_driven', y = 'selling_price')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = df, x = 'Age', y = 'selling_price')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = df, x = 'fuel', y = 'selling_price')\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#encoding-categorical-variables",
    "href": "chap07_regression.html#encoding-categorical-variables",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Encoding Categorical Variables",
    "text": "Encoding Categorical Variables\n\nLabel Encoding vs One Hot Encoding vs Ordinal Encoding\n\nLabel Encoding: Label encoding assigns a unique numerical label to each category in a categorical variable. It preserves the ordinal relationship between categories if present. For example, “Red” may be encoded as 1, “Green” as 2, and “Blue” as 3.\nOne-Hot Encoding: One-hot encoding converts each category in a categorical variable into a binary vector. It creates new binary columns for each category, representing the presence or absence of the category. Each category is mutually exclusive. For example, “Red” may be encoded as [1, 0, 0], “Green” as [0, 1, 0], and “Blue” as [0, 0, 1].\nOrdinal Encoding: Ordinal encoding is similar to label encoding but considers the order or rank of categories. It assigns unique numerical labels to each category, preserving the ordinal relationship between categories. For example, “Cold” may be encoded as 1, “Warm” as 2, and “Hot” as 3.\n\n\n\nDummy Variable Trap\nThe Dummy variable trap is a scenario where there are attributes that are highly correlated (Multicollinear) and one variable predicts the value of others. When we use one-hot encoding for handling the categorical data, then one dummy variable (attribute) can be predicted with the help of other dummy variables. Hence, one dummy variable is highly correlated with other dummy variables. Using all dummy variables for regression models leads to a dummy variable trap. So, the regression models should be designed to exclude one dummy variable.\n\ncat_col = df.select_dtypes(include = 'object').columns.values\ncat_col\ndf = pd.get_dummies(df, cat_col, drop_first = True)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4340 entries, 0 to 4339\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype\n---  ------                        --------------  -----\n 0   Age                           4340 non-null   int64\n 1   selling_price                 4340 non-null   int64\n 2   km_driven                     4340 non-null   int64\n 3   fuel_Diesel                   4340 non-null   bool \n 4   fuel_Electric                 4340 non-null   bool \n 5   fuel_LPG                      4340 non-null   bool \n 6   fuel_Petrol                   4340 non-null   bool \n 7   seller_type_Individual        4340 non-null   bool \n 8   seller_type_Trustmark Dealer  4340 non-null   bool \n 9   transmission_Manual           4340 non-null   bool \n 10  owner_Fourth & Above Owner    4340 non-null   bool \n 11  owner_Second Owner            4340 non-null   bool \n 12  owner_Test Drive Car          4340 non-null   bool \n 13  owner_Third Owner             4340 non-null   bool \ndtypes: bool(11), int64(3)\nmemory usage: 148.5 KB",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#correlation-analysis",
    "href": "chap07_regression.html#correlation-analysis",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\n\nplt.figure(figsize = (12,6))\ncmap = sns.diverging_palette(125, 28, s=100, l=65, sep=50, as_cmap=True)\nsns.heatmap(df.corr(), annot = True, cmap = cmap)\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#standardize-normalize-the-data",
    "href": "chap07_regression.html#standardize-normalize-the-data",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Standardize (Normalize) the Data",
    "text": "Standardize (Normalize) the Data\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nTrain the Model\n\nfrom sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train_scaled, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\npd.DataFrame(data = np.append(linear_reg.intercept_ , linear_reg.coef_), index = ['Intercept']+[col+\" Coef.\" for col in X.columns], columns=['Value']).sort_values('Value', ascending=False)\n\n\n\n\n\n\n\n\nValue\n\n\n\n\nIntercept\n510002.261027\n\n\nfuel_Diesel Coef.\n146649.985510\n\n\nseller_type_Trustmark Dealer Coef.\n24531.468884\n\n\nowner_Test Drive Car Coef.\n10297.650184\n\n\nfuel_LPG Coef.\n5419.717299\n\n\nowner_Fourth & Above Owner Coef.\n-1696.295901\n\n\nfuel_Petrol Coef.\n-7297.080696\n\n\nowner_Third Owner Coef.\n-8607.130414\n\n\nfuel_Electric Coef.\n-11304.751995\n\n\nowner_Second Owner Coef.\n-14611.147442\n\n\nseller_type_Individual Coef.\n-25802.779434\n\n\nkm_driven Coef.\n-49258.412716\n\n\nAge Coef.\n-149688.999744\n\n\ntransmission_Manual Coef.\n-273774.485929",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#check-for-homoscedasticity",
    "href": "chap07_regression.html#check-for-homoscedasticity",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Check for Homoscedasticity:",
    "text": "Check for Homoscedasticity:\nAssumption 03 - Residual Errors have a Mean Value of Zero\nAssumption 04 - Residual Errors have Constant Variance\nHomoscedasticity means that the residuals have equal or almost equal variance across the regression line. By plotting the error terms with predicted terms we can check that there should not be any pattern in the error terms.\nFigure 5 and Figure 6 shows these assumptions.\n\n\n\n\n\n\nFigure 5: This is What We Want to See\n\n\n\n\n\n\n\n\n\nFigure 6: This is What We Do Not Want to See\n\n\n\n\nregr = LinearRegression()\nregr.fit(X_train,y_train)\ny_pred = regr.predict(X_train)\nresiduals = y_train.values-y_pred\n\n\nassum_df = pd.DataFrame({'y_pred':y_pred, 'residuals': residuals})\nplt.figure(figsize = (15,8))\n\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.0, color_codes=True)\n\np = sns.scatterplot(data = assum_df, x = y_pred, y = residuals)\nplt.xlabel('y_pred/predicted values')\nplt.ylabel('Residuals')\n#plt.ylim(-10,20)\n#plt.xlim(0,25)\np = sns.lineplot(x = [0,26],y = [0,0],color='blue')\np = plt.title('Residuals vs fitted values plot for homoscedasticity check')\nplt.show()\n\n\n\n\n\n\n\n\n\nGoldfeld Quandt Test\nChecking heteroscedasticity : Using Goldfeld Quandt we test for heteroscedasticity.\nNull Hypothesis: Error terms are homoscedastic\nAlternative Hypothesis: Error terms are heteroscedastic.\n\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(residuals, X_train)\nlzip(name, test)\n\n[('F statistic', np.float64(1.0812414049241326)),\n ('p-value', np.float64(0.06483149744205788))]\n\n\nSince p value is more than 0.05 in Goldfeld Quandt Test, we can’t reject null hypothesis that error terms are homoscedastic.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#check-for-normality-of-error-termsresiduals",
    "href": "chap07_regression.html#check-for-normality-of-error-termsresiduals",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Check for Normality of Error Terms/Residuals",
    "text": "Check for Normality of Error Terms/Residuals\nAssumptions 01 - Linearity of the Data\n\np = sns.distplot(residuals,kde=True)\np = plt.title('Normality of error terms/residuals')\nplt.show()\n\n\n\n\n\n\n\n\nThe residual terms are pretty much normally distributed for the number of test points we took. Remember the central limit theorem which says that as the sample size increases the distribution tends to be normal. A skew is also visible from the plot. It’s very difficult to get perfect curves, distributions in real life data.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#no-autocorrelation-of-residuals",
    "href": "chap07_regression.html#no-autocorrelation-of-residuals",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "No Autocorrelation of Residuals",
    "text": "No Autocorrelation of Residuals\nAssumptions 02 - Predictors (x) are Independent & Observed with Negligible Error\nWhen the residuals are autocorrelated, it means that the current value is dependent of the previous (historic) values and that there is a definite unexplained pattern in the Y variable that shows up in the error terms. Though it is more evident in time series data.\nIn plain terms autocorrelation takes place when there’s a pattern in the rows of the data. This is usual in time series data as there is a pattern of time for eg. Week of the day effect which is a very famous pattern seen in stock markets where people tend to buy stocks more towards the beginning of weekends and tend to sell more on Mondays. There’s been great study about this phenomenon and it is still a matter of research as to what actual factors cause this trend.\n\nplt.figure(figsize=(10,5))\np = sns.lineplot(x = y_pred,y = residuals,marker='o',color='blue')\nplt.xlabel('y_pred/predicted values')\nplt.ylabel('Residuals')\n#plt.ylim(-10,10)\n#plt.xlim(0,26)\np = sns.lineplot(x = [0,26], y =[0,0],color='red')\np = plt.title('Residuals vs fitted values plot for autocorrelation check')\nplt.show()\n\n\n\n\n\n\n\n\n\n# autocorrelation\nsm.graphics.tsa.plot_acf(residuals, lags=40)\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#multicollinearity",
    "href": "chap07_regression.html#multicollinearity",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nIn regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \n\n# VIF dataframe \nvif_data = pd.DataFrame() \nvif_data[\"feature\"] = X.columns \n  \n# calculating VIF for each feature \nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) \n                          for i in range(len(X.columns))] \n  \nprint(vif_data)\n\n                         feature       VIF\n0                            Age  7.016427\n1                      km_driven  4.435080\n2                    fuel_Diesel  6.555573\n3                  fuel_Electric  1.003684\n4                       fuel_LPG  1.086386\n5                    fuel_Petrol  6.843066\n6         seller_type_Individual  4.810789\n7   seller_type_Trustmark Dealer  1.108127\n8            transmission_Manual  9.739791\n9     owner_Fourth & Above Owner  1.108982\n10            owner_Second Owner  1.694604\n11          owner_Test Drive Car  1.028466\n12             owner_Third Owner  1.291087",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#wrapper-methods",
    "href": "chap07_regression.html#wrapper-methods",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Wrapper Methods",
    "text": "Wrapper Methods\nIn wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset.\nIt follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The evaluation criterion is simply the performance measure that depends on the type of problem, e.g., in regression, evaluation criterion can be p-values, R-squared, Adjusted R-squared, similarly for classification the evaluation criterion can be accuracy, precision, recall, f1-score, etc. Finally, it selects the combination of features that gives the optimal results for the specified machine learning algorithm.\nMost commonly used techniques under wrapper methods are:\n\nForward selection\nBackward elimination\nBi-directional elimination(Stepwise Selection)",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#forward-selection",
    "href": "chap07_regression.html#forward-selection",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Forward Selection",
    "text": "Forward Selection\nIn forward selection, we start with a null model and then start fitting the model with each individual feature one at a time and select the feature with the minimum p-value. Now fit a model with two features by trying combinations of the earlier selected feature with all other remaining features. Again select the feature with the minimum p-value. Now fit a model with three features by trying combinations of two previously selected features with other remaining features. Repeat this process until we have a set of selected features with a p-value of individual features less than the significance level.\nIn short, the steps for the forward selection technique are as follows :\n\nChoose a significance level (e.g. SL = 0.05 with a 95% confidence).\nFit all possible simple regression models by considering one feature at a time. Total ’n’ models are possible. Select the feature with the lowest p-value.\nFit all possible models with one extra feature added to the previously selected feature(s).\nAgain, select the feature with a minimum p-value. if p_value &lt; significance level then go to Step 3, otherwise terminate the process.\n\n\nForward Selection Using User Defined Function\n\ndef forward_selection(data, target, significance_level=0.05):\n    initial_features = data.columns.tolist()\n    best_features = []\n    while (len(initial_features)&gt;0):\n        remaining_features = list(set(initial_features)-set(best_features))\n        new_pval = pd.Series(index=remaining_features)\n        for new_column in remaining_features:\n            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        min_p_value = new_pval.min()\n        if(min_p_value&lt;significance_level):\n            best_features.append(new_pval.idxmin())\n        else:\n            break\n    return best_features\n\n\nfor i in forward_selection(X,y):\n  print (i)\n\ntransmission_Manual\nAge\nfuel_Diesel\nkm_driven\nseller_type_Individual\nseller_type_Trustmark Dealer\nowner_Second Owner\n\n\n\n\nForward Selection Using Built-in Functions in Python\n\n## Installing mlxtend package \n#!pip install mlxtend\n\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LinearRegression\n# Sequential Forward Selection(sfs)\nsfs = SFS(LinearRegression(),\n          k_features=7,\n          forward=True,\n          floating=False,\n          scoring = 'r2',\n          cv = 0)\n\nSequentialFeatureSelector() function accepts the following major arguments :\n\nLinearRegression() is an estimator for the entire process. Similarly, it can be any classification based algorithm.\nk_features indicates the number of features to be selected. It can be any random value, but the optimal value can be found by analyzing and visualizing the scores for different numbers of features.\nforward and floating arguments for different flavors of wrapper methods, here, forward = True and floating = False are for forward selection technique.\nThe scoring argument specifies the evaluation criterion to be used. For regression problems, there is only r2 score in default implementation. Similarly for classification, it can be accuracy, precision, recall, f1-score, etc.\ncv argument is for k-fold cross-validation.\n\n\nsfs.fit(X,y)\nfor x in sfs.k_feature_names_:\n  print (x)\n\nAge\nkm_driven\nfuel_Diesel\nseller_type_Individual\nseller_type_Trustmark Dealer\ntransmission_Manual\nowner_Second Owner",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#backward-selection",
    "href": "chap07_regression.html#backward-selection",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Backward Selection",
    "text": "Backward Selection\nIn backward elimination, we start with the full model (including all the independent variables) and then remove the insignificant feature with the highest p-value(&gt; significance level). This process repeats again and again until we have the final set of significant features.\nIn short, the steps involved in backward elimination are as follows:\n\nChoose a significance level (e.g. SL = 0.05 with a 95% confidence).\nFit a full model including all the features.\nConsider the feature with the highest p-value. If the p-value &gt; significance level then go to Step 4, otherwise terminate the process.\nRemove the feature which is under consideration.\nFit a model without this feature. Repeat the entire process from Step 3.\n\n\nBackward Selection Using User Defined Functions\n\ndef backward_elimination(data, target,significance_level = 0.05):\n    features = data.columns.tolist()\n    while(len(features)&gt;0):\n        features_with_constant = sm.add_constant(data[features])\n        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]\n        max_p_value = p_values.max()\n        if(max_p_value &gt;= significance_level):\n            excluded_feature = p_values.idxmax()\n            features.remove(excluded_feature)\n        else:\n            break \n    return features\n\n\nfor x in backward_elimination(X,y):\n  print (x)\n\nAge\nkm_driven\nfuel_Diesel\nseller_type_Individual\nseller_type_Trustmark Dealer\ntransmission_Manual\nowner_Second Owner\n\n\n\n\nBackward Selection Using Built-in Functions in Python\n\n#Sequential backward selection(sbs)\nsbs = SFS(LinearRegression(),\n         k_features=7,\n         forward=False,\n         floating=False,\n         cv=0)\nsbs.fit(X, y)\n\nSequentialFeatureSelector(cv=0, estimator=LinearRegression(), forward=False,\n                          k_features=(7, 7), scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SequentialFeatureSelectoriFittedSequentialFeatureSelector(cv=0, estimator=LinearRegression(), forward=False,\n                          k_features=(7, 7), scoring='r2') estimator: LinearRegressionLinearRegression() LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\nfor x in sbs.k_feature_names_:\n  print (x)\n\nAge\nkm_driven\nfuel_Diesel\nseller_type_Individual\nseller_type_Trustmark Dealer\ntransmission_Manual\nowner_Second Owner",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  },
  {
    "objectID": "chap07_regression.html#step-wise-selection",
    "href": "chap07_regression.html#step-wise-selection",
    "title": "\nChapter # 07  Regression Analysis",
    "section": "Step-wise Selection",
    "text": "Step-wise Selection\nIt is similar to forward selection but the difference is while adding a new feature it also checks the significance of already added features and if it finds any of the already selected features insignificant then it simply removes that particular feature through backward elimination.\nHence, It is a combination of forward selection and backward elimination.\nIn short, the steps involved in bi-directional elimination (Stepwise Selection) are as follows:\n\nChoose a significance level to enter and exit the model (e.g. SL_in = 0.05 and SL_out = 0.05 with 95% confidence).\nPerform the next step of forward selection (newly added feature must have p-value &lt; SL_in to enter).\nPerform all steps of backward elimination (any previously added feature with p-value&gt;SL_out is ready to exit the model).\nRepeat steps 2 and 3 until we get a final optimal set of features.\n\n\nStepwise Selection Using User Defined Functions\n\ndef stepwise_selection(data, target,SL_in=0.05,SL_out = 0.05):\n    initial_features = data.columns.tolist()\n    best_features = []\n    while (len(initial_features)&gt;0):\n        remaining_features = list(set(initial_features)-set(best_features))\n        new_pval = pd.Series(index=remaining_features)\n        for new_column in remaining_features:\n            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        min_p_value = new_pval.min()\n        if(min_p_value&lt;SL_in):\n            best_features.append(new_pval.idxmin())\n            while(len(best_features)&gt;0):\n                best_features_with_constant = sm.add_constant(data[best_features])\n                p_values = sm.OLS(target, best_features_with_constant).fit().pvalues[1:]\n                max_p_value = p_values.max()\n                if(max_p_value &gt;= SL_out):\n                    excluded_feature = p_values.idxmax()\n                    best_features.remove(excluded_feature)\n                else:\n                    break \n        else:\n            break\n    return best_features\n\n\nfor x in stepwise_selection(X,y):\n  print (x)\n\ntransmission_Manual\nAge\nfuel_Diesel\nkm_driven\nseller_type_Individual\nseller_type_Trustmark Dealer\nowner_Second Owner\n\n\n\n\nStepwise Selection Using Built-in Functions in Python\n\n# Sequential Forward Floating Selection(sffs)\nsffs = SFS(LinearRegression(),\n         k_features=(3,7),\n         forward=True,\n         floating=True,\n         cv=0)\nsffs.fit(X, y)\n\nSequentialFeatureSelector(cv=0, estimator=LinearRegression(), floating=True,\n                          k_features=(3, 7), scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SequentialFeatureSelectoriFittedSequentialFeatureSelector(cv=0, estimator=LinearRegression(), floating=True,\n                          k_features=(3, 7), scoring='r2') estimator: LinearRegressionLinearRegression() LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\nfor x in sffs.k_feature_names_:\n  print(x)\n\nAge\nkm_driven\nfuel_Diesel\nseller_type_Individual\nseller_type_Trustmark Dealer\ntransmission_Manual\nowner_Second Owner\n\n\n\n\nVisualization of Feature Selection\n\nsfs1 = SFS(LinearRegression(),\n         k_features=(3,11),\n         forward=True,\n         floating=False,\n         cv=0)\nsfs1.fit(X, y)\n\nSequentialFeatureSelector(cv=0, estimator=LinearRegression(),\n                          k_features=(3, 11), scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SequentialFeatureSelectoriFittedSequentialFeatureSelector(cv=0, estimator=LinearRegression(),\n                          k_features=(3, 11), scoring='r2') estimator: LinearRegressionLinearRegression() LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\nfig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\nplt.title('Sequential Forward Selection')\nplt.show()\n\n\n\n\n\n\n\n\nHere, on the y-axis, the performance label indicates the R-squared values for the different number of features.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 07 <br> Regression Analysis"
    ]
  }
]