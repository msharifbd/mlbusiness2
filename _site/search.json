[
  {
    "objectID": "syllabus.html#books",
    "href": "syllabus.html#books",
    "title": "Syllabus",
    "section": "Books",
    "text": "Books"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "syllabus.html#classroom-etiquette",
    "href": "syllabus.html#classroom-etiquette",
    "title": "Syllabus",
    "section": "Classroom Etiquette",
    "text": "Classroom Etiquette"
  },
  {
    "objectID": "syllabus.html#modules",
    "href": "syllabus.html#modules",
    "title": "Syllabus",
    "section": "Modules",
    "text": "Modules\n\n01. Basics of R, Python, Tableau, and Power BI\n\n\n02. Working with Data\n\n\n03. Data Visualization\n\n\n04. Working with Databases\n\n\n05. Regression\n\n\n06. Machine Learning\n\n\n07. Textual Analysis"
  },
  {
    "objectID": "recent-developments.html",
    "href": "recent-developments.html",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow AI/ML will Transform Business?\n\n\n\n\n\nAI’s vast capabilities are already transforming business and changing the nature of many jobs\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "python_eda.html",
    "href": "python_eda.html",
    "title": "\nExploratory Data Analysis in Python",
    "section": "",
    "text": "The objective of this document is to introduce the necessary functions from pandas library in Python for data manipulation and matplotlib and seaborn libraries for data visualization. There are basically six functions - select(), filter(), mutate(), arrange(), group_by(), and summarize() - from dplyr package of tidyverse ecosystem that are very much necessary for data manipulation. These six functions can be used for 80% of data manipulation problems. In this document, we will compare the above six functions from dplyr with the equivalent pandas functions. Additionally, this handout also compares some other Python packages, particularly plotnine library that can be used to apply ggplot in Python.",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#changing-the-types-of-the-variables",
    "href": "python_eda.html#changing-the-types-of-the-variables",
    "title": "\nExploratory Data Analysis in Python",
    "section": "6.1 Changing the Types of the Variables",
    "text": "6.1 Changing the Types of the Variables\n      There are several types of data in Python as it is in R. Table Table 1 lists the data types in python.\n\n\n\n\nTable 1: Types of Data in Python\n\n\n\n\n\n\nPython Data Type\nData Nature\n\n\n\n\nfloat64\nReal Numbers\n\n\ncategory\ncateogries\n\n\ndatetime64\nDate Times\n\n\nint64\nIntegers\n\n\nbool\nTrue or False\n\n\nstring\nText\n\n\n\n\n\n\n\n\n\n\n\n# Changing the DATE variable from object to date\nproduct['DATE'] = pd.to_datetime(product['DATE']) \nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  int64         \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  int64         \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  int64         \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(5), object(4)\nmemory usage: 1.6+ MB\n\n\n\n# converting integer to object\nproduct.INVOICENO = product.INVOICENO.astype(str) \nproduct[['MONTH', 'PRODUCTID']] = product[['MONTH', 'PRODUCTID']].astype(str) \nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#select-equivalent-in-python---accessing-columns",
    "href": "python_eda.html#select-equivalent-in-python---accessing-columns",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.1 select () Equivalent in Python - Accessing Columns",
    "text": "7.1 select () Equivalent in Python - Accessing Columns\n\nprod2 = product[['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE']]\nprod2.head()\n\n   YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0  2014      159.0       0.0        159\n1  2014      159.2       0.2        199\n2  2014      119.2       0.2        149\n3  2014      159.0       0.0        159\n4  2014      159.0       0.0        159\n\n\n\nproduct.loc[:,['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE']]\n\n       YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0      2014      159.0       0.0        159\n1      2014      159.2       0.2        199\n2      2014      119.2       0.2        149\n3      2014      159.0       0.0        159\n4      2014      159.0       0.0        159\n...     ...        ...       ...        ...\n14962  2016      139.0       0.0        139\n14963  2016      149.0       0.0        149\n14964  2016      125.3       0.3        179\n14965  2016      199.0       0.0        199\n14966  2016      125.1       0.1        139\n\n[14967 rows x 4 columns]\n\nproduct.loc[0:5,['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE']]\n\n   YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0  2014      159.0       0.0        159\n1  2014      159.2       0.2        199\n2  2014      119.2       0.2        149\n3  2014      159.0       0.0        159\n4  2014      159.0       0.0        159\n5  2014      159.0       0.0        159\n\n\n\nproduct.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n\n       YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0      2014      159.0       0.0        159\n1      2014      159.2       0.2        199\n2      2014      119.2       0.2        149\n3      2014      159.0       0.0        159\n4      2014      159.0       0.0        159\n...     ...        ...       ...        ...\n14962  2016      139.0       0.0        139\n14963  2016      149.0       0.0        149\n14964  2016      125.3       0.3        179\n14965  2016      199.0       0.0        199\n14966  2016      125.1       0.1        139\n\n[14967 rows x 4 columns]\n\n\n\n# Regular Expression (Regex)\nproduct.filter(regex = \"PRICE$\") # Ends with Price \n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\nproduct.filter(regex = \"^SIZE\")  # Starts with SIZE\n\n       SIZE_US SIZE_EUROPE  SIZE_UK\n0         11.0          44     10.5\n1         11.5       44-45     11.0\n2          9.5       42-43      9.0\n3          9.5          40      7.5\n4          9.0       39-40      7.0\n...        ...         ...      ...\n14962      9.5       42-43      9.0\n14963     12.0       42-43     10.0\n14964     10.5       43-44     10.0\n14965      9.5          40      7.5\n14966      6.5          37      4.5\n\n[14967 rows x 3 columns]\n\nproduct.filter(regex = \"PRICE\")  # Contains the word Price \n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\n\n\nproduct.select_dtypes('object')\n\n      INVOICENO         COUNTRY PRODUCTID  SHOP  GENDER SIZE_EUROPE MONTH\n0         52389  United Kingdom      2152   UK2    Male          44     1\n1         52390   United States      2230  US15    Male       44-45     1\n2         52391          Canada      2160  CAN7    Male       42-43     1\n3         52392   United States      2234   US6  Female          40     1\n4         52393  United Kingdom      2222   UK4  Female       39-40     1\n...         ...             ...       ...   ...     ...         ...   ...\n14962     65773  United Kingdom      2154   UK2    Male       42-43    12\n14963     65774   United States      2181  US12  Female       42-43    12\n14964     65775          Canada      2203  CAN6    Male       43-44    12\n14965     65776         Germany      2231  GER1  Female          40    12\n14966     65777         Germany      2156  GER1  Female          37    12\n\n[14967 rows x 7 columns]\n\nproduct.select_dtypes('int')\n\n       UNITPRICE  YEAR\n0            159  2014\n1            199  2014\n2            149  2014\n3            159  2014\n4            159  2014\n...          ...   ...\n14962        139  2016\n14963        149  2016\n14964        179  2016\n14965        199  2016\n14966        139  2016\n\n[14967 rows x 2 columns]\n\n\n\nproduct.loc[:,product.columns.str.startswith('SIZE')]\n\n       SIZE_US SIZE_EUROPE  SIZE_UK\n0         11.0          44     10.5\n1         11.5       44-45     11.0\n2          9.5       42-43      9.0\n3          9.5          40      7.5\n4          9.0       39-40      7.0\n...        ...         ...      ...\n14962      9.5       42-43      9.0\n14963     12.0       42-43     10.0\n14964     10.5       43-44     10.0\n14965      9.5          40      7.5\n14966      6.5          37      4.5\n\n[14967 rows x 3 columns]\n\nproduct.loc[:,product.columns.str.contains('PRICE')]\n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\nproduct.loc[:,product.columns.str.endswith('PRICE')]\n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\n\n\n# Dropping some columns \nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB\n\nproduct.drop(columns = ['SIZE_EUROPE', 'SIZE_UK'], axis = 1)\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n0         52389 2014-01-01  United Kingdom  ...  2014     1     159.0\n1         52390 2014-01-01   United States  ...  2014     1     159.2\n2         52391 2014-01-01          Canada  ...  2014     1     119.2\n3         52392 2014-01-01   United States  ...  2014     1     159.0\n4         52393 2014-01-01  United Kingdom  ...  2014     1     159.0\n...         ...        ...             ...  ...   ...   ...       ...\n14962     65773 2016-12-31  United Kingdom  ...  2016    12     139.0\n14963     65774 2016-12-31   United States  ...  2016    12     149.0\n14964     65775 2016-12-31          Canada  ...  2016    12     125.3\n14965     65776 2016-12-31         Germany  ...  2016    12     199.0\n14966     65777 2016-12-31         Germany  ...  2016    12     125.1\n\n[14967 rows x 12 columns]\n\nproduct.drop(columns = ['SIZE_EUROPE', 'SIZE_UK'], axis = 1) \\\n    .pipe(lambda x: x.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 12 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   INVOICENO  14967 non-null  object        \n 1   DATE       14967 non-null  datetime64[ns]\n 2   COUNTRY    14967 non-null  object        \n 3   PRODUCTID  14967 non-null  object        \n 4   SHOP       14967 non-null  object        \n 5   GENDER     14967 non-null  object        \n 6   SIZE_US    14967 non-null  float64       \n 7   UNITPRICE  14967 non-null  int64         \n 8   DISCOUNT   14967 non-null  float64       \n 9   YEAR       14967 non-null  int64         \n 10  MONTH      14967 non-null  object        \n 11  SALEPRICE  14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(3), int64(2), object(6)\nmemory usage: 1.4+ MB\n\n\n\n7.1.1 Rearranging Columns\n\n# Sorting Alphabetically\nproduct.reindex(sorted(product.columns), axis = 1)\n\n              COUNTRY       DATE  DISCOUNT  ... SIZE_US UNITPRICE  YEAR\n0      United Kingdom 2014-01-01       0.0  ...    11.0       159  2014\n1       United States 2014-01-01       0.2  ...    11.5       199  2014\n2              Canada 2014-01-01       0.2  ...     9.5       149  2014\n3       United States 2014-01-01       0.0  ...     9.5       159  2014\n4      United Kingdom 2014-01-01       0.0  ...     9.0       159  2014\n...               ...        ...       ...  ...     ...       ...   ...\n14962  United Kingdom 2016-12-31       0.0  ...     9.5       139  2016\n14963   United States 2016-12-31       0.0  ...    12.0       149  2016\n14964          Canada 2016-12-31       0.3  ...    10.5       179  2016\n14965         Germany 2016-12-31       0.0  ...     9.5       199  2016\n14966         Germany 2016-12-31       0.1  ...     6.5       139  2016\n\n[14967 rows x 14 columns]\n\n# Sorting As You Want (ASY)\nproduct.columns.to_list()\n\n['INVOICENO', 'DATE', 'COUNTRY', 'PRODUCTID', 'SHOP', 'GENDER', 'SIZE_US', 'SIZE_EUROPE', 'SIZE_UK', 'UNITPRICE', 'DISCOUNT', 'YEAR', 'MONTH', 'SALEPRICE']\n\ncol_first = ['YEAR','MONTH']\ncol_rest = product.columns.difference(col_first, sort=False).to_list()\nproduct2 = product[col_first + col_rest]\nproduct2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   YEAR         14967 non-null  int64         \n 1   MONTH        14967 non-null  object        \n 2   INVOICENO    14967 non-null  object        \n 3   DATE         14967 non-null  datetime64[ns]\n 4   COUNTRY      14967 non-null  object        \n 5   PRODUCTID    14967 non-null  object        \n 6   SHOP         14967 non-null  object        \n 7   GENDER       14967 non-null  object        \n 8   SIZE_US      14967 non-null  float64       \n 9   SIZE_EUROPE  14967 non-null  object        \n 10  SIZE_UK      14967 non-null  float64       \n 11  UNITPRICE    14967 non-null  int64         \n 12  DISCOUNT     14967 non-null  float64       \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#filter-equivalent-in-python---accessing-rows",
    "href": "python_eda.html#filter-equivalent-in-python---accessing-rows",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.2 filter () Equivalent in Python - Accessing Rows",
    "text": "7.2 filter () Equivalent in Python - Accessing Rows\n\nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB\n\nproduct.COUNTRY.value_counts()\n\nCOUNTRY\nUnited States     5886\nGermany           4392\nCanada            2952\nUnited Kingdom    1737\nName: count, dtype: int64\n\nproduct['YEAR'].unique()\n\narray([2014, 2015, 2016])\n\nproduct['YEAR'].value_counts()\n\nYEAR\n2016    7366\n2015    4848\n2014    2753\nName: count, dtype: int64\n\n\n\nproduct.query('COUNTRY == \"United States\"')\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n8         52397 2014-01-02  United States  ...  2014     1     139.0\n10        52399 2014-01-02  United States  ...  2014     1     129.0\n...         ...        ...            ...  ...   ...   ...       ...\n14956     65767 2016-12-31  United States  ...  2016    12     139.0\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n\n[5886 rows x 14 columns]\n\nproduct.query('COUNTRY == \"United States\" | COUNTRY == \"Canada\"')\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n2         52391 2014-01-01         Canada  ...  2014     1     119.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n7         52396 2014-01-02         Canada  ...  2014     1     169.0\n...         ...        ...            ...  ...   ...   ...       ...\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n14964     65775 2016-12-31         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\nproduct.query(\"COUNTRY in ['United States', 'Canada']\")\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n2         52391 2014-01-01         Canada  ...  2014     1     119.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n7         52396 2014-01-02         Canada  ...  2014     1     169.0\n...         ...        ...            ...  ...   ...   ...       ...\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n14964     65775 2016-12-31         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\nproduct.query(\"COUNTRY == 'United States' & YEAR == 2016\")\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n7610      59206 2016-01-02  United States  ...  2016     1     132.3\n7613      59209 2016-01-02  United States  ...  2016     1     127.2\n7617      59213 2016-01-02  United States  ...  2016     1     125.3\n7618      59214 2016-01-02  United States  ...  2016     1     151.2\n7619      59214 2016-01-02  United States  ...  2016     1     151.2\n...         ...        ...            ...  ...   ...   ...       ...\n14956     65767 2016-12-31  United States  ...  2016    12     139.0\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n\n[2935 rows x 14 columns]\n\nproduct.query(\"COUNTRY == 'United States' & YEAR in [2015,2016]\")\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n2753      54725 2015-01-01  United States  ...  2015     1     179.0\n2754      54726 2015-01-01  United States  ...  2015     1     169.0\n2755      54727 2015-01-01  United States  ...  2015     1     116.1\n2761      54733 2015-01-02  United States  ...  2015     1     179.0\n2766      54738 2015-01-02  United States  ...  2015     1     199.0\n...         ...        ...            ...  ...   ...   ...       ...\n14956     65767 2016-12-31  United States  ...  2016    12     139.0\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n\n[4859 rows x 14 columns]\n\n\n\nproduct.loc[(product['COUNTRY'] == \"United States\")]\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n8         52397 2014-01-02  United States  ...  2014     1     139.0\n10        52399 2014-01-02  United States  ...  2014     1     129.0\n...         ...        ...            ...  ...   ...   ...       ...\n14956     65767 2016-12-31  United States  ...  2016    12     139.0\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n\n[5886 rows x 14 columns]\n\nproduct.loc[product['COUNTRY'].isin([\"United States\", \"Canada\"])]\n\n      INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390 2014-01-01  United States  ...  2014     1     159.2\n2         52391 2014-01-01         Canada  ...  2014     1     119.2\n3         52392 2014-01-01  United States  ...  2014     1     159.0\n5         52394 2014-01-01  United States  ...  2014     1     159.0\n7         52396 2014-01-02         Canada  ...  2014     1     169.0\n...         ...        ...            ...  ...   ...   ...       ...\n14959     65770 2016-12-31  United States  ...  2016    12     119.2\n14960     65771 2016-12-31  United States  ...  2016    12     189.0\n14961     65772 2016-12-31  United States  ...  2016    12     129.0\n14963     65774 2016-12-31  United States  ...  2016    12     149.0\n14964     65775 2016-12-31         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\nproduct.loc[product['COUNTRY'] \\\n  .isin([\"United States\", \"Canada\"]) & (product['YEAR'] == 2014)]\n\n     INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1        52390 2014-01-01  United States  ...  2014     1     159.2\n2        52391 2014-01-01         Canada  ...  2014     1     119.2\n3        52392 2014-01-01  United States  ...  2014     1     159.0\n5        52394 2014-01-01  United States  ...  2014     1     159.0\n7        52396 2014-01-02         Canada  ...  2014     1     169.0\n...        ...        ...            ...  ...   ...   ...       ...\n2739     54713 2014-12-30  United States  ...  2014    12     189.0\n2745     54718 2014-12-31         Canada  ...  2014    12     151.2\n2746     54719 2014-12-31  United States  ...  2014    12     199.0\n2748     54721 2014-12-31         Canada  ...  2014    12      74.5\n2749     54722 2014-12-31  United States  ...  2014    12     118.3\n\n[1649 rows x 14 columns]\n\nproduct.loc[(product['COUNTRY'] == \"United States\") & (product[\"YEAR\"] == 2014)]\n\n     INVOICENO       DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1        52390 2014-01-01  United States  ...  2014     1     159.2\n3        52392 2014-01-01  United States  ...  2014     1     159.0\n5        52394 2014-01-01  United States  ...  2014     1     159.0\n8        52397 2014-01-02  United States  ...  2014     1     139.0\n10       52399 2014-01-02  United States  ...  2014     1     129.0\n...        ...        ...            ...  ...   ...   ...       ...\n2731     54705 2014-12-29  United States  ...  2014    12     179.0\n2734     54708 2014-12-30  United States  ...  2014    12     159.0\n2739     54713 2014-12-30  United States  ...  2014    12     189.0\n2746     54719 2014-12-31  United States  ...  2014    12     199.0\n2749     54722 2014-12-31  United States  ...  2014    12     118.3\n\n[1027 rows x 14 columns]\n\n\n\n7.2.1 loc[] Function can be used both for Slicing (selecting Rows) and Selecting Columns\n\nproduct.loc[\n  product['COUNTRY'] == 'United States',\n  ['COUNTRY', \"UNITPRICE\", \"SALEPRICE\"]]\n\n             COUNTRY  UNITPRICE  SALEPRICE\n1      United States        199      159.2\n3      United States        159      159.0\n5      United States        159      159.0\n8      United States        139      139.0\n10     United States        129      129.0\n...              ...        ...        ...\n14956  United States        139      139.0\n14959  United States        149      119.2\n14960  United States        189      189.0\n14961  United States        129      129.0\n14963  United States        149      149.0\n\n[5886 rows x 3 columns]",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#arrange-equivalent-in-python---sorting-or-arranging-rows",
    "href": "python_eda.html#arrange-equivalent-in-python---sorting-or-arranging-rows",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.3 arrange () Equivalent in Python - Sorting or Arranging Rows",
    "text": "7.3 arrange () Equivalent in Python - Sorting or Arranging Rows\n\nproduct.sort_values(by = ['MONTH'])\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n0         52389 2014-01-01  United Kingdom  ...  2014     1     159.0\n2862      54831 2015-01-13         Germany  ...  2015     1     169.0\n2863      54832 2015-01-13         Germany  ...  2015     1     125.1\n2864      54833 2015-01-13   United States  ...  2015     1     135.2\n2865      54834 2015-01-13         Germany  ...  2015     1     149.0\n...         ...        ...             ...  ...   ...   ...       ...\n13225     64202 2016-09-28          Canada  ...  2016     9     189.0\n13224     64201 2016-09-28         Germany  ...  2016     9     149.0\n13223     64200 2016-09-28          Canada  ...  2016     9     199.0\n13235     64210 2016-09-29   United States  ...  2016     9     135.2\n12749     63775 2016-09-08         Germany  ...  2016     9     139.0\n\n[14967 rows x 14 columns]\n\nproduct.sort_values(by = ['MONTH'], ascending = False)\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n13162     64145 2016-09-26  United Kingdom  ...  2016     9      90.3\n5946      57674 2015-09-12   United States  ...  2015     9     159.0\n5944      57672 2015-09-12          Canada  ...  2015     9     179.0\n5943      57671 2015-09-12         Germany  ...  2015     9     159.0\n5942      57670 2015-09-12          Canada  ...  2015     9     179.0\n...         ...        ...             ...  ...   ...   ...       ...\n3037      54995 2015-01-29   United States  ...  2015     1     103.2\n3038      54996 2015-01-30          Canada  ...  2015     1     161.1\n3039      54997 2015-01-30          Canada  ...  2015     1      84.5\n3040      54997 2015-01-30          Canada  ...  2015     1     169.0\n0         52389 2014-01-01  United Kingdom  ...  2014     1     159.0\n\n[14967 rows x 14 columns]\n\nproduct.sort_values(by = ['MONTH', 'SALEPRICE'])\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n33        52414 2014-01-05         Germany  ...  2014     1      64.5\n177       52533 2014-01-25          Canada  ...  2014     1      64.5\n185       52539 2014-01-26  United Kingdom  ...  2014     1      64.5\n194       52548 2014-01-27  United Kingdom  ...  2014     1      64.5\n2762      54734 2015-01-02         Germany  ...  2015     1      64.5\n...         ...        ...             ...  ...   ...   ...       ...\n13245     64219 2016-09-29  United Kingdom  ...  2016     9     199.0\n13246     64220 2016-09-29   United States  ...  2016     9     199.0\n13248     64222 2016-09-29   United States  ...  2016     9     199.0\n13251     64224 2016-09-29         Germany  ...  2016     9     199.0\n13272     64244 2016-09-30   United States  ...  2016     9     199.0\n\n[14967 rows x 14 columns]",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#rename-equivalent-in-python---renaming-column-names",
    "href": "python_eda.html#rename-equivalent-in-python---renaming-column-names",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.4 rename () Equivalent in Python - Renaming Column Names",
    "text": "7.4 rename () Equivalent in Python - Renaming Column Names\n      We already did some renaming of the columns using str. function. Here we use rename () function to change the name of the columns.\n\nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB\n\nproduct.rename( columns = \n  {'SIZE_(EUROPE)': 'SIZE_EUROPE',\n   'SIZE_(UK)' : 'SIZE_UK'}) \\\n   .pipe(lambda x: x.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(2), object(7)\nmemory usage: 1.6+ MB",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#mutate-equivalent-in-python---creating-new-columns-variables",
    "href": "python_eda.html#mutate-equivalent-in-python---creating-new-columns-variables",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.5 mutate () Equivalent in Python - Creating New Columns (Variables)",
    "text": "7.5 mutate () Equivalent in Python - Creating New Columns (Variables)\n\nproduct['NECOLUMN'] = 5\nproduct.head()\n\n  INVOICENO       DATE         COUNTRY  ... MONTH SALEPRICE NECOLUMN\n0     52389 2014-01-01  United Kingdom  ...     1     159.0        5\n1     52390 2014-01-01   United States  ...     1     159.2        5\n2     52391 2014-01-01          Canada  ...     1     119.2        5\n3     52392 2014-01-01   United States  ...     1     159.0        5\n4     52393 2014-01-01  United Kingdom  ...     1     159.0        5\n\n[5 rows x 15 columns]\n\nproduct.drop(columns = ['NECOLUMN'], axis = 1, inplace = True) \n\n\nproduct['SALEPRICE2'] = product['UNITPRICE']*(1-product['DISCOUNT'])\nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \n 14  SALEPRICE2   14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(5), int64(2), object(7)\nmemory usage: 1.7+ MB\n\n\n\n# Using the assign () function \nproduct[['PRODUCTID', 'UNITPRICE', 'DISCOUNT']] \\\n .assign(SALEPRICE3 = lambda x: x.UNITPRICE*(1-x.DISCOUNT)) \\\n .head(5)\n\n  PRODUCTID  UNITPRICE  DISCOUNT  SALEPRICE3\n0      2152        159       0.0       159.0\n1      2230        199       0.2       159.2\n2      2160        149       0.2       119.2\n3      2234        159       0.0       159.0\n4      2222        159       0.0       159.0",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#group_by-and-summarize-equivalent-in-python---summarizing-data",
    "href": "python_eda.html#group_by-and-summarize-equivalent-in-python---summarizing-data",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.6 group_by () and summarize () Equivalent in Python - Summarizing Data",
    "text": "7.6 group_by () and summarize () Equivalent in Python - Summarizing Data\n      Figure Figure 1 presents the split-apply-combine principle in group_by () and summarize () functions.\n\n\n\n\n\n\n\n\nFigure 1: Split Apply and Combine Principle\n\n\n\n\n\n\nproduct.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  object        \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  object        \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  object        \n 13  SALEPRICE    14967 non-null  float64       \n 14  SALEPRICE2   14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(5), int64(2), object(7)\nmemory usage: 1.7+ MB\n\nproduct.groupby(['COUNTRY']) ['UNITPRICE'].mean()\n\nCOUNTRY\nCanada            164.691057\nGermany           164.163934\nUnited Kingdom    165.614853\nUnited States     163.490316\nName: UNITPRICE, dtype: float64\n\nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']].mean()\n\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(np.mean)\n\n&lt;string&gt;:3: FutureWarning: The provided callable &lt;function mean at 0x000001B69A4936A0&gt; is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(\"mean\")\n\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\n       \nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(AVG_UNITPRICE = (\"UNITPRICE\", \"mean\"), \n            AVG_LISTPRICE = (\"SALEPRICE\", \"mean\"))\n\n                AVG_UNITPRICE  AVG_LISTPRICE\nCOUNTRY                                     \nCanada             164.691057     144.228963\nGermany            164.163934     143.574658\nUnited Kingdom     165.614853     145.505872\nUnited States      163.490316     143.727421\n\n            \n            \nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(AVG_UNITPRICE = (\"UNITPRICE\", \"mean\"), \n            AVG_LISTPRICE = (\"SALEPRICE\", \"mean\"),\n            TOTALN = (\"SALEPRICE\", \"size\"), # size function for n\n            TOTALOBS = (\"SALEPRICE\", \"count\") # count function for n\n            )\n\n                AVG_UNITPRICE  AVG_LISTPRICE  TOTALN  TOTALOBS\nCOUNTRY                                                       \nCanada             164.691057     144.228963    2952      2952\nGermany            164.163934     143.574658    4392      4392\nUnited Kingdom     165.614853     145.505872    1737      1737\nUnited States      163.490316     143.727421    5886      5886\n\n# Combining Several Pandas Functions together           \nproduct.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n       .agg(AVG_UNITPRICE = (\"UNITPRICE\", \"mean\"), \n            AVG_LISTPRICE = (\"SALEPRICE\", \"mean\"),\n            TOTALN = (\"SALEPRICE\", \"size\"), # size function for n\n            TOTALOBS = (\"SALEPRICE\", \"count\") # count function for n\n            ) \\\n       .sort_values(by = ['TOTALOBS'], ascending = False) \\\n       .reset_index() \\\n       .query ('COUNTRY == \"United States\"')\n\n         COUNTRY  AVG_UNITPRICE  AVG_LISTPRICE  TOTALN  TOTALOBS\n0  United States     163.490316     143.727421    5886      5886",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#summary-statistics-in-python",
    "href": "python_eda.html#summary-statistics-in-python",
    "title": "\nExploratory Data Analysis in Python",
    "section": "7.7 Summary Statistics in Python",
    "text": "7.7 Summary Statistics in Python\n\n# Summary Statistics in Python \nproduct.GENDER.value_counts()\n\nGENDER\nMale      8919\nFemale    6048\nName: count, dtype: int64\n\n# Encoding a Categorical Variables \nproduct['SEX'] = product['GENDER'].map({\n  'Male':1,\n  'Female':0\n})\n\n\n# Defining a Function \ndef percentile(n):\n    def percentile_(x):\n        return x.quantile(n)\n    percentile_.__name__ = 'percentile_{:02.0f}'.format(n*100)\n    return percentile_\n  \nproduct [['SALEPRICE', 'UNITPRICE', 'SEX']] \\\n     .agg([\"count\",\"mean\", \"std\", \"median\", percentile(0.25), percentile(0.75)]) \\\n     .transpose () \\\n     .reset_index() \\\n     .rename(columns = {'index': \"Variables\",\n                        'percentile_25': 'P25',\n                        'percentile_75': 'P75',\n                        'count': 'n',\n                        'mean' : 'Mean',\n                        'median' : 'Median',\n                        'std': 'Std'\n                       }) \\\n      .round(3) # rounding to two decimal places \n\n   Variables        n     Mean     Std  Median    P25    P75\n0  SALEPRICE  14967.0  143.988  35.181   149.0  125.1  169.0\n1  UNITPRICE  14967.0  164.171  22.941   159.0  149.0  179.0\n2        SEX  14967.0    0.596   0.491     1.0    0.0    1.0",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#bar-chart",
    "href": "python_eda.html#bar-chart",
    "title": "\nExploratory Data Analysis in Python",
    "section": "9.1 Bar Chart",
    "text": "9.1 Bar Chart\n\nbar_r = product.filter (['COUNTRY']) \\\n       .value_counts() \\\n       .reset_index() \\\n       .rename (columns = {'count':'n'}) \\\n       .sort_values (by = ['n'])\n\nplot = (ggplot(data = bar_r, \n  mapping = aes(x = 'COUNTRY', y = 'n'))+\n  geom_bar (fill = \"pink\", stat = \"identity\")+\n  labs (x = 'Country',\n  y = 'Number of Observations'\n  #,title = 'Total Observations of Countries'\n  )\n)\nplot.draw(True)\n\n\n\n\nTotal Observations of Countries",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#line-chart",
    "href": "python_eda.html#line-chart",
    "title": "\nExploratory Data Analysis in Python",
    "section": "9.2 Line Chart",
    "text": "9.2 Line Chart\n\nplot = (ggplot(product, aes(x = 'SIZE_US', y= 'UNITPRICE', color = 'GENDER'))+\n facet_wrap('COUNTRY')+\n geom_smooth(se = False, method = 'lm')+\n labs(x = \"Shoe Size (US)\", y = \"Price\")+\n theme (legend_position = \"top\")\n)\nplot.draw(True)\n\n\n\n\nRelations between Shoe Size and Sale Price in Different Countries\n\n\n\n\n\nmonth_sales = product['MONTH'] \\\n    .value_counts(sort = False) \\\n    .reset_index(name = 'SALES') \\\n    .rename (columns = {'index' : 'MONTH'})\n\nmonth_sales['MONTH'] = pd.to_numeric(month_sales['MONTH']) \n\nplot = (ggplot(month_sales, aes (\"MONTH\", \"SALES\"))\n + geom_point(color = 'blue')\n + labs(x = \"Month\", y = \"Total Sales\"\n   #,title = \"SALES IN DIFFERENT MONTHS\"\n   )\n)\nplot.draw(True)\n\n\n\n\nSales of Shoe in Different Months",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "python_eda.html#footnotes",
    "href": "python_eda.html#footnotes",
    "title": "\nExploratory Data Analysis in Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.techrepublic.com/article/why-data-scientist-is-the-most-promising-job-of-2019/↩︎",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in Python"
    ]
  },
  {
    "objectID": "posts/how-ai-transform-business.html",
    "href": "posts/how-ai-transform-business.html",
    "title": "How AI/ML will Transform Business?",
    "section": "",
    "text": "How Artificial Intelligence Will Transform Businesses\n\n\n\n Back to top"
  },
  {
    "objectID": "moduels.html",
    "href": "moduels.html",
    "title": "Modules & Slides",
    "section": "",
    "text": "The following Modules (Chapters) are suggested for the course -\n\nChapter # 01 - Introduction to Machine Learning (ML) & Concepts\nChapter # 02 - Introduction to R and Python and Exploratory Data Analysis (EDA)\nChapter # 03 - KNN Algorithm and Fundamental Concepts in ML Modeling\nChapter # 04 - Logistic Regression Algorithm\nChapter # 05 - Discriminant Analysis\nChapter # 06 - Support Vector Machine (SVM)\nChapter # 07 - Linear Regression\nChapter # 08 - Decision Tree Analysis\nChapter # 09 - Dimension Reduction and Principal Component Analysis\nChapter # 10 - K- Means Clustering",
    "crumbs": [
      "Modules & Slides"
    ]
  },
  {
    "objectID": "moduels.html#modules",
    "href": "moduels.html#modules",
    "title": "Modules & Slides",
    "section": "",
    "text": "The following Modules (Chapters) are suggested for the course -\n\nChapter # 01 - Introduction to Machine Learning (ML) & Concepts\nChapter # 02 - Introduction to R and Python and Exploratory Data Analysis (EDA)\nChapter # 03 - KNN Algorithm and Fundamental Concepts in ML Modeling\nChapter # 04 - Logistic Regression Algorithm\nChapter # 05 - Discriminant Analysis\nChapter # 06 - Support Vector Machine (SVM)\nChapter # 07 - Linear Regression\nChapter # 08 - Decision Tree Analysis\nChapter # 09 - Dimension Reduction and Principal Component Analysis\nChapter # 10 - K- Means Clustering",
    "crumbs": [
      "Modules & Slides"
    ]
  },
  {
    "objectID": "moduels.html#slides",
    "href": "moduels.html#slides",
    "title": "Modules & Slides",
    "section": "Slides",
    "text": "Slides\nThe slides for the course are available here.",
    "crumbs": [
      "Modules & Slides"
    ]
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "href": "mlbusiness2/Lib/site-packages/pyzmq-26.2.0.dist-info/licenses/LICENSE.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/period/test_constructors.html",
    "href": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/period/test_constructors.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "mlbusiness2/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "chap10_kmeansclustering.html",
    "href": "chap10_kmeansclustering.html",
    "title": "\nChapter # 10  K-means Clustering",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Modules & Slides",
      "Clustering",
      "<center> Chapter # 10 <br> K-means Clustering"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#assumptions-of-linear-discriminant-analysis",
    "href": "chap05_discriminant.html#assumptions-of-linear-discriminant-analysis",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "2 Assumptions of Linear Discriminant Analysis",
    "text": "2 Assumptions of Linear Discriminant Analysis\nDiscriminant analysis assumes that:\n\nThe data is normally distributed.\nMeans of each class are specific to that class.\nAll classes have a common covariance matrix.\n\nIf these assumptions are realized, DA generates a linear decision boundary.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#loading-python-packages",
    "href": "chap05_discriminant.html#loading-python-packages",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "3 Loading Python Packages",
    "text": "3 Loading Python Packages\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Python version that is used for this code file is {}'.format(sys.version))\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\nprint ('The Panda version that is used for this code file is {}'.format(pd.__version__))\nprint ('The Numpy version that is used for this code file is {}'.format(np.__version__))\n\nThe Python version that is used for this code file is 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\nThe Scikit-learn version that is used for this code file is 1.6.0\nThe Panda version that is used for this code file is 2.2.3\nThe Numpy version that is used for this code file is 2.2.1",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#working-directory",
    "href": "chap05_discriminant.html#working-directory",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "4 Working Directory",
    "text": "4 Working Directory\n\nimport os\nos.getcwd()\n\n\nfor x in os.listdir():\n  print (x)",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#importing-datasets",
    "href": "chap05_discriminant.html#importing-datasets",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "5 Importing Datasets",
    "text": "5 Importing Datasets\n\nfrom sklearn import datasets\ndataset = datasets.load_wine()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#metadata-of-the-imported-dataset",
    "href": "chap05_discriminant.html#metadata-of-the-imported-dataset",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "6 Metadata of the Imported Dataset",
    "text": "6 Metadata of the Imported Dataset\n\ndataset.keys()\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])\n\n\n\ndataset['data']\n\narray([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n        1.065e+03],\n       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n        1.050e+03],\n       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n        1.185e+03],\n       ...,\n       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n        8.350e+02],\n       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n        8.400e+02],\n       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n        5.600e+02]], shape=(178, 13))\n\n\n\ndataset['target']\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n\n\n\ndataset['target_names']\n\narray(['class_0', 'class_1', 'class_2'], dtype='&lt;U7')\n\n\n\n# Creating Data frame from the array \ndata = pd.DataFrame(dataset['data'], columns = dataset['feature_names'])\ndata.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n\n\n\n\n\n\n\n\n# Feature Vector \nfeatures_df = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n# Target Vector \ntarget_df = pd.Categorical.from_codes(dataset.target, dataset.target_names)\n\n\ntarget_df\n\n['class_0', 'class_0', 'class_0', 'class_0', 'class_0', ..., 'class_2', 'class_2', 'class_2', 'class_2', 'class_2']\nLength: 178\nCategories (3, object): ['class_0', 'class_1', 'class_2']\n\n\n\n# Joining the above two datasets \ndf = features_df.join(pd.Series(target_df, name = 'class'))\ndf.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nclass\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\nclass_0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\nclass_0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\nclass_0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\nclass_0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\nclass_0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 178 entries, 0 to 177\nData columns (total 14 columns):\n #   Column                        Non-Null Count  Dtype   \n---  ------                        --------------  -----   \n 0   alcohol                       178 non-null    float64 \n 1   malic_acid                    178 non-null    float64 \n 2   ash                           178 non-null    float64 \n 3   alcalinity_of_ash             178 non-null    float64 \n 4   magnesium                     178 non-null    float64 \n 5   total_phenols                 178 non-null    float64 \n 6   flavanoids                    178 non-null    float64 \n 7   nonflavanoid_phenols          178 non-null    float64 \n 8   proanthocyanins               178 non-null    float64 \n 9   color_intensity               178 non-null    float64 \n 10  hue                           178 non-null    float64 \n 11  od280/od315_of_diluted_wines  178 non-null    float64 \n 12  proline                       178 non-null    float64 \n 13  class                         178 non-null    category\ndtypes: category(1), float64(13)\nmemory usage: 18.5 KB\n\n\n\ndf.columns\nnum_features= dataset.feature_names\nnum_features\n# Looping functions \ndf.groupby('class')[num_features].mean().transpose()\n\n\n\n\n\n\n\nclass\nclass_0\nclass_1\nclass_2\n\n\n\n\nalcohol\n13.744746\n12.278732\n13.153750\n\n\nmalic_acid\n2.010678\n1.932676\n3.333750\n\n\nash\n2.455593\n2.244789\n2.437083\n\n\nalcalinity_of_ash\n17.037288\n20.238028\n21.416667\n\n\nmagnesium\n106.338983\n94.549296\n99.312500\n\n\ntotal_phenols\n2.840169\n2.258873\n1.678750\n\n\nflavanoids\n2.982373\n2.080845\n0.781458\n\n\nnonflavanoid_phenols\n0.290000\n0.363662\n0.447500\n\n\nproanthocyanins\n1.899322\n1.630282\n1.153542\n\n\ncolor_intensity\n5.528305\n3.086620\n7.396250\n\n\nhue\n1.062034\n1.056282\n0.682708\n\n\nod280/od315_of_diluted_wines\n3.157797\n2.785352\n1.683542\n\n\nproline\n1115.711864\n519.507042\n629.895833",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#analysis-of-variance-anova",
    "href": "chap05_discriminant.html#analysis-of-variance-anova",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "7 Analysis of Variance (ANOVA)",
    "text": "7 Analysis of Variance (ANOVA)\nOne-way ANOVA (also known as “Analysis of Variance”) is a test that is used to find out whether there exists a statistically significant difference between the mean values of more than one group.\nSee Figure 1 to know when you should use which correlation\n\n\n\n\n\n\nFigure 1: When You Should Use Which Correlation?\n\n\n\nA one-way ANOVA has the below given null and alternative hypotheses:\nH0 (Null hypothesis): μ1 = μ2 = μ3 = … = μk (It implies that the means of all the population are equal)\nH1 (Alternate hypothesis): It states that there will be at least one population mean that differs from the rest\n\ndataset.target_names\nalc_class0 = df[df['class']=='class_0']['alcohol']\ntype(alc_class0)\nalc_class1 = df[df['class']=='class_1']['alcohol']\nalc_class2 = df[df['class']=='class_2']['alcohol']\n\n\nfrom scipy.stats import f_oneway\nf_oneway(alc_class0,alc_class1,alc_class2)\n\nF_onewayResult(statistic=np.float64(135.07762424279912), pvalue=np.float64(3.319503795619639e-36))\n\n\nThe F statistic is 135.0776 and p-value is 0.000. Since the p-value is less than 0.05, we reject Null Hypothesis (H0). The findings imply that there exists a difference between three groups for the variable alcohol.\nSo, where does the difference come from? We can perform Post Hoc Analysis to check where does the differences come from\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\ntukey = pairwise_tukeyhsd(endog=df['alcohol'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey.summary()\n\n\nMultiple Comparison of Means - Tukey HSD, FWER=0.05\n\n\ngroup1\ngroup2\nmeandiff\np-adj\nlower\nupper\nreject\n\n\nclass_0\nclass_1\n-1.466\n0.0\n-1.6792\n-1.2528\nTrue\n\n\nclass_0\nclass_2\n-0.591\n0.0\n-0.8262\n-0.3558\nTrue\n\n\nclass_1\nclass_2\n0.875\n0.0\n0.6489\n1.1011\nTrue\n\n\n\n\n\nSee Figure 2 for the sources of differences of target variable and alcohol\n\ntukey.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Where Does the Difference Come From between Target variable and variable alcohol?\n\n\n\n\n\n\n7.1 Using ANOVA for Feature Selection\n\nlist(dataset.target_names)\n\n[np.str_('class_0'), np.str_('class_1'), np.str_('class_2')]\n\n\n\ntukey_malic = pairwise_tukeyhsd(endog=df['malic_acid'],     # Data\n                          groups=df['class'],   # Groups\n                          alpha=0.05)\ntukey_malic.summary()\n\n\nMultiple Comparison of Means - Tukey HSD, FWER=0.05\n\n\ngroup1\ngroup2\nmeandiff\np-adj\nlower\nupper\nreject\n\n\nclass_0\nclass_1\n-0.078\n0.8855\n-0.4703\n0.3143\nFalse\n\n\nclass_0\nclass_2\n1.3231\n0.0\n0.8902\n1.7559\nTrue\n\n\nclass_1\nclass_2\n1.4011\n0.0\n0.9849\n1.8172\nTrue\n\n\n\n\n\nSee Figure 3 for the difference between target variable and malic_acid\n\ntukey_malic.plot_simultaneous()    # Plot group confidence intervals\nplt.vlines(x=49.57,ymin=-0.5,ymax=4.5, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\nFigure 3: Where Does the Difference Come From between Target variable and variable malic_acid?\n\n\n\n\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ndf.rename(columns = {\n  'od280/od315_of_diluted_wines': 'diluted_wines'}, inplace = True)",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#linear-discriminant-analysis",
    "href": "chap05_discriminant.html#linear-discriminant-analysis",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "8 Linear Discriminant Analysis",
    "text": "8 Linear Discriminant Analysis\n\nX = dataset.data\ny = dataset.target\ntarget_names = dataset.target_names\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nlda = LinearDiscriminantAnalysis(n_components = 2)\n\n\nX_r2 = lda.fit(X,y).transform(X)\n\n\nX_r2[0:10,]\n\narray([[-4.70024401,  1.97913835],\n       [-4.30195811,  1.17041286],\n       [-3.42071952,  1.42910139],\n       [-4.20575366,  4.00287148],\n       [-1.50998168,  0.4512239 ],\n       [-4.51868934,  3.21313756],\n       [-4.52737794,  3.26912179],\n       [-4.14834781,  3.10411765],\n       [-3.86082876,  1.95338263],\n       [-3.36662444,  1.67864327]])\n\n\n\nlda.explained_variance_ratio_\n\narray([0.68747889, 0.31252111])\n\n\n\n8.1 Plotting the Dataset\n\nplt.figure(figsize = (15,8))\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target,cmap = 'gnuplot', alpha = 0.7)\nplt.xlabel('DF1')\nplt.ylabel('DF2')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.2 Distribution of LDA Components\n\ndf_lda = pd.DataFrame(zip(X_r2[:,0], X_r2[:,1],y), columns = [\"ld1\", \"ld2\", \"class\"])\nsns.set(rc={'figure.figsize':(12,8)})\nplt.subplot(2,1,1)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld1')\nplt.subplot(2,1,2)\nsns.boxplot(data = df_lda, x = 'class', y = 'ld2')\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#using-lda-to-solve-classification-problem",
    "href": "chap05_discriminant.html#using-lda-to-solve-classification-problem",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "9 Using LDA to Solve Classification Problem",
    "text": "9 Using LDA to Solve Classification Problem\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 2024)\n\n\n9.1 Training the Model\n\nlda_model = LinearDiscriminantAnalysis(n_components = 2)\nlda_model.fit(X_train, y_train)\n\nLinearDiscriminantAnalysis(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearDiscriminantAnalysis?Documentation for LinearDiscriminantAnalysisiFittedLinearDiscriminantAnalysis(n_components=2) \n\n\n\n\n9.2 Testing the Model\n\ny_pred = lda_model.predict(X_test)\n\n\n\n9.3 Checking Model Accuracy\n\nfrom sklearn.metrics import accuracy_score\nprint (\"The Accuracy of LDA Model is %0.2f%%.\" % (accuracy_score(y_test, y_pred)*100))\n\nThe Accuracy of LDA Model is 100.00%.\n\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#cross-validation",
    "href": "chap05_discriminant.html#cross-validation",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "10 Cross Validation",
    "text": "10 Cross Validation\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n\ncv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 50, random_state = 1)\nscores = cross_val_score(lda_model, X,y, scoring = \"accuracy\", cv = cv, n_jobs = -1)\nprint(np.mean(scores))",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#lda-vs-pca-visualization-difference",
    "href": "chap05_discriminant.html#lda-vs-pca-visualization-difference",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "11 LDA vs PCA (Visualization Difference)",
    "text": "11 LDA vs PCA (Visualization Difference)\n\n11.1 PCA Model\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_pca = pca.fit(X).transform(X)\n\n\nfrom pylab import *\nsubplot(2,1,1)\ntitle (\"PCA\")\nplt.scatter(X_pca[:,0], X_pca[:,1], c = dataset.target, cmap = \"gnuplot\")\nsubplot(2,1,2)\ntitle (\"LDA\")\nplt.scatter(X_r2[:,0], X_r2[:,1], c = dataset.target, cmap = \"gnuplot\")\nplt.show()\n\n\n\n\n\n\n\n\nBoth algorithms have successfully reduced the components but created different clusters because both have reduced the components based on different principles.\nNow let’s also visualize and compare the distributions of each of the algorithms on their respective components. Here we will visualize the distribution of the first component of each algorithm (LDA-1 and PCA-1).\n\n# creating dataframs\ndf=pd.DataFrame(zip(X_pca[:,0],X_r2[:,0],y),columns=[\"pc1\",\"ld1\",\"class\"])\n# plotting the lda1\nplt.subplot(2,1,1)\nsns.boxplot(x='class', y='ld1', data=df)\n# plotting pca1\nplt.subplot(2,1,2)\nsns.boxplot(x='class', y='pc1', data=df)\nplt.show()\n\n\n\n\n\n\n\n\nThere is a slight difference in the distribution of both of the algorithms. For example, the PCA result shows outliers only at the first target variable, whereas the LDA result contains outliers for every target variable.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#variance-covariance-matrix",
    "href": "chap05_discriminant.html#variance-covariance-matrix",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "12 Variance Covariance Matrix",
    "text": "12 Variance Covariance Matrix\nTo calculate the covariance matrix in Python using NumPy, you can import NumPy as np, create or load your data as a NumPy array, subtract the mean of each column from the data, transpose the array, multiply the transposed array and original array, divide the multiplied array by the number of observations, and print the array. Alternatively, you can use the np.cov function which takes the data array as an input and returns the covariance matrix as an output.\nTo learn more about variance-covariance matrix.\nTo learn more about eigenvalues and eigenvectors.\n\nA = [45, 37, 42, 35, 39]\nB = [38, 31, 26, 28, 33]\nC = [10, 15, 17, 21, 12]\n\ndata = np.array([A, B, C])\n\ncov_matrix = np.cov(data, bias=True)\nprint(cov_matrix)\n\n[[ 12.64   7.68  -9.6 ]\n [  7.68  17.36 -13.8 ]\n [ -9.6  -13.8   14.8 ]]\n\n\n\nnp.var(A)\n\nnp.float64(12.64)\n\n\n\nnp.var(C)\n\nnp.float64(14.8)\n\n\n\n12.1 Eigenvalues and Eigenvector for Variance-covariance Matrix\n\n# eigendecomposition\nfrom numpy.linalg import eig\n\n\n# calculate eigendecomposition\nvalues, vectors = eig(cov_matrix)\n# Eigenvalues \nprint(values)\n\n[36.22111819  6.98906964  1.58981217]\n\n\n\n# Eigenvectors \nprint(vectors)\n\n[[-0.45932764 -0.83268027  0.30929225]\n [-0.63870049  0.55159313  0.53647618]\n [ 0.61731661 -0.04887322  0.78519527]]",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#regularized-discriminant-analysis",
    "href": "chap05_discriminant.html#regularized-discriminant-analysis",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "13 Regularized Discriminant Analysis",
    "text": "13 Regularized Discriminant Analysis\nSince regularization techniques have been highly successful in the solution of ill-posed and poorly-posed inverse problems so to mitigate this problem the most reliable way is to use the regularization technique.\n\nA poorly posed problem occurs when the number of parameters to be estimated is comparable to the number of observations.\nSimilarly,ill-posed if that number exceeds the sample size.\n\nIn these cases the parameter estimates can be highly unstable, giving rise to high variance. Regularization would help to improve the estimates by shifting them away from their sample-based values towards values that are more physically valid; this would be achieved by applying shrinkage to each class.\nWhile regularization reduces the variance associated with the sample-based estimate, it may also increase bias. This process known as bias-variance trade-off is generally controlled by one or more degree-of-belief parameters that determine how strongly biasing towards “plausible” values of population parameters takes place.\nWhenever the sample size is not significantly greater than the dimension of measurement space for any class, Quantitative discriminant analysis (QDA) is ill-posed. Typically, regularization is applied to a discriminant analysis by replacing the individual class sample covariance matrices with the average weights assigned to the eigenvalues.\nThis applies a considerable degree of regularization by substantially reducing the number of parameters to be estimated. The regularization parameter () which is added to the equation of QDA and LDA takes a value between 0 to 1. It controls the degree of shrinkage of the individual class covariance matrix estimates toward the pooled estimate. Values between these limits represent degrees of regularization.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,confusion_matrix\nfrom imblearn.over_sampling import SMOTE # To install module, run this line of code - pip install imblearn\nfrom sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n\n\n# Reading a new dataset \ndf = pd.read_csv('DATA/healthcare-dataset-stroke-data.csv')\nprint(\"Records = \", df.shape[0], \"\\nFeatures = \", df.shape[1])\n\nRecords =  5110 \nFeatures =  12\n\n\n\ndf.sample(5)\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n4269\n23210\nMale\n31.0\n0\n0\nNo\nPrivate\nRural\n77.95\n25.1\nnever smoked\n0\n\n\n1173\n47735\nFemale\n59.0\n0\n0\nYes\nPrivate\nRural\n224.71\n42.9\nnever smoked\n0\n\n\n3801\n18876\nFemale\n28.0\n0\n0\nYes\nPrivate\nUrban\n69.50\n24.5\nnever smoked\n0\n\n\n1536\n25454\nFemale\n13.0\n0\n0\nNo\nchildren\nRural\n93.30\n25.9\nUnknown\n0\n\n\n75\n66159\nFemale\n80.0\n0\n1\nYes\nSelf-employed\nRural\n66.72\n21.7\nformerly smoked\n1\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5110 entries, 0 to 5109\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 5110 non-null   int64  \n 1   gender             5110 non-null   object \n 2   age                5110 non-null   float64\n 3   hypertension       5110 non-null   int64  \n 4   heart_disease      5110 non-null   int64  \n 5   ever_married       5110 non-null   object \n 6   work_type          5110 non-null   object \n 7   Residence_type     5110 non-null   object \n 8   avg_glucose_level  5110 non-null   float64\n 9   bmi                4909 non-null   float64\n 10  smoking_status     5110 non-null   object \n 11  stroke             5110 non-null   int64  \ndtypes: float64(3), int64(4), object(5)\nmemory usage: 479.2+ KB\n\n\n\n# Missing Values \n(df.isnull().sum()/len(df)*100)\n\nid                   0.000000\ngender               0.000000\nage                  0.000000\nhypertension         0.000000\nheart_disease        0.000000\never_married         0.000000\nwork_type            0.000000\nResidence_type       0.000000\navg_glucose_level    0.000000\nbmi                  3.933464\nsmoking_status       0.000000\nstroke               0.000000\ndtype: float64\n\n\n\n# Dropping the Missing Observations\ndf.dropna(axis = 0, inplace = True)\ndf.shape\n\n(4909, 12)\n\n\n\n# Creating the Dummies \ndf_pre = pd.get_dummies(df, drop_first = True)\ndf_pre.sample(5)\n\n\n\n\n\n\n\n\nid\nage\nhypertension\nheart_disease\navg_glucose_level\nbmi\nstroke\ngender_Male\ngender_Other\never_married_Yes\nwork_type_Never_worked\nwork_type_Private\nwork_type_Self-employed\nwork_type_children\nResidence_type_Urban\nsmoking_status_formerly smoked\nsmoking_status_never smoked\nsmoking_status_smokes\n\n\n\n\n4073\n10636\n74.0\n0\n0\n82.27\n23.6\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n559\n36561\n39.0\n0\n0\n191.47\n28.3\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3909\n4077\n49.0\n0\n0\n219.70\n53.8\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n3146\n13358\n75.0\n0\n0\n207.62\n31.8\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2685\n62414\n80.0\n1\n0\n178.89\n27.4\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n# Training and Testing the Split \nX = df_pre.drop(['stroke'], axis = 1)\ny = df_pre['stroke']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 25)\n\n\n# Building the LDA\nLDA = LinearDiscriminantAnalysis()\nLDA.fit_transform(X_train, y_train)\nX_test['predictions'] = LDA.predict(X_test)\nConfusionMatrixDisplay.from_predictions(y_test, X_test['predictions'])\nplt.show()\n\n\n\n\n\n\n\n\n\ntn, fp, fn, tp = confusion_matrix(list(y_test), list(X_test['predictions']), labels=[0, 1]).ravel()\n\n\nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\nprint(\"Precision score\",precision_score(y_test,X_test['predictions']))\n\nTrue Positive : 6\nTrue Negative : 1397\nFalse Positive : 13\nFalse Negative : 57\nPrecision score 0.3157894736842105\n\n\nIt has only 32% precision rate, which is very poor performance.\n\nprint(\"Accuracy Score\",accuracy_score(y_test,X_test['predictions']))\n\nAccuracy Score 0.9524779361846571\n\n\nThe accuracy is approximately 95%, but the precision is 32%.\n\n13.1 Cross Validation of the Dataset\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n\n#Define method to evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=50, random_state=1)\n\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='precision', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n\n0.23822585829203477\n\n\n\n#evaluate model\nscores = cross_val_score(LDA, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(np.mean(scores)) \n\n0.9466250593260561\n\n\nEven after cross validation, the precision is about 24% and the accuracy is 95% approximately. There is no significant improvement of the metrics of the model.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#regularizing-and-shrinking-the-lda",
    "href": "chap05_discriminant.html#regularizing-and-shrinking-the-lda",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "14 Regularizing and Shrinking the LDA",
    "text": "14 Regularizing and Shrinking the LDA\n\ndf_pre['stroke'].value_counts()\n\nstroke\n0    4700\n1     209\nName: count, dtype: int64\n\n\nAs observed by the value count of the dependent variable the data is imbalanced as the quantity of 1’s is approx 4% of the total dependent variable. So, it needs to be balanced for the learner to be a good predictor.\n\n14.1 Balancing the Dependent Variable\nThere are two ways by which the data can be synthesized: one by oversampling and the second, by undersampling. In this scenario, oversampling is better which will synthesize the lesser category linear interpolation.\n\noversample = SMOTE()\nX_smote, y_smote = oversample.fit_resample(X, y)\nXs_train, Xs_test, ys_train, ys_test = train_test_split(X_smote, y_smote, test_size=0.30, random_state=42)\n\nThe imbalance is mitigated by using the Synthetic Minority Oversampling Technique (SMOTE) but this will not help much we also need to regularize the leaner by using the GridSearchCV which will find the best parameters for the learner and add a penalty to the solver which will shrink the eigenvalue i.e regularization.\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\ngrid = dict()\ngrid['solver'] = ['eigen','lsqr']\ngrid['shrinkage'] = ['auto',0.2,1,0.3,0.5]\nsearch = GridSearchCV(LDA, grid, scoring='precision', cv=cv, n_jobs=-1)\nresults = search.fit(Xs_train, ys_train)\nprint('Precision: %.3f' % results.best_score_)\nprint('Configuration:',results.best_params_)\n\nPrecision: 0.872\nConfiguration: {'shrinkage': 'auto', 'solver': 'eigen'}\n\n\nThe precision score jumped right from 35% to 87% with the help of regularization and shrinkage of the learner and the best solver for the Linear Discriminant Analysis is eigen and the shrinkage method is auto which uses the Ledoit-Wolf lemma for finding the shrinkage penalty.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#building-the-regularized-discriminant-analysis-rda",
    "href": "chap05_discriminant.html#building-the-regularized-discriminant-analysis-rda",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "15 Building the Regularized Discriminant Analysis (RDA)",
    "text": "15 Building the Regularized Discriminant Analysis (RDA)\n\n# Build the RDA\nLDA_final=LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')\nLDA_final.fit_transform(Xs_train,ys_train)\nXs_test['predictions']=LDA_final.predict(Xs_test)\nConfusionMatrixDisplay.from_predictions(ys_test, Xs_test['predictions'])\nplt.show()\n \ntn, fp, fn, tp = confusion_matrix(list(ys_test), list(Xs_test['predictions']), labels=[0, 1]).ravel()\n \nprint('True Positive :', tp)\nprint('True Negative :', tn)\nprint('False Positive :', fp)\nprint('False Negative :', fn)\n\n\n\n\n\n\n\n\nTrue Positive : 1265\nTrue Negative : 1230\nFalse Positive : 177\nFalse Negative : 148\n\n\n\nprint(\"Precision score\",np.round(precision_score(ys_test,Xs_test['predictions']),3))\n\nPrecision score 0.877\n\n\n\nprint(\"Accuracy score\",np.round(accuracy_score(ys_test,Xs_test['predictions']),3))\n\nAccuracy score 0.885",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap05_discriminant.html#conclusion",
    "href": "chap05_discriminant.html#conclusion",
    "title": "\nChapter # 05  Discriminant Analysis",
    "section": "16 Conclusion",
    "text": "16 Conclusion",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 05 <br> Discriminant Analysis"
    ]
  },
  {
    "objectID": "chap03_slide.html#introduction",
    "href": "chap03_slide.html#introduction",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#introduction-contd",
    "href": "chap03_slide.html#introduction-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Introduction (Cont’d)",
    "text": "Introduction (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#what-is-knn-algorithm",
    "href": "chap03_slide.html#what-is-knn-algorithm",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "What is KNN Algorithm?",
    "text": "What is KNN Algorithm?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#how-does-knn-learn",
    "href": "chap03_slide.html#how-does-knn-learn",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "How Does KNN Learn?",
    "text": "How Does KNN Learn?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#how-does-knn-learn-contd",
    "href": "chap03_slide.html#how-does-knn-learn-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "How Does KNN Learn? (Cont’d)",
    "text": "How Does KNN Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#how-does-knn-learn-contd-1",
    "href": "chap03_slide.html#how-does-knn-learn-contd-1",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "How Does KNN Learn? (Cont’d)",
    "text": "How Does KNN Learn? (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#what-happens-if-the-vote-is-tied",
    "href": "chap03_slide.html#what-happens-if-the-vote-is-tied",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "What Happens if the Vote is Tied?",
    "text": "What Happens if the Vote is Tied?",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#fyi",
    "href": "chap03_slide.html#fyi",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "FYI",
    "text": "FYI",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#building-knn-algorithm",
    "href": "chap03_slide.html#building-knn-algorithm",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Building KNN Algorithm",
    "text": "Building KNN Algorithm",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#defining-the-task",
    "href": "chap03_slide.html#defining-the-task",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Defining the Task",
    "text": "Defining the Task",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#define-the-learner",
    "href": "chap03_slide.html#define-the-learner",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Define the Learner",
    "text": "Define the Learner",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#train-the-model",
    "href": "chap03_slide.html#train-the-model",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Train the Model",
    "text": "Train the Model",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#making-predictions-using-our-model-and-evaluating-performance-of-the-model",
    "href": "chap03_slide.html#making-predictions-using-our-model-and-evaluating-performance-of-the-model",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Making Predictions using our Model and Evaluating Performance of the Model",
    "text": "Making Predictions using our Model and Evaluating Performance of the Model",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#bias-variance-trade-off-two-sources-of-model-error",
    "href": "chap03_slide.html#bias-variance-trade-off-two-sources-of-model-error",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Bias-Variance Trade Off – Two Sources of Model Error",
    "text": "Bias-Variance Trade Off – Two Sources of Model Error",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#bias-variance-trade-off-two-sources-of-model-error-contd",
    "href": "chap03_slide.html#bias-variance-trade-off-two-sources-of-model-error-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Bias-Variance Trade Off – Two Sources of Model Error (Cont’d)",
    "text": "Bias-Variance Trade Off – Two Sources of Model Error (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#holdout-cross-validation-cv",
    "href": "chap03_slide.html#holdout-cross-validation-cv",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Holdout Cross-validation (CV)",
    "text": "Holdout Cross-validation (CV)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#holdout-cross-validation-cv-contd",
    "href": "chap03_slide.html#holdout-cross-validation-cv-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Holdout Cross-validation (CV) (Cont’d)",
    "text": "Holdout Cross-validation (CV) (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#k-fold-cv",
    "href": "chap03_slide.html#k-fold-cv",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "K-fold CV",
    "text": "K-fold CV",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#k-fold-cv-contd",
    "href": "chap03_slide.html#k-fold-cv-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "K-fold CV (Cont’d)",
    "text": "K-fold CV (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#leave-one-out-loo-cv",
    "href": "chap03_slide.html#leave-one-out-loo-cv",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Leave-One-Out (LOO) CV",
    "text": "Leave-One-Out (LOO) CV",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#parameters-vs-hyperparameters",
    "href": "chap03_slide.html#parameters-vs-hyperparameters",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Parameters vs Hyperparameters",
    "text": "Parameters vs Hyperparameters",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#parameters-vs-hyperparameters-contd",
    "href": "chap03_slide.html#parameters-vs-hyperparameters-contd",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Parameters vs Hyperparameters (Cont’d)",
    "text": "Parameters vs Hyperparameters (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap03_slide.html#questions-or-queries",
    "href": "chap03_slide.html#questions-or-queries",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN)",
    "section": "Questions or Queries",
    "text": "Questions or Queries\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nQueries",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN)"
    ]
  },
  {
    "objectID": "chap02_RpythonEDA.html",
    "href": "chap02_RpythonEDA.html",
    "title": "\nChapter # 02  An Introduction to R and Python",
    "section": "",
    "text": "Please see the Appendix to learn about R and Python\n\n\n\n Back to top",
    "crumbs": [
      "Modules & Slides",
      "Introduction",
      "<center> Chapter # 02 <br> An Introduction to R and Python"
    ]
  },
  {
    "objectID": "chap01_intro.html",
    "href": "chap01_intro.html",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "",
    "text": "To get the slides of this chapter, please see the “slide” dropdown menu on “Modules & Slides” tab on the menu bar of this website.\n\n\n\n Back to top",
    "crumbs": [
      "Modules & Slides",
      "Introduction",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The website is for the course BSAN405 - Machine Learning in Business - in College of Business & Analytics in Southern Illinois University Carbondale (SIUC). If you have questions or queries, please email the intructor.\n\n\n\n Back to top"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Books are not required for the course. All of the books are suggested.\n\nMachine Learning with R, the tidyverse, and mlr by Hefin I. Rhys. The book is available on Manning Publications website. The hard copy of the book is not required. Online copy is fine.\n\n\n\n\n\n\n\nPython Data Science Handbook: Essential Tools for Working with Data by Jake VanderPlas. The book is available on Github. The hard copy of the book is not required. Online copy is fine. This is the first edition of the book. The second edition of the book is also available. If you are interested, you can buy one from Amazon.\n\n\n\n\n\n\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron. The book is available on Amazon. The hard copy of the book is not required. A pdf copy of the book is available from me on request.\n\n\n\n\n\n\n\nSupplemental Reading\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund is an excellent book to learn about the basics of R. The online version of 2nd Edition of the book is available free.\n\n\n\n\n\n\n\nTo learn about the mathematics underlying many Machine Learning (ML) algorithms, Mathematics for Machine Learning by Marc Deisenroth, A Aldo Faisal, and Cheng Ong can be used -\n\n\n\n\n\n\n\nTo learn about the Ethics in Machine Learning, Fairness and Machine Learning by Solon Barocas, Mortiz Hardt, and Arvind Narayanan can be used. The book is not needed to be purchased. An online version of the book is available.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "chap01_slide.html#introduction",
    "href": "chap01_slide.html#introduction",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#one-caveat",
    "href": "chap01_slide.html#one-caveat",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "One Caveat",
    "text": "One Caveat",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#what-is-machine-learning-ml",
    "href": "chap01_slide.html#what-is-machine-learning-ml",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "What is Machine Learning (ML)",
    "text": "What is Machine Learning (ML)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#artificial-intelligence-al-vs-machine-learning-ml",
    "href": "chap01_slide.html#artificial-intelligence-al-vs-machine-learning-ml",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Artificial Intelligence (AL) vs Machine Learning (ML)",
    "text": "Artificial Intelligence (AL) vs Machine Learning (ML)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#artificial-intelligence-ai-vs-machine-learning-mlcontd",
    "href": "chap01_slide.html#artificial-intelligence-ai-vs-machine-learning-mlcontd",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Artificial Intelligence (AI) vs Machine Learning (ML)(Cont’d)",
    "text": "Artificial Intelligence (AI) vs Machine Learning (ML)(Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#model-vs-algorithm",
    "href": "chap01_slide.html#model-vs-algorithm",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Model vs Algorithm",
    "text": "Model vs Algorithm",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#model-vs-algorithm-contd",
    "href": "chap01_slide.html#model-vs-algorithm-contd",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Model vs Algorithm (Cont’d)",
    "text": "Model vs Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#model-vs-algorithm-contd-1",
    "href": "chap01_slide.html#model-vs-algorithm-contd-1",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Model vs Algorithm (Cont’d)",
    "text": "Model vs Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#classes-of-machine-learning-algorithm",
    "href": "chap01_slide.html#classes-of-machine-learning-algorithm",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Classes of Machine Learning Algorithm",
    "text": "Classes of Machine Learning Algorithm",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#classes-of-machine-learning-algorithm-contd",
    "href": "chap01_slide.html#classes-of-machine-learning-algorithm-contd",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Classes of Machine Learning Algorithm (Cont’d)",
    "text": "Classes of Machine Learning Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#classes-of-machine-learning-algorithm-contd-1",
    "href": "chap01_slide.html#classes-of-machine-learning-algorithm-contd-1",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Classes of Machine Learning Algorithm (Cont’d)",
    "text": "Classes of Machine Learning Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#classes-of-machine-learning-algorithm-contd-2",
    "href": "chap01_slide.html#classes-of-machine-learning-algorithm-contd-2",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Classes of Machine Learning Algorithm (Cont’d)",
    "text": "Classes of Machine Learning Algorithm (Cont’d)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#artificial-intelligence-ai-machine-learning-ml-and-deep-learning-dl",
    "href": "chap01_slide.html#artificial-intelligence-ai-machine-learning-ml-and-deep-learning-dl",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Artificial Intelligence (AI), Machine learning (ML), and Deep learning (DL)",
    "text": "Artificial Intelligence (AI), Machine learning (ML), and Deep learning (DL)",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#ethics-in-machine-learning",
    "href": "chap01_slide.html#ethics-in-machine-learning",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Ethics in Machine Learning",
    "text": "Ethics in Machine Learning",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#python-vs.-r-for-machine-learning",
    "href": "chap01_slide.html#python-vs.-r-for-machine-learning",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Python VS. R for Machine Learning",
    "text": "Python VS. R for Machine Learning",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#what-will-we-learn-from-bsan405",
    "href": "chap01_slide.html#what-will-we-learn-from-bsan405",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "What will We Learn from BSAN405",
    "text": "What will We Learn from BSAN405",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap01_slide.html#questions-or-queries",
    "href": "chap01_slide.html#questions-or-queries",
    "title": "\nChapter # 01  Introduction to Machine Learning",
    "section": "Questions or Queries",
    "text": "Questions or Queries\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nQueries",
    "crumbs": [
      "Modules & Slides",
      "Slides",
      "<center> Chapter # 01 <br> Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "chap03_knn.html#assigning-column-names",
    "href": "chap03_knn.html#assigning-column-names",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "5.1 Assigning Column Names",
    "text": "5.1 Assigning Column Names\n\ncol_names = ['Id', 'Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape', 'Marginal_Adhesion','Single_Epithelial_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class']\n\ndf.columns = col_names\n\ndf.columns\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 699 entries, 0 to 698\nData columns (total 11 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Id                           699 non-null    int64 \n 1   Clump_thickness              699 non-null    int64 \n 2   Uniformity_Cell_Size         699 non-null    int64 \n 3   Uniformity_Cell_Shape        699 non-null    int64 \n 4   Marginal_Adhesion            699 non-null    int64 \n 5   Single_Epithelial_Cell_Size  699 non-null    int64 \n 6   Bare_Nuclei                  699 non-null    object\n 7   Bland_Chromatin              699 non-null    int64 \n 8   Normal_Nucleoli              699 non-null    int64 \n 9   Mitoses                      699 non-null    int64 \n 10  Class                        699 non-null    int64 \ndtypes: int64(10), object(1)\nmemory usage: 60.2+ KB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nId\nClump_thickness\nUniformity_Cell_Size\nUniformity_Cell_Shape\nMarginal_Adhesion\nSingle_Epithelial_Cell_Size\nBare_Nuclei\nBland_Chromatin\nNormal_Nucleoli\nMitoses\nClass\n\n\n\n\n0\n1000025\n5\n1\n1\n1\n2\n1\n3\n1\n1\n2\n\n\n1\n1002945\n5\n4\n4\n5\n7\n10\n3\n2\n1\n2\n\n\n2\n1015425\n3\n1\n1\n1\n2\n2\n3\n1\n1\n2\n\n\n3\n1016277\n6\n8\n8\n1\n3\n4\n3\n7\n1\n2\n\n\n4\n1017023\n4\n1\n1\n3\n2\n1\n3\n1\n1\n2",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#dropping-redundant-columns",
    "href": "chap03_knn.html#dropping-redundant-columns",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "5.2 Dropping Redundant Columns",
    "text": "5.2 Dropping Redundant Columns\n\ndf.drop('Id', axis=1, inplace=True)\ndf.info()\ndf.dtypes\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 699 entries, 0 to 698\nData columns (total 10 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Clump_thickness              699 non-null    int64 \n 1   Uniformity_Cell_Size         699 non-null    int64 \n 2   Uniformity_Cell_Shape        699 non-null    int64 \n 3   Marginal_Adhesion            699 non-null    int64 \n 4   Single_Epithelial_Cell_Size  699 non-null    int64 \n 5   Bare_Nuclei                  699 non-null    object\n 6   Bland_Chromatin              699 non-null    int64 \n 7   Normal_Nucleoli              699 non-null    int64 \n 8   Mitoses                      699 non-null    int64 \n 9   Class                        699 non-null    int64 \ndtypes: int64(9), object(1)\nmemory usage: 54.7+ KB\n\n\nClump_thickness                 int64\nUniformity_Cell_Size            int64\nUniformity_Cell_Shape           int64\nMarginal_Adhesion               int64\nSingle_Epithelial_Cell_Size     int64\nBare_Nuclei                    object\nBland_Chromatin                 int64\nNormal_Nucleoli                 int64\nMitoses                         int64\nClass                           int64\ndtype: object\n\n\n\ndf['Class'].value_counts()\n\nClass\n2    458\n4    241\nName: count, dtype: int64",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#changing-data-types-of-variables",
    "href": "chap03_knn.html#changing-data-types-of-variables",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "5.3 Changing Data Types of Variables",
    "text": "5.3 Changing Data Types of Variables\n\ndf['Bare_Nuclei'] = pd.to_numeric(df['Bare_Nuclei'], errors='coerce')\ndf.dtypes\n\nClump_thickness                  int64\nUniformity_Cell_Size             int64\nUniformity_Cell_Shape            int64\nMarginal_Adhesion                int64\nSingle_Epithelial_Cell_Size      int64\nBare_Nuclei                    float64\nBland_Chromatin                  int64\nNormal_Nucleoli                  int64\nMitoses                          int64\nClass                            int64\ndtype: object",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#checking-missing-observations-in-the-dataset",
    "href": "chap03_knn.html#checking-missing-observations-in-the-dataset",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "5.4 Checking Missing Observations in the Dataset",
    "text": "5.4 Checking Missing Observations in the Dataset\n\ndf.isnull().sum() # Checking missing values in variables\ndf.isna().sum() # Checking missing values in the dataframe \n\nClump_thickness                 0\nUniformity_Cell_Size            0\nUniformity_Cell_Shape           0\nMarginal_Adhesion               0\nSingle_Epithelial_Cell_Size     0\nBare_Nuclei                    16\nBland_Chromatin                 0\nNormal_Nucleoli                 0\nMitoses                         0\nClass                           0\ndtype: int64\n\n\n\ndf['Bare_Nuclei'].value_counts()\n\nBare_Nuclei\n1.0     402\n10.0    132\n2.0      30\n5.0      30\n3.0      28\n8.0      21\n4.0      19\n9.0       9\n7.0       8\n6.0       4\nName: count, dtype: int64\n\n\n\ndf['Bare_Nuclei'].unique()\n\narray([ 1., 10.,  2.,  4.,  3.,  9.,  7., nan,  5.,  8.,  6.])",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#multivariate-plots",
    "href": "chap03_knn.html#multivariate-plots",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "7.1 Multivariate Plots",
    "text": "7.1 Multivariate Plots\n\ncorrelation = df.corr()\ncorrelation['Class'].sort_values(ascending=False)\n\nClass                          1.000000\nBare_Nuclei                    0.822696\nUniformity_Cell_Shape          0.818934\nUniformity_Cell_Size           0.817904\nBland_Chromatin                0.756616\nClump_thickness                0.716001\nNormal_Nucleoli                0.712244\nMarginal_Adhesion              0.696800\nSingle_Epithelial_Cell_Size    0.682785\nMitoses                        0.423170\nName: Class, dtype: float64\n\n\nInterpretation: The correlation coefficient ranges from -1 to +1.\nWhen it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is a strong positive correlation between Class and Bare_Nuclei, Class and Uniformity_Cell_Shape, Class and Uniformity_Cell_Size.\nWhen it is close to -1, it means that there is a strong negative correlation. When it is close to 0, it means that there is no correlation.\nWe can see that all the variables are positively correlated with Class variable. Some variables are strongly positive correlated while some variables are negatively correlated.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#discover-pattern-and-relationship",
    "href": "chap03_knn.html#discover-pattern-and-relationship",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "7.2 Discover Pattern and Relationship",
    "text": "7.2 Discover Pattern and Relationship\nAn important step in EDA is to discover patterns and relationships between variables in the dataset. I will use the seaborn heatmap to explore the patterns and relationships in the dataset.\n\nplt.figure(figsize=(10,8))\nplt.title('Correlation of Attributes with Class variable')\na = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white')\na.set_xticklabels(a.get_xticklabels(), rotation=90)\na.set_yticklabels(a.get_yticklabels(), rotation=30)           \nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#engineering-missing-values-in-variables",
    "href": "chap03_knn.html#engineering-missing-values-in-variables",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "10.1 Engineering Missing Values in Variables",
    "text": "10.1 Engineering Missing Values in Variables\n\n# check missing values in numerical variables in X_test\n\nX_test.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    3\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\n\n# print percentage of missing values in the numerical variables in training set\n\nfor col in X_train.columns:\n    if X_train[col].isnull().mean()&gt;0:\n        print(col, round(X_train[col].isnull().mean(),4))\n\nBare_Nuclei 0.0233\n\n\nAssumption I assume that the data are missing completely at random (MCAR). There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation. When there are outliers in the dataset, we should use median imputation. So, I will use median imputation because median imputation is robust to outliers.\nI will impute missing values with the appropriate statistical measures of the data, in this case median. Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting.\n\n# impute missing values in X_train and X_test with respective column median in X_train\n\nfor df in [X_train, X_test]:\n    for col in X_train.columns:\n        col_median=X_train[col].median()\n        df[col].fillna(col_median, inplace=True)  \n\n\n# check again missing values in numerical variables in X_train\n\nX_train.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    0\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\n\n# check missing values in numerical variables in X_test\n\nX_test.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    0\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\nWe now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called feature scaling. I will do it as follows.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#feature-selection",
    "href": "chap03_knn.html#feature-selection",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "10.2 Feature Selection",
    "text": "10.2 Feature Selection\nFeature selection involves identifying the features that have the greatest explanatory power to predict the target variables. Therere are many techniques that can be used for feature selection. When there are many variables, using feature selection is very much important. Otherwise, noises might be introduced in the model.\n\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\n\n\nchi_feature = SelectKBest(chi2, k = 4).fit(X_train, y_train)\nprint ('Score: ', chi_feature.scores_)\nprint ('Features: ', X_train.columns)\n\nScore:  [ 493.52822606 1144.54028237 1046.47720631  844.84992698  404.07603727\n 1364.87717094  532.42045225  951.992508    180.65784477]\nFeatures:  Index(['Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape',\n       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses'],\n      dtype='object')\n\n\n\nanova_feature = SelectKBest (f_classif, k = 4).fit(X_train, y_train)\nprint ('Scores: ', anova_feature.scores_)\nprint ('Features: ', X_train.columns)\n\nScores:  [ 545.20592252 1142.16232995 1093.33969603  582.15032766  518.143572\n 1022.11887635  716.06901174  593.48561929  124.1776932 ]\nFeatures:  Index(['Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape',\n       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses'],\n      dtype='object')",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#feature-scaling",
    "href": "chap03_knn.html#feature-scaling",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "10.3 Feature Scaling",
    "text": "10.3 Feature Scaling\n\ncols = X_train.columns\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n\n\nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])\nX_train.head()\n\n\n\n\n\n\n\n\nClump_thickness\nUniformity_Cell_Size\nUniformity_Cell_Shape\nMarginal_Adhesion\nSingle_Epithelial_Cell_Size\nBare_Nuclei\nBland_Chromatin\nNormal_Nucleoli\nMitoses\n\n\n\n\n0\n2.028383\n0.299506\n0.289573\n1.119077\n-0.546543\n1.858357\n-0.577774\n0.041241\n-0.324258\n\n\n1\n1.669451\n2.257680\n2.304569\n-0.622471\n3.106879\n1.297589\n-0.159953\n0.041241\n-0.324258\n\n\n2\n-1.202005\n-0.679581\n-0.717925\n0.074148\n-1.003220\n-0.104329\n-0.995595\n-0.608165\n-0.324258\n\n\n3\n-0.125209\n-0.026856\n-0.046260\n-0.622471\n-0.546543\n-0.665096\n-0.159953\n0.041241\n-0.324258\n\n\n4\n0.233723\n-0.353219\n-0.382092\n-0.274161\n-0.546543\n-0.665096\n-0.577774\n-0.283462\n-0.324258",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#compare-the-train-set-and-test-set-accuracy",
    "href": "chap03_knn.html#compare-the-train-set-and-test-set-accuracy",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "13.1 Compare the Train-set and Test-set Accuracy",
    "text": "13.1 Compare the Train-set and Test-set Accuracy\nNow, I will compare the train-set and test-set accuracy to check for overfitting.\n\ny_pred_train = knn.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\nTraining-set accuracy score: 0.9821",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#compare-model-accuracy-with-null-accuracy",
    "href": "chap03_knn.html#compare-model-accuracy-with-null-accuracy",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "14.1 Compare Model Accuracy with Null Accuracy",
    "text": "14.1 Compare Model Accuracy with Null Accuracy\nSo, the model accuracy is 0.9714. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\nSo, we should first check the class distribution in the test set.\n\n# check class distribution in test set\n\ny_test.value_counts()\n\nClass\n2    85\n4    55\nName: count, dtype: int64\n\n\nWe can see that the occurences of most frequent class is 85. So, we can calculate null accuracy by dividing 85 by total number of occurences.\n\n# check null accuracy score\n\nnull_accuracy = (85/(85+55))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))\n\nNull accuracy score: 0.6071\n\n\nWe can see that our model accuracy score is 0.9714 but null accuracy score is 0.6071. So, we can conclude that our K Nearest Neighbors model is doing a very good job in predicting the class labels.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#rebuild-knn-classification-model-using-k6",
    "href": "chap03_knn.html#rebuild-knn-classification-model-using-k6",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "15.1 Rebuild KNN Classification Model Using K=6",
    "text": "15.1 Rebuild KNN Classification Model Using K=6\n\n# instantiate the model with k=6\nknn_6 = KNeighborsClassifier(n_neighbors=6)\n\n\n# fit the model to the training set\nknn_6.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_6 = knn_6.predict(X_test)\n\n\nprint('Model accuracy score with k=6 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_6)))\n\nModel accuracy score with k=6 : 0.9786",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#rebuild-knn-classification-model-using-k7",
    "href": "chap03_knn.html#rebuild-knn-classification-model-using-k7",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "15.2 Rebuild KNN Classification Model Using K=7",
    "text": "15.2 Rebuild KNN Classification Model Using K=7\n\n# instantiate the model with k=7\nknn_7 = KNeighborsClassifier(n_neighbors=7)\n\n\n# fit the model to the training set\nknn_7.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_7 = knn_7.predict(X_test)\n\n\nprint('Model accuracy score with k=7 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_7)))\n\nModel accuracy score with k=7 : 0.9786",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#rebuild-knn-classification-model-using-k8",
    "href": "chap03_knn.html#rebuild-knn-classification-model-using-k8",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "15.3 Rebuild KNN Classification Model Using K=8",
    "text": "15.3 Rebuild KNN Classification Model Using K=8\n\n# instantiate the model with k=8\nknn_8 = KNeighborsClassifier(n_neighbors=8)\n\n\n# fit the model to the training set\nknn_8.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_8 = knn_8.predict(X_test)\n\n\nprint('Model accuracy score with k=8 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_8)))\n\nModel accuracy score with k=8 : 0.9786",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#rebuild-knn-classification-model-using-k9",
    "href": "chap03_knn.html#rebuild-knn-classification-model-using-k9",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "15.4 Rebuild KNN Classification Model Using K=9",
    "text": "15.4 Rebuild KNN Classification Model Using K=9\n\n# instantiate the model with k=9\nknn_9 = KNeighborsClassifier(n_neighbors=9)\n\n\n# fit the model to the training set\nknn_9.fit(X_train, y_train)\n\n\n# predict on the test-set\ny_pred_9 = knn_9.predict(X_test)\n\n\nprint('Model accuracy score with k=9 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_9)))\n\nModel accuracy score with k=9 : 0.9714\n\n\nInterpretation: Our original model accuracy score with k=3 is 0.9714. Now, we can see that we get same accuracy score of 0.9714 with k=5. But, if we increase the value of k further, this would result in enhanced accuracy.\nWith k=6,7,8 we get accuracy score of 0.9786. So, it results in performance improvement.\nIf we increase k to 9, then accuracy decreases again to 0.9714.\nNow, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\nWe have another tool called Confusion matrix that comes to our rescue.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#classification-report",
    "href": "chap03_knn.html#classification-report",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.1 Classification Report",
    "text": "19.1 Classification Report\nClassification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. I have described these terms in later.\nWe can print a classification report as follows:-\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_7))\n\n              precision    recall  f1-score   support\n\n           2       0.99      0.98      0.98        85\n           4       0.96      0.98      0.97        55\n\n    accuracy                           0.98       140\n   macro avg       0.98      0.98      0.98       140\nweighted avg       0.98      0.98      0.98       140",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#classification-accuracy",
    "href": "chap03_knn.html#classification-accuracy",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.2 Classification Accuracy",
    "text": "19.2 Classification Accuracy\n\nTP = cm_7[0,0]\nTN = cm_7[1,1]\nFP = cm_7[0,1]\nFN = cm_7[1,0]\n\n\n# print classification accuracy\n\nclassification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n\nClassification accuracy : 0.9786",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#classification-error",
    "href": "chap03_knn.html#classification-error",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.3 Classification Error",
    "text": "19.3 Classification Error\n\n# print classification error\n\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))\n\nClassification error : 0.0214",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#precision",
    "href": "chap03_knn.html#precision",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.4 Precision",
    "text": "19.4 Precision\nPrecision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). Precision is a metric that tells us about the quality of positive predictions. So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class. Precision is a useful metric in cases where False Positive is a higher concern than False Negatives. Precision is important in music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.tt\nMathematically, precision can be defined as the ratio of TP to (TP + FP).\n\n# print precision score\n\nprecision = TP / float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))\n\nPrecision : 0.9765",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#recall",
    "href": "chap03_knn.html#recall",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.5 Recall",
    "text": "19.5 Recall\nRecall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall tells us about how well the model identifies true positives. Recall is also called Sensitivity. Recall identifies the proportion of correctly predicted actual positives. Mathematically, recall can be given as the ratio of TP to (TP + FN). Recall is a useful metric in cases where False Negative triumphs over False Positive. Recall is important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n\nrecall = TP / float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))\n\nRecall or Sensitivity : 0.9881",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#precision-vs-recall",
    "href": "chap03_knn.html#precision-vs-recall",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.6 Precision vs Recall",
    "text": "19.6 Precision vs Recall\nData scientists optimize their model to have higher precision or recall depending on the circumstances. A model with higher recall than precision often makes more positive predictions. A model like this comes with higher false positives and low false negatives. In scenarios like disease prediction, models should always be optimized for recall. False positives are better than false negatives in the healthcare industry.\nOn the other hand, a model with higher precision will have fewer false positives and more false negatives. If you were to build a bot detection machine learning model for an online store, you may want to optimize for higher precision, since banning legitimate users from the website will lead to a decline in sales.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#f1-score",
    "href": "chap03_knn.html#f1-score",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.7 f1-score",
    "text": "19.7 f1-score\nIn practice, when we try to increase the precision of our model, the recall goes down, and vice-versa. The F1-score captures both the trends in a single value:\n        f1-score = 2/((1/Recall) + (1/Precision))\nf1-score is the weighted harmonic mean of precision and recall, and so it gives a combined idea about these two metrics. It is maximum when Precision is equal to Recall. The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#support",
    "href": "chap03_knn.html#support",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.8 Support",
    "text": "19.8 Support\nSupport is the actual number of occurrences of the class in our dataset.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#true-positive-rate",
    "href": "chap03_knn.html#true-positive-rate",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.9 True Positive Rate",
    "text": "19.9 True Positive Rate\nTrue Positive Rate is synonymous with Recall.\n\ntrue_positive_rate = TP / float(TP + FN)\n\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n\nTrue Positive Rate : 0.9881",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#false-positive-rate",
    "href": "chap03_knn.html#false-positive-rate",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.10 False Positive Rate",
    "text": "19.10 False Positive Rate\n\nfalse_positive_rate = FP / float(FP + TN)\n\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n\nFalse Positive Rate : 0.0357",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#specificity-true-negative-rate",
    "href": "chap03_knn.html#specificity-true-negative-rate",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.11 Specificity (True Negative Rate)",
    "text": "19.11 Specificity (True Negative Rate)\n\nspecificity = TN / (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))\n\nSpecificity : 0.9643",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#adjusting-the-classification-threshold-level",
    "href": "chap03_knn.html#adjusting-the-classification-threshold-level",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "19.12 Adjusting the Classification Threshold Level",
    "text": "19.12 Adjusting the Classification Threshold Level\n\n# print the first 10 predicted probabilities of two classes- 2 and 4\n\ny_pred_prob = knn.predict_proba(X_test)[0:10]\n\ny_pred_prob\n\narray([[1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.21219604, 0.78780396],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.40998626, 0.59001374]])\n\n\nObservations In each row, the numbers sum to 1. There are 2 columns which correspond to 2 classes - 2 and 4.\n\nClass 2 - predicted probability that there is benign cancer.\nClass 4 - predicted probability that there is malignant cancer.\n\nImportance of predicted probabilities\nWe can rank the observations by probability of benign or malignant cancer.\n\npredict_proba process\n   * Predicts the probabilities\n\n   * Choose the class with the highest probability\nClassification threshold level\n   * There is a classification threshold level of 0.5.\n\n   * Class 4 - probability of malignant cancer is predicted if probability &gt; 0.5.\n\n   * Class 2 - probability of benign cancer is predicted if probability &lt; 0.5.\n\n\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, \\\ncolumns=['Prob of - benign cancer (2)', 'Prob of - malignant cancer (4)'])\n\ny_pred_prob_df\n\n\n\n\n\n\n\n\nProb of - benign cancer (2)\nProb of - malignant cancer (4)\n\n\n\n\n0\n1.000000\n0.000000\n\n\n1\n1.000000\n0.000000\n\n\n2\n0.212196\n0.787804\n\n\n3\n1.000000\n0.000000\n\n\n4\n0.000000\n1.000000\n\n\n5\n1.000000\n0.000000\n\n\n6\n0.000000\n1.000000\n\n\n7\n1.000000\n0.000000\n\n\n8\n0.000000\n1.000000\n\n\n9\n0.409986\n0.590014\n\n\n\n\n\n\n\n\n# print the first 10 predicted probabilities for class 4 - Probability of malignant cancer\n\nknn.predict_proba(X_test)[0:10, 1]\n\narray([0.        , 0.        , 0.78780396, 0.        , 1.        ,\n       0.        , 1.        , 0.        , 1.        , 0.59001374])\n\n\n\n# store the predicted probabilities for class 4 - Probability of malignant cancer\n\ny_pred_1 = knn.predict_proba(X_test)[:, 1]\n\n\n# plot histogram of predicted probabilities\n\n# adjust figure size\nplt.figure(figsize=(6,4))\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n# plot histogram with 10 bins\nplt.hist(y_pred_1, bins = 10)\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities of malignant cancer')\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n# set the title\nplt.xlabel('Predicted probabilities of malignant cancer')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nObservations\n\nWe can see that the above histogram is positively skewed.\nThe first column tell us that there are approximately 80 observations with 0 * * * * probability of malignant cancer.\nThere are few observations with probability &gt; 0.5.\nSo, these few observations predict that there will be malignant cancer.\n\nComments\n\nIn binary problems, the threshold of 0.5 is used by default to convert predicted probabilities into class predictions.\nThreshold can be adjusted to increase sensitivity or specificity.\nSensitivity and specificity have an inverse relationship. Increasing one would always decrease the other and vice versa.\nAdjusting the threshold level should be one of the last step you do in the model-building process.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#roc-curve",
    "href": "chap03_knn.html#roc-curve",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "20.1 ROC Curve",
    "text": "20.1 ROC Curve\nAnother tool to measure the classification model performance visually is ROC Curve. ROC Curve stands for Receiver Operating Characteristic Curve. An ROC Curve is a plot which shows the performance of a classification model at various classification threshold levels.\nThe ROC Curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold levels. True Positive Rate (TPR) is also called Recall. It is defined as the ratio of TP to (TP + FN). False Positive Rate (FPR) is defined as the ratio of FP to (FP + TN).\nIn the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positive. It will increase both True Positives (TP) and False Positives (FP).\n\n# plot ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_1, pos_label=4)\n\nplt.figure(figsize=(6,4))\n\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\n\nplt.title('ROC curve for Breast Cancer kNN classifier')\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\n\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()\n\n\n\n\n\n\n\n\nROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap03_knn.html#roc-auc",
    "href": "chap03_knn.html#roc-auc",
    "title": "\nChapter # 03  K Nearest Neighbor (KNN) Algorithm",
    "section": "20.2 ROC AUC",
    "text": "20.2 ROC AUC\nROC AUC stands for Receiver Operating Characteristic - Area Under Curve. It is a technique to compare classifier performance. In this technique, we measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. So, ROC AUC is the percentage of the ROC plot that is underneath the curve.\n\n# compute ROC AUC\n\nfrom sklearn.metrics import roc_auc_score\n\nROC_AUC = roc_auc_score(y_test, y_pred_1)\n\nprint('ROC AUC : {:.4f}'.format(ROC_AUC))\n\nROC AUC : 0.9827\n\n\nInterpretation\n\nROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\nROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it is benign or malignant cancer.\n\n\nfrom sklearn.model_selection import cross_val_score\n\nCross_validated_ROC_AUC = \\\ncross_val_score(knn_7, X_train, y_train, cv=5,scoring='roc_auc').mean()\n\nprint('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))\n\nCross validated ROC AUC : 0.9910\n\n\nInterpretation\nOur Cross Validated ROC AUC is very close to 1. So, we can conclude that, the KNN classifier is indeed a very good model.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 03 <br> K Nearest Neighbor (KNN) Algorithm"
    ]
  },
  {
    "objectID": "chap04_logistic.html#loading-python-package",
    "href": "chap04_logistic.html#loading-python-package",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "2 Loading Python Package",
    "text": "2 Loading Python Package\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n# For Visualization\nsns.set(style = \"white\")\nsns.set(style = \"whitegrid\", color_codes = True)\n\nimport sklearn # For Machine Learning \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.version\n\nprint ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))\n\nThe Scikit-learn version that is used for this code file is 1.6.0",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#loading-dataset",
    "href": "chap04_logistic.html#loading-dataset",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "3 Loading Dataset",
    "text": "3 Loading Dataset\n\n# Importing Training Dataset \ntrain_df = pd.read_csv(\"DATA/train.csv\")\n# Importing Testing Dataset\ntest_df = pd.read_csv(\"DATA/test.csv\")\n\n\n3.1 Metadata of the Dataset\n\nprint(\"The total number of rows and columns in the dataset is {} and {} respectively.\".format(train_df.shape[0],train_df.shape[1]))\nprint (\"\\nThe names and the types of the variables of the dataset:\")\ntrain_df.info()\ntrain_df.head()\nprint(\"\\nThe types of the variables in the dataset are:\")\ntrain_df.dtypes\n\nThe total number of rows and columns in the dataset is 891 and 12 respectively.\n\nThe names and the types of the variables of the dataset:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\nThe types of the variables in the dataset are:\n\n\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\n\nfor x in train_df.columns:\n  print  (x)\ntrain_df['Pclass'].value_counts(sort = True, ascending = True)\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\nPclass\n2    184\n1    216\n3    491\nName: count, dtype: int64\n\n\n\ntrain_df['Sex'].value_counts()\n\nSex\nmale      577\nfemale    314\nName: count, dtype: int64",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#data-quality-missing-value-assessment",
    "href": "chap04_logistic.html#data-quality-missing-value-assessment",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "4 Data Quality & Missing Value Assessment",
    "text": "4 Data Quality & Missing Value Assessment\n\n# Missing value in Training Dataset \ntrain_df.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\n# Missing value in Testing Dataset \ntest_df.isnull().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\n\n4.1 Age - Missing Value\n\nprint ('The percent of missing \"Age\" record in the training dataset is %0.2f%%' %(train_df['Age'].isnull().sum()/train_df.shape[0]*100))\n\nThe percent of missing \"Age\" record in the training dataset is 19.87%\n\n\n\nprint ('The percent of missing \"Age\" record in testing dataset is %0.3f%%' %(test_df['Age'].isnull().sum()/test_df.shape[0]*100))\n\nThe percent of missing \"Age\" record in testing dataset is 20.574%\n\n\n\nplt.figure(figsize = (10,8))\ntrain_df['Age'].hist(bins = 15, density = True, color = 'teal', alpha = 0.60)\ntrain_df['Age'].plot(kind = 'density', color = 'teal', alpha = 0.60)\nplt.show()\n\n\n\n\n\n\n\n\nSince the variable Age is a little bit right skewed, using the mean to replace the missing observations might bias our results. Therefore, it is recommended that median be used to replace the missing observations.\n\nprint ('The mean of \"Age\" variable is %0.3f.' %(train_df['Age'].mean(skipna = True)))\n\nThe mean of \"Age\" variable is 29.699.\n\n\n\nprint ('The median of \"Age\" variable is %0.2f.' %(train_df['Age'].median(skipna = True)))\n\nThe median of \"Age\" variable is 28.00.\n\n\n\n\n4.2 Cabin - Missing Value\n\ntrain_df['Cabin'].value_counts()\n\nCabin\nG6             4\nC23 C25 C27    4\nB96 B98        4\nF2             3\nD              3\n              ..\nE17            1\nA24            1\nC50            1\nB42            1\nC148           1\nName: count, Length: 147, dtype: int64\n\n\n\nprint ('The percent of cabin variable missing value is %0.2f%%.' %(train_df['Cabin'].isnull().sum()/train_df.shape[0]*100))\n\nThe percent of cabin variable missing value is 77.10%.\n\n\n77% observations of the Cabin variable is missing. Therefore, it is better to prune the variable from the dataset. Moreover, the drop of the Cabin variable is justified because it has correlation with two other variables - Fare and Pclass.\n\n\n4.3 Embarked - Missing Value\n\ntrain_df['Embarked'].value_counts()\n\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\n\n# Percent of missing 'Embarked' Variable\nprint(\n    \"The percent of missing 'Embarked' records is %.2f%%\" %\n    (train_df['Embarked'].isnull().sum()/train_df.shape[0]*100)\n)\n\nThe percent of missing 'Embarked' records is 0.22%\n\n\nSince there are only 0.22% missing observation for Embarked, we can impute the missing values with the port where most people embarked.\n\nprint('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\nprint(train_df['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train_df, palette='Set2')\nplt.show()\n\nBoarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\nprint('The most common boarding port of embarkation is %s.' %train_df['Embarked'].value_counts().idxmax())\n\nThe most common boarding port of embarkation is S.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#final-adjustment-to-the-datasets-training-testing",
    "href": "chap04_logistic.html#final-adjustment-to-the-datasets-training-testing",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "5 Final Adjustment to the Datasets (Training & Testing)",
    "text": "5 Final Adjustment to the Datasets (Training & Testing)\nBased on the assessment of the missing values in the dataset, We will make the following changes to the data:\n\nThe missing value of Age variable will be imputed with 28 (median value Age)\nThe missing value of Embarked variable will be imputed with S (the most common boarding point)\nThere are many missing values for the variable Cabin; therefore, the variable will be dropped. Moreover, the drop will not affect the model as the variable is associated with two other variables - Pclass and Fare.\n\n\ntrain_data = train_df.copy()\ntrain_data['Age'].fillna(train_data['Age'].median(skipna = True), inplace = True)\ntrain_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace = True)\ntrain_data.drop(['Cabin'], axis = 1, inplace = True)\ntrain_data.isnull().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nEmbarked       0\ndtype: int64\n\n\n\ntrain_data.tail()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nEmbarked\n\n\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.00\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.00\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\n28.0\n1\n2\nW./C. 6607\n23.45\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.00\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.75\nQ\n\n\n\n\n\n\n\n\nplt.figure(figsize=(15,8))\n# Data with missing observations\nax = train_df['Age'].hist(bins = 15, density = True, stacked = True, color = 'teal', alpha = 0.6)\ntrain_df['Age'].plot(kind = 'density', color = 'teal')\n# Data without missing observations\nax = train_data['Age'].hist(bins = 15, density = True, stacked = True, color = 'orange', alpha = 0.6)\ntrain_data['Age'].plot(kind = 'density', color = 'orange')\nplt.xlim(-10,85)\nax.legend([\"Raw Age\", \"Adjusted Age\"])\nax.set(xlabel = 'Age')\nplt.show()\n\n\n\n\n\n\n\n\n\n5.1 Additional Variables\nThe variable SibSp means whether the passenger has sibling or spouse aboard and the variable Parch means whether the passenger has parents or children aboard. For the sake of simplicity and to account for multicollinearity, these two variables will be combined into a categorical variable: whether or not the individual was traveling alone.\n\n# Creating categorical variable for Traveling alone\ntrain_data['TravelAlone'] = np.where((train_data['SibSp']+train_data['Parch']) &gt; 0, 0, 1)\ntrain_data.drop('SibSp', axis = 1, inplace = True)\ntrain_data.drop('Parch', axis = 1, inplace = True)\n\nFor variables Pclass, Sex, and Embarked, categorical variables will be created\n\ntraining = pd.get_dummies(train_data, columns= [\"Pclass\", \"Embarked\", \"Sex\"])\ntraining.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Name         891 non-null    object \n 3   Age          891 non-null    float64\n 4   Ticket       891 non-null    object \n 5   Fare         891 non-null    float64\n 6   TravelAlone  891 non-null    int64  \n 7   Pclass_1     891 non-null    bool   \n 8   Pclass_2     891 non-null    bool   \n 9   Pclass_3     891 non-null    bool   \n 10  Embarked_C   891 non-null    bool   \n 11  Embarked_Q   891 non-null    bool   \n 12  Embarked_S   891 non-null    bool   \n 13  Sex_female   891 non-null    bool   \n 14  Sex_male     891 non-null    bool   \ndtypes: bool(8), float64(2), int64(3), object(2)\nmemory usage: 55.8+ KB\n\n\n\ntraining[['Pclass_1','Pclass_2','Pclass_3', 'Embarked_C','Embarked_Q','Embarked_S', 'Sex_female', 'Sex_male']].head()\n\n\n\n\n\n\n\n\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_female\nSex_male\n\n\n\n\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n3\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n4\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\ntraining.drop(['Sex_female', 'PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)\ntraining.tail()\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n886\n0\n27.0\n13.00\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n887\n1\n19.0\n30.00\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n888\n0\n28.0\n23.45\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n889\n1\n26.0\n30.00\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n890\n0\n32.0\n7.75\n1\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\ntraining.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Survived     891 non-null    int64  \n 1   Age          891 non-null    float64\n 2   Fare         891 non-null    float64\n 3   TravelAlone  891 non-null    int64  \n 4   Pclass_1     891 non-null    bool   \n 5   Pclass_2     891 non-null    bool   \n 6   Pclass_3     891 non-null    bool   \n 7   Embarked_C   891 non-null    bool   \n 8   Embarked_Q   891 non-null    bool   \n 9   Embarked_S   891 non-null    bool   \n 10  Sex_male     891 non-null    bool   \ndtypes: bool(7), float64(2), int64(2)\nmemory usage: 34.1 KB\n\n\n\nfinal_train = training\nfinal_train.tail()\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n886\n0\n27.0\n13.00\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\n\n\n887\n1\n19.0\n30.00\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n888\n0\n28.0\n23.45\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n889\n1\n26.0\n30.00\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n890\n0\n32.0\n7.75\n1\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\ntest_df.isna().sum()\n\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n\n\nNow we apply the same changes in the testing dataset.\n\nWe will apply to same imputation for Age in the Test data as we did for my Training data (if missing, Age = 28).\nWe will also remove the Cabin variable from the test data, as we’ve decided not to include it in my analysis.\nThere were no missing values in the Embarked port variable.\nWe will add the dummy variables to finalize the test set.\nFinally, We will impute the 1 missing value for Fare with the median, 14.45.\n\n\nprint('The median value of \"Fare\" variable in testing dataset is %0.3f.' %(train_df['Fare'].median(skipna = True)))\n\nThe median value of \"Fare\" variable in testing dataset is 14.454.\n\n\n\ntest_data = test_df.copy()\ntest_data['Age'].fillna(test_data['Age'].median(skipna = True), inplace = True)\ntest_data['Fare'].fillna(test_data['Fare'].median(skipna = True), inplace = True)\ntest_data.drop(['Cabin'], axis  = 1, inplace = True)\n\n\n\n5.2 Creating New Variables\n\n# Creating new variable - TravelAlone\ntest_data['TravelAlone']= np.where(test_data['SibSp']+test_data['Parch']&gt;0,0,1)\ntest_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\ntest_data.sample(5)\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nTicket\nFare\nEmbarked\nTravelAlone\n\n\n\n\n371\n1263\n1\nWilson, Miss. Helen Alice\nfemale\n31.00\n16966\n134.500\nC\n1\n\n\n186\n1078\n2\nPhillips, Miss. Alice Frances Louisa\nfemale\n21.00\nS.O./P.P. 2\n21.000\nS\n0\n\n\n180\n1072\n2\nMcCrie, Mr. James Matthew\nmale\n30.00\n233478\n13.000\nS\n1\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.50\nSOTON/O.Q. 3101262\n7.250\nS\n1\n\n\n354\n1246\n3\nDean, Miss. Elizabeth Gladys Millvina\"\"\nfemale\n0.17\nC.A. 2315\n20.575\nS\n0\n\n\n\n\n\n\n\n\n5.2.1 Creating the Dummies for Categorical Variables\n\ntesting = pd.get_dummies(test_data, columns = ['Pclass', 'Sex', 'Embarked'])\ntesting.drop(['Sex_female','PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)\ntesting.tail()\n\n\n\n\n\n\n\n\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n413\n27.0\n8.0500\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n414\n39.0\n108.9000\n1\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n415\n38.5\n7.2500\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n416\n27.0\n8.0500\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n417\n27.0\n22.3583\n0\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\nfinal_test = testing\nfinal_test.head()\n\n\n\n\n\n\n\n\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n34.5\n7.8292\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n1\n47.0\n7.0000\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n2\n62.0\n9.6875\n1\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n3\n27.0\n8.6625\n1\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\n\n\n4\n22.0\n12.2875\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#exploratory-data-analysis-eda",
    "href": "chap04_logistic.html#exploratory-data-analysis-eda",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "6 Exploratory Data Analysis (EDA)",
    "text": "6 Exploratory Data Analysis (EDA)\n\n6.1 Exploration of Age Variable\n\nfinal_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Survived     891 non-null    int64  \n 1   Age          891 non-null    float64\n 2   Fare         891 non-null    float64\n 3   TravelAlone  891 non-null    int64  \n 4   Pclass_1     891 non-null    bool   \n 5   Pclass_2     891 non-null    bool   \n 6   Pclass_3     891 non-null    bool   \n 7   Embarked_C   891 non-null    bool   \n 8   Embarked_Q   891 non-null    bool   \n 9   Embarked_S   891 non-null    bool   \n 10  Sex_male     891 non-null    bool   \ndtypes: bool(7), float64(2), int64(2)\nmemory usage: 34.1 KB\n\n\n\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Age'][final_train.Survived == 1], shade=True, color = 'darkturquoise')\nsns.kdeplot(final_train['Age'][final_train.Survived == 0], shade=True, color = 'lightcoral')\nax.legend(['Survived', 'Died']) # or you can use plt.legend(['Survived', 'Died])\nplt.title('Density Plot for Surviving Population and Deceased Population')\nplt.xlabel('Age') # or you can use ax.set(xlabel = 'Age')\nplt.xlim(-10,85)\nplt.show()\n\n\n\n\n\n\n\n\nThe age distribution for survivors and deceased is actually very similar. One notable difference is that, of the survivors, a larger proportion were children. The passengers evidently made an attempt to save children by giving them a place on the life rafts.\n\navg_age = final_train.groupby(['Survived']) ['Age'].mean()\navg_age.to_frame().reset_index()\n\n\n\n\n\n\n\n\nSurvived\nAge\n\n\n\n\n0\n0\n30.028233\n\n\n1\n1\n28.291433\n\n\n\n\n\n\n\n\nsns.boxplot(data = final_train, x  = 'Survived', y = 'Age', palette='Set2')\nplt.title(\"Comparison of Age of Passengers Conditioned on Survived\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating a Dummy Variable IsMinor\nfinal_train['IsMinor'] = np.where(final_train['Age'] &lt;= 16, 1, 0)\nfinal_test['IsMinor'] = np.where(final_test['Age'] &lt;= 16, 1, 0)\n\n\n\n6.2 Exploration of Fare Variable\n\ntrain_df.groupby(['Survived']) ['Fare'].mean().to_frame().reset_index()\n\n\n\n\n\n\n\n\nSurvived\nFare\n\n\n\n\n0\n0\n22.117887\n\n\n1\n1\n48.395408\n\n\n\n\n\n\n\n\nsns.boxplot(data = final_train, x = 'Survived', y = 'Fare', palette='Set2')\nplt.ylim(0, 100)\nplt.title('Comparison of Fare of Passengers Conditioned on Survived')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train['Fare'][final_train.Survived == 1],shade=True, color='darkturquoise')\nsns.kdeplot(final_train['Fare'][final_train.Survived==0], shade=True, color='lightcoral')\nax.legend(['Survived', 'Died'])\nax.set(xlabel= 'Fare')\nplt.xlim(-20,200)\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nplt.show()\n\n\n\n\n\n\n\n\nAs the distributions are clearly different for the fares of survivors vs. deceased, it’s likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class, which we’ll look at next.\n\n# Pair Plot of two continuous variables (Age and Fare)\nplt.figure(figsize=(15,8))\nsns.pairplot(data=train_data, hue='Survived', vars= ['Age', 'Fare'])\nplt.show()\n\n&lt;Figure size 1440x768 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n6.3 Exploration of PClass Variable\n\nsns.barplot(data = train_df, x = 'Pclass', y = 'Survived', color='darkturquoise')\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, first class passengers were more likely to survive.\n\n\n6.4 Exploration of Embarked Variable\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=train_df, color=\"teal\")\nplt.show()\n\n\n\n\n\n\n\n\nPassengers who boarded in Cherbourg, France, appear to have the highest survival rate. Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown. This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck). It’s also worth noting the size of the whiskers in these plots. Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest. The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker. It’s possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts.\n\n\n6.5 Exploration of TravelAlone Variable\n\nsns.barplot(x = 'TravelAlone', y = 'Survived', data=final_train, color=\"mediumturquoise\")\nplt.xlabel('Travel Alone')\nplt.show()\n\n\n\n\n\n\n\n\nIndividuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it’s likely that individuals traveling alone were likely male.\n\n\n6.6 Exploration of Gender Variable\n\nsns.barplot(x = 'Sex', y = 'Survived', data=train_df, color=\"aquamarine\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.7 Chi-square Test of Independence\n\n6.7.1 Chi-square Test of Independence betweeen Survived and Sex\n\npd.crosstab(train_df['Survived'], train_df['Sex'])\n\n\n\n\n\n\n\nSex\nfemale\nmale\n\n\nSurvived\n\n\n\n\n\n\n0\n81\n468\n\n\n1\n233\n109\n\n\n\n\n\n\n\n\n# Importing scipy package \nfrom scipy import stats\n\n\nstats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\n\nChi2ContingencyResult(statistic=np.float64(260.71702016732104), pvalue=np.float64(1.1973570627755645e-58), dof=1, expected_freq=array([[193.47474747, 355.52525253],\n       [120.52525253, 221.47474747]]))\n\n\n\nchi2_stat, p, dof, expected = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))\nprint(f\"chi2 statistic:     {chi2_stat:.5g}\")\nprint(f\"p-value:            {p:.5g}\")\nprint(f\"degrees of freedom: {dof}\")\nprint(\"expected frequencies:\\n\",expected)\n\nchi2 statistic:     260.72\np-value:            1.1974e-58\ndegrees of freedom: 1\nexpected frequencies:\n [[193.47474747 355.52525253]\n [120.52525253 221.47474747]]\n\n\n\n\n6.7.2 Chi-square Test of Independence betweeen Survived and Pclass\n\nchi2_stat_2, p_2, dof_2, expected_2 = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Pclass']))\nprint(f\"chi2 statistic:     {chi2_stat_2:.5g}\")\nprint(f\"p-value:            {p_2:.5g}\")\nprint(f\"degrees of freedom: {dof_2}\")\nprint(\"expected frequencies:\\n\",expected_2)\n\nchi2 statistic:     102.89\np-value:            4.5493e-23\ndegrees of freedom: 2\nexpected frequencies:\n [[133.09090909 113.37373737 302.53535354]\n [ 82.90909091  70.62626263 188.46464646]]\n\n\n\n\n\n6.8 Post Hoc Analysis of Pclass\nThe explanation of Post Hoc analysis is given in this link.\n\npclass_cross = pd.crosstab(train_df['Survived'], train_df['Pclass'])\npclass_cross\n\n\n\n\n\n\n\nPclass\n1\n2\n3\n\n\nSurvived\n\n\n\n\n\n\n\n0\n80\n97\n372\n\n\n1\n136\n87\n119\n\n\n\n\n\n\n\n\nimport gc\nfrom itertools import combinations\nimport scipy.stats\nimport statsmodels.stats.multicomp as multi\nfrom statsmodels.stats.multitest import multipletests\n\n\np_vals_chi = []\npairs_of_class = list(combinations(train_df['Pclass'].unique(),2))\n\nfor each_pair in pairs_of_class:\n    each_df = train_df[(train_df['Pclass']==each_pair[0]) | (train_df['Pclass']==each_pair[1])]\n    p_vals_chi.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Pclass']))[1]\n         )\n\n\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of class',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of class'] = pairs_of_class\nbonferroni_results['original p value'] = p_vals_chi\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni = multipletests(p_vals_chi, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni[0]\nbonferroni_results.head()\n\n\n\n\n\n\n\n\npair of class\noriginal p value\ncorrected p value\nReject Null?\n\n\n\n\n0\n(3, 1)\n1.212338e-22\n3.637013e-22\nTrue\n\n\n1\n(3, 2)\n1.224965e-08\n3.674894e-08\nTrue\n\n\n2\n(1, 2)\n2.319805e-03\n6.959414e-03\nTrue\n\n\n\n\n\n\n\n\n\n6.9 Post Hoc Analysis of Embarked\n\n# Write code Here \np_vals_chi_embark = []\npairs_of_embark = list(combinations(train_data['Embarked'].unique(),2))\n\nfor each_pair in pairs_of_embark:\n    each_df = train_data[(train_data['Embarked']==each_pair[0]) | (train_data['Embarked']==each_pair[1])]\n    p_vals_chi_embark.append(\\\n          scipy.stats.chi2_contingency(\n            pd.crosstab(each_df['Survived'], each_df['Embarked']))[1]\n         )\n\n\n#Write code Here\n#Results of Bonferroni Adjustment\nbonferroni_results = pd.DataFrame(columns=['pair of embark',\\\n                                           'original p value',\\\n                                           'corrected p value',\\\n                                           'Reject Null?'])\n\nbonferroni_results['pair of embark'] = pairs_of_embark\nbonferroni_results['original p value'] = p_vals_chi_embark\n\n#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.\nmulti_test_results_bonferroni_embark = multipletests(p_vals_chi_embark, method='bonferroni')\n\nbonferroni_results['corrected p value'] = multi_test_results_bonferroni_embark[1]\nbonferroni_results['Reject Null?'] = multi_test_results_bonferroni_embark[0]\nbonferroni_results.head()\n\n\n\n\n\n\n\n\npair of embark\noriginal p value\ncorrected p value\nReject Null?\n\n\n\n\n0\n(S, C)\n5.537903e-07\n0.000002\nTrue\n\n\n1\n(S, Q)\n4.493936e-01\n1.000000\nFalse\n\n\n2\n(C, Q)\n2.475540e-02\n0.074266\nFalse",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#logistic-regression-results",
    "href": "chap04_logistic.html#logistic-regression-results",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "7 Logistic Regression & Results",
    "text": "7 Logistic Regression & Results\n\n7.1 Feature Selection\n\n7.1.1 Recursive Feature Selection (RFE)\nGiven an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\n# Our all training dataset \ntrain_df\ntrain_data # impute missing values \ntraining # create the dummies \nfinal_train\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\nTravelAlone\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\nIsMinor\n\n\n\n\n0\n0\n22.0\n7.2500\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n0\n\n\n1\n1\n38.0\n71.2833\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n0\n\n\n2\n1\n26.0\n7.9250\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n0\n\n\n3\n1\n35.0\n53.1000\n0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n0\n\n\n4\n0\n35.0\n8.0500\n1\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n27.0\n13.0000\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\n0\n\n\n887\n1\n19.0\n30.0000\n1\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n0\n\n\n888\n0\n28.0\n23.4500\n0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n0\n\n\n889\n1\n26.0\n30.0000\n1\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n0\n\n\n890\n0\n32.0\n7.7500\n1\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n0\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ncols = ['Age', 'Fare', 'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX = final_train[cols] # features vector\ny = final_train['Survived'] # Target vector \n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\n\n\n# Build the model \nmodel = LogisticRegression()\n# Create the RFE model \nrfe = RFE(estimator=model, n_features_to_select=8)\nrfe = rfe.fit(X ,y)\ndir(rfe)\n\n['__abstractmethods__',\n '__annotations__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__firstlineno__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__sklearn_clone__',\n '__sklearn_tags__',\n '__static_attributes__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_abc_impl',\n '_build_request_for_signature',\n '_check_feature_names',\n '_check_n_features',\n '_doc_link_module',\n '_doc_link_template',\n '_doc_link_url_param_generator',\n '_estimator_type',\n '_fit',\n '_get_default_requests',\n '_get_doc_link',\n '_get_metadata_request',\n '_get_param_names',\n '_get_support_mask',\n '_get_tags',\n '_more_tags',\n '_parameter_constraints',\n '_repr_html_',\n '_repr_html_inner',\n '_repr_mimebundle_',\n '_sklearn_auto_wrap_output_keys',\n '_transform',\n '_validate_data',\n '_validate_params',\n 'classes_',\n 'decision_function',\n 'estimator',\n 'estimator_',\n 'feature_names_in_',\n 'fit',\n 'fit_transform',\n 'get_feature_names_out',\n 'get_metadata_routing',\n 'get_params',\n 'get_support',\n 'importance_getter',\n 'inverse_transform',\n 'n_features_',\n 'n_features_in_',\n 'n_features_to_select',\n 'predict',\n 'predict_log_proba',\n 'predict_proba',\n 'ranking_',\n 'score',\n 'set_output',\n 'set_params',\n 'step',\n 'support_',\n 'transform',\n 'verbose']\n\n\n\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))\n\nSelected features: ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_Q', 'Sex_male', 'IsMinor']\n\n\n\n# Getting the Regression Results \nimport statsmodels.api as sm\nlogit_model = sm.Logit(y,X.astype(float))\nresult = logit_model.fit()\nprint(result.summary2())\n\nOptimization terminated successfully.\n         Current function value: 0.448075\n         Iterations 6\n                         Results: Logit\n=================================================================\nModel:              Logit            Method:           MLE       \nDependent Variable: Survived         Pseudo R-squared: 0.327     \nDate:               2025-01-12 11:25 AIC:              816.4704  \nNo. Observations:   891              BIC:              859.6015  \nDf Model:           8                Log-Likelihood:   -399.24   \nDf Residuals:       882              LL-Null:          -593.33   \nConverged:          1.0000           LLR p-value:      6.3002e-79\nNo. Iterations:     6.0000           Scale:            1.0000    \n------------------------------------------------------------------\n              Coef.   Std.Err.     z      P&gt;|z|    [0.025   0.975]\n------------------------------------------------------------------\nAge          -0.0121    0.0057   -2.1381  0.0325  -0.0232  -0.0010\nFare          0.0015    0.0022    0.6878  0.4916  -0.0028   0.0058\nTravelAlone   0.2803    0.1936    1.4475  0.1477  -0.0992   0.6597\nPclass_1      2.1646    0.2838    7.6272  0.0000   1.6083   2.7208\nPclass_2      1.3948    0.2309    6.0399  0.0000   0.9422   1.8473\nEmbarked_C    0.6014    0.2332    2.5790  0.0099   0.1443   1.0584\nEmbarked_Q    0.6366    0.3139    2.0282  0.0425   0.0214   1.2517\nSex_male     -2.5527    0.1955  -13.0567  0.0000  -2.9359  -2.1695\nIsMinor       0.9987    0.2737    3.6488  0.0003   0.4623   1.5352\n=================================================================\n\n\n\n\ncols2 = ['Age',  'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']\nX_alt = final_train[cols2] # features vector\nlogit_model2 = sm.Logit(y,sm.add_constant(X_alt.astype(float)))\nresult2 = logit_model2.fit()\nprint(result2.summary2())\n\nOptimization terminated successfully.\n         Current function value: 0.446363\n         Iterations 6\n                         Results: Logit\n=================================================================\nModel:              Logit            Method:           MLE       \nDependent Variable: Survived         Pseudo R-squared: 0.330     \nDate:               2025-01-12 11:25 AIC:              813.4192  \nNo. Observations:   891              BIC:              856.5503  \nDf Model:           8                Log-Likelihood:   -397.71   \nDf Residuals:       882              LL-Null:          -593.33   \nConverged:          1.0000           LLR p-value:      1.4027e-79\nNo. Iterations:     6.0000           Scale:            1.0000    \n------------------------------------------------------------------\n              Coef.   Std.Err.     z      P&gt;|z|    [0.025   0.975]\n------------------------------------------------------------------\nconst         0.6181    0.3319    1.8623  0.0626  -0.0324   1.2686\nAge          -0.0245    0.0091   -2.7055  0.0068  -0.0423  -0.0068\nTravelAlone   0.1475    0.1973    0.7476  0.4547  -0.2392   0.5343\nPclass_1      2.2826    0.2546    8.9666  0.0000   1.7837   2.7815\nPclass_2      1.3380    0.2340    5.7178  0.0000   0.8793   1.7966\nEmbarked_C    0.5518    0.2351    2.3471  0.0189   0.0910   1.0126\nEmbarked_Q    0.5323    0.3217    1.6548  0.0980  -0.0982   1.1627\nSex_male     -2.6141    0.1973  -13.2482  0.0000  -3.0009  -2.2274\nIsMinor       0.5957    0.3560    1.6735  0.0942  -0.1020   1.2935\n=================================================================\n\n\n\n\nfrom stargazer.stargazer import Stargazer\ntitanic_logit = Stargazer([result, result2])\ntitanic_logit\n\n\nDependent variable: Survived(1)(2)\n\n\nAge-0.012**-0.025***\n(0.006)(0.009)\nEmbarked_C0.601***0.552**\n(0.233)(0.235)\nEmbarked_Q0.637**0.532*\n(0.314)(0.322)\nFare0.002\n(0.002)\nIsMinor0.999***0.596*\n(0.274)(0.356)\nPclass_12.165***2.283***\n(0.284)(0.255)\nPclass_21.395***1.338***\n(0.231)(0.234)\nSex_male-2.553***-2.614***\n(0.196)(0.197)\nTravelAlone0.2800.148\n(0.194)(0.197)\nconst0.618*\n(0.332)\n\n\nObservations891891Pseudo R20.3270.330\nNote:*p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n# Interpreting the coefficients, which are the log odds; therefore, we need to convert them into odds ratio. \nnp.exp(-2.5527)\n\nnp.float64(0.0778711298546485)\n\n\n\nnp.exp(result.params) # Getting the Odds Ratio of all Features \n\nAge            0.987957\nFare           1.001519\nTravelAlone    1.323475\nPclass_1       8.710827\nPclass_2       4.033966\nEmbarked_C     1.824624\nEmbarked_Q     1.889992\nSex_male       0.077872\nIsMinor        2.714860\ndtype: float64\n\n\n\n\n7.1.2 Feature Ranking with Recursive Feature Elimination and Cross-validation (RFECV)\nRFECV performs RFE in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation\n\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(max_iter = 5000), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\nfor x in dir(rfecv):\n  print(x)\n\n_RFECV__metadata_request__fit\n__abstractmethods__\n__annotations__\n__class__\n__delattr__\n__dict__\n__dir__\n__doc__\n__eq__\n__firstlineno__\n__format__\n__ge__\n__getattribute__\n__getstate__\n__gt__\n__hash__\n__init__\n__init_subclass__\n__le__\n__lt__\n__module__\n__ne__\n__new__\n__reduce__\n__reduce_ex__\n__repr__\n__setattr__\n__setstate__\n__sizeof__\n__sklearn_clone__\n__sklearn_tags__\n__static_attributes__\n__str__\n__subclasshook__\n__weakref__\n_abc_impl\n_build_request_for_signature\n_check_feature_names\n_check_n_features\n_doc_link_module\n_doc_link_template\n_doc_link_url_param_generator\n_estimator_type\n_fit\n_get_default_requests\n_get_doc_link\n_get_metadata_request\n_get_param_names\n_get_scorer\n_get_support_mask\n_get_tags\n_more_tags\n_parameter_constraints\n_repr_html_\n_repr_html_inner\n_repr_mimebundle_\n_sklearn_auto_wrap_output_keys\n_transform\n_validate_data\n_validate_params\nclasses_\ncv\ncv_results_\ndecision_function\nestimator\nestimator_\nfeature_names_in_\nfit\nfit_transform\nget_feature_names_out\nget_metadata_routing\nget_params\nget_support\nimportance_getter\ninverse_transform\nmin_features_to_select\nn_features_\nn_features_in_\nn_jobs\npredict\npredict_log_proba\npredict_proba\nranking_\nscore\nscoring\nset_output\nset_params\nstep\nsupport_\ntransform\nverbose\n\n\n\n# To get the accuracy\nrfecv.cv_results_\n\n{'mean_test_score': array([0.78672909, 0.78672909, 0.78672909, 0.79009988, 0.78560549,\n        0.78561798, 0.78790262, 0.79574282, 0.79463171]),\n 'std_test_score': array([0.02859935, 0.02859935, 0.02859935, 0.02989151, 0.02604354,\n        0.02546772, 0.03370206, 0.02689652, 0.02691683]),\n 'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}\n\n\n\ntype(rfecv.cv_results_)\n\ndict\n\n\n\nrfecv.cv_results_\n\n{'mean_test_score': array([0.78672909, 0.78672909, 0.78672909, 0.79009988, 0.78560549,\n        0.78561798, 0.78790262, 0.79574282, 0.79463171]),\n 'std_test_score': array([0.02859935, 0.02859935, 0.02859935, 0.02989151, 0.02604354,\n        0.02546772, 0.03370206, 0.02689652, 0.02691683]),\n 'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}\n\n\n\nrfecv.n_features_\n\nnp.int64(8)\n\n\n\nlist(X.columns[rfecv.support_])\n\n['Age',\n 'TravelAlone',\n 'Pclass_1',\n 'Pclass_2',\n 'Embarked_C',\n 'Embarked_Q',\n 'Sex_male',\n 'IsMinor']\n\n\n\n# To check whether the RFE and RFECV generate the same features\nset(list(X.columns[rfecv.support_])) == set(list((X.columns[rfe.support_])))  \n\nTrue\n\n\n\nrfecv.cv_results_.keys()\n\ndict_keys(['mean_test_score', 'std_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'split5_test_score', 'split6_test_score', 'split7_test_score', 'split8_test_score', 'split9_test_score', 'n_features'])\n\n\n\ncrossVal_results = rfecv.cv_results_\ncrossVal_results\ndel crossVal_results['mean_test_score']\ndel crossVal_results['std_test_score']\ncrossVal_results\n\n{'split0_test_score': array([0.81111111, 0.81111111, 0.81111111, 0.81111111, 0.81111111,\n        0.8       , 0.76666667, 0.78888889, 0.77777778]),\n 'split1_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.76404494,\n        0.75280899, 0.76404494, 0.79775281, 0.79775281]),\n 'split2_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.78651685, 0.7752809 , 0.7752809 , 0.7752809 ]),\n 'split3_test_score': array([0.84269663, 0.84269663, 0.84269663, 0.85393258, 0.83146067,\n        0.83146067, 0.84269663, 0.85393258, 0.85393258]),\n 'split4_test_score': array([0.79775281, 0.79775281, 0.79775281, 0.79775281, 0.79775281,\n        0.79775281, 0.79775281, 0.7752809 , 0.7752809 ]),\n 'split5_test_score': array([0.7752809 , 0.7752809 , 0.7752809 , 0.78651685, 0.78651685,\n        0.78651685, 0.78651685, 0.78651685, 0.7752809 ]),\n 'split6_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.76404494, 0.76404494,\n        0.76404494, 0.76404494, 0.76404494, 0.7752809 ]),\n 'split7_test_score': array([0.74157303, 0.74157303, 0.74157303, 0.74157303, 0.74157303,\n        0.74157303, 0.74157303, 0.7752809 , 0.7752809 ]),\n 'split8_test_score': array([0.80898876, 0.80898876, 0.80898876, 0.80898876, 0.80898876,\n        0.80898876, 0.85393258, 0.83146067, 0.83146067]),\n 'split9_test_score': array([0.76404494, 0.76404494, 0.76404494, 0.7752809 , 0.78651685,\n        0.78651685, 0.78651685, 0.80898876, 0.80898876]),\n 'n_features': array([1, 2, 3, 4, 5, 6, 7, 8, 9])}",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#correlation-matrix",
    "href": "chap04_logistic.html#correlation-matrix",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "8 Correlation Matrix",
    "text": "8 Correlation Matrix\n\nSelected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n                     'Embarked_S', 'Sex_male', 'IsMinor']\nX = final_train[Selected_features] # Recreated features vector \n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\") # for cmap = 'viridis' can also be used.\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(final_train[['Survived', 'Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_S', 'Sex_male', 'IsMinor']].corr(), annot = True, cmap = 'viridis')\nplt.show()",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#model-evaluation-procedures",
    "href": "chap04_logistic.html#model-evaluation-procedures",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "9 Model Evaluation Procedures",
    "text": "9 Model Evaluation Procedures\n\n9.1 Model Evaluation Based on Train/Test Split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n\nX = final_train[Selected_features]\ny = final_train['Survived']\n\n\n# use train/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n\n\n# Check classification scores of Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test) # Prediction actual class\ny_pred_proba = logreg.predict_proba(X_test) [:,1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba)) \nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr)) \n\nTrain/Test split results:\nLogisticRegression accuracy is 0.810\nLogisticRegression log_loss is 0.446\nLogisticRegression auc is 0.847\n\n\n\n\n9.2 Confusion Matrix\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n\nConfusion matrix\n\n [[98 13]\n [21 47]]\n\nTrue Positives(TP) =  98\n\nTrue Negatives(TN) =  47\n\nFalse Positives(FP) =  13\n\nFalse Negatives(FN) =  21\n\n\n\n# Visualizing Confusion Matrix\nplt.figure(figsize=(6,4))\ncm_matrix = pd.DataFrame(data= cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.3 Classification Report\n\nprint (classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.82      0.88      0.85       111\n           1       0.78      0.69      0.73        68\n\n    accuracy                           0.81       179\n   macro avg       0.80      0.79      0.79       179\nweighted avg       0.81      0.81      0.81       179\n\n\n\n\n\n9.4 ROC-AUC Curve\n\nidx = np.min(np.where(tpr &gt; 0.95)) # index of the first threshold for which the sensibility (true positive rate (tpr)) &gt; 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall/sensitivity)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))\n\n\n\n\n\n\n\n\nUsing a threshold of 0.087 guarantees a sensitivity of 0.956 and a specificity of 0.225, i.e. a false positive rate of 77.48%.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#model-evaluation-based-on-k-fold-cross-validation-cross_val_score-function",
    "href": "chap04_logistic.html#model-evaluation-based-on-k-fold-cross-validation-cross_val_score-function",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "10 Model Evaluation Based on K-fold Cross-validation cross_val_score() Function",
    "text": "10 Model Evaluation Based on K-fold Cross-validation cross_val_score() Function\n\n# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression(max_iter=5000)\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())\n\nK-fold cross-validation results:\nLogisticRegression average accuracy is 0.796\nLogisticRegression average log_loss is 0.454\nLogisticRegression average auc is 0.850\n\n\n\n10.1 Model Evaluation Based on K-fold Cross-validation Using cross_validate() Function\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std())) \n\nK-fold cross-validation results:\nLogisticRegression average accuracy: 0.796 (+/-0.024)\nLogisticRegression average log_loss: 0.454 (+/-0.037)\nLogisticRegression average auc: 0.850 (+/-0.028)\n\n\nWhat happens when we add the feature Fare -\n\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"]\nX = final_train[cols]\n\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n\nmodelCV = LogisticRegression(max_iter=5000)\n\nresults = cross_validate(modelCV, final_train[cols], y, cv=10, scoring=list(scoring.values()), \n                         return_train_score=False)\n\nprint('K-fold cross-validation results:')\nfor sc in range(len(scoring)):\n    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n                               if list(scoring.values())[sc]=='neg_log_loss' \n                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n                               results['test_%s' % list(scoring.values())[sc]].std()))\n\nK-fold cross-validation results:\nLogisticRegression average accuracy: 0.795 (+/-0.027)\nLogisticRegression average log_loss: 0.455 (+/-0.037)\nLogisticRegression average auc: 0.849 (+/-0.028)\n\n\nWe notice that the model is slightly deteriorated. The Fare variable does not carry any useful information. Its presence is just a noise for the logistic regression model.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#gridsearchcv-evaluating-using-multiple-scorers-simultaneously",
    "href": "chap04_logistic.html#gridsearchcv-evaluating-using-multiple-scorers-simultaneously",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "11 GridSearchCV Evaluating Using Multiple Scorers Simultaneously",
    "text": "11 GridSearchCV Evaluating Using Multiple Scorers Simultaneously\n\nfrom sklearn.model_selection import GridSearchCV\n\nX = final_train[Selected_features]\n\nparam_grid = {'C': np.arange(1e-05, 3, 0.1)}\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n\ngs = GridSearchCV(LogisticRegression(max_iter=5000), return_train_score=True,\n                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n\ngs.fit(X, y)\nresults = gs.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(gs.best_estimator_))\nprint(\"best params: \" + str(gs.best_params_))\nprint('best score:', gs.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, param_grid['C'].max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n\n====================\nbest params: LogisticRegression(C=np.float64(2.50001), max_iter=5000)\nbest params: {'C': np.float64(2.50001)}\nbest score: 0.8069662921348316\n====================\n\n\n\n\n\n\n\n\n\n\n11.1 GridSearchCV Evaluating Using Multiple scorers, RepeatedStratifiedKFold and pipeline for Preprocessing Simultaneously\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\n\n#Define simple model\n###############################################################################\nC = np.arange(1e-05, 5.5, 0.1)\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\nlog_reg = LogisticRegression(max_iter=5000)\n\n#Simple pre-processing estimators\n###############################################################################\nstd_scale = StandardScaler(with_mean=False, with_std=False)\n#std_scale = StandardScaler()\n\n#Defining the CV method: Using the Repeated Stratified K Fold\n###############################################################################\n\nn_folds=5\nn_repeats=5\n\nrskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)\n\n#Creating simple pipeline and defining the gridsearch\n###############################################################################\n\nlog_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])\n\nlog_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,\n              scoring=scoring, return_train_score=True,\n              param_grid=dict(clf__C=C), refit='Accuracy')\n\nlog_clf.fit(X, y)\nresults = log_clf.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(log_clf.best_estimator_))\nprint(\"best params: \" + str(log_clf.best_params_))\nprint('best score:', log_clf.best_score_)\nprint('='*20)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n\nplt.xlabel(\"Inverse of regularization strength: C\")\nplt.ylabel(\"Score\")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, C.max()) \nax.set_ylim(0.35, 0.95)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results['param_clf__C'].data, dtype=float)\n\nfor scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n        \n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show() \n\n====================\nbest params: Pipeline(steps=[('scale', StandardScaler(with_mean=False, with_std=False)),\n                ('clf',\n                 LogisticRegression(C=np.float64(4.90001), max_iter=5000))])\nbest params: {'clf__C': np.float64(4.90001)}\nbest score: 0.7995505617977527\n====================",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#regularization",
    "href": "chap04_logistic.html#regularization",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "12 Regularization",
    "text": "12 Regularization\nRegularization is a method of preventing overfitting, which is a common problem in machine learning. Overfitting means that your model learns too much from the specific data you have, and fails to generalize well to new or unseen data. This can lead to poor predictions and low performance. Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables.\nSince the size of each coefficient depends on the scale of its corresponding variable, scaling the data is required so that the regularization penalizes each variable equally. The regularization strength is determined by C and as C increases, the regularization term becomes smaller (and for extremely large C values, it’s as if there is no regularization at all).\nIf the initial model is overfit (as in, it fits the training data too well), then adding a strong regularization term (with small C value) makes the model perform worse for the training data, but introducing such “noise” improves the model’s performance on unseen (or test) data.\nAn example with 1000 samples and 200 features shown below. As can be seen from the plot of accuracy over different values of C, if C is large (with very little regularization), there is a big gap between how the model performs on training data and test data. However, as C decreases, the model performs worse on training data but performs better on test data (test accuracy increases). However, when C becomes too small (or the regularization becomes too strong), the model begins performing worse again because now the regularization term completely dominates the objective function.\n\n# Necessary Python Packages \nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n# make sample data\nX, y = make_classification(1000, 200, n_informative=195, random_state=2023)\n# split into train-test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2023)\n\n# normalize the data\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# train Logistic Regression models for different values of C\n# and collect train and test accuracies\nscores = {}\nfor C in (10**k for k in range(-6, 6)):\n    lr = LogisticRegression(C=C)\n    lr.fit(X_train, y_train)\n    scores[C] = {'train accuracy': lr.score(X_train, y_train), \n                 'test accuracy': lr.score(X_test, y_test)}\n\n# plot the accuracy scores for different values of C\npd.DataFrame.from_dict(scores, 'index').plot(logx=True, xlabel='C', ylabel='accuracy')\n\n\n\n\n\n\n\n\n\n12.1 Types of Regularization\n\n12.1.1 L1 regularization\nL1 regularization, also known as Lasso regularization, adds the sum of the absolute values of the model’s coefficients to the loss function. It encourages sparsity in the model by shrinking some coefficients to precisely zero. This has the effect of performing feature selection, as the model can effectively ignore irrelevant or less important features. L1 regularization is particularly useful when dealing with high-dimensional datasets with desired feature selection.\nMathematically, the L1 regularization term can be written as:\nL1 regularization = λ * Σ|wi|\nHere, λ is the regularization parameter that controls the strength of regularization, wi represents the individual model coefficients and the sum is taken over all coefficients.\n\n\n12.1.2 L2 regularization\nL2 regularization, also known as Ridge regularization, adds the sum of the squared values of the model’s coefficients to the loss function. Unlike L1 regularization, L2 regularization does not force the coefficients to be exactly zero but instead encourages them to be small. L2 regularization can prevent overfitting by spreading the influence of a single feature across multiple features. It is advantageous when there are correlations between the input features.\nMathematically, the L2 regularization term can be written as:\nL2 regularization = λ * Σ(wi^2)\nSimilar to L1 regularization, λ is the regularization parameter, and wi represents the model coefficients. The sum is taken over all coefficients, and the squares of the coefficients are summed.\nThe choice between L1 and L2 regularization depends on the specific problem and the characteristics of the data. For example, L1 regularization produces sparse models, which can be advantageous when feature selection is desired. L2 regularization, on the other hand, encourages small but non-zero coefficients and can be more suitable when there are strong correlations between features.",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap04_logistic.html#conclusion",
    "href": "chap04_logistic.html#conclusion",
    "title": "\nChapter # 04  Logistic Regression",
    "section": "13 Conclusion",
    "text": "13 Conclusion",
    "crumbs": [
      "Modules & Slides",
      "Supervised ML Algorithm",
      "<center> Chapter # 04 <br> Logistic Regression"
    ]
  },
  {
    "objectID": "chap09_pca.html",
    "href": "chap09_pca.html",
    "title": "\nChapter # 09  Dimension Reduction & PCA",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Modules & Slides",
      "Unsupervised ML Algorithm",
      "<center> Chapter # 09 <br> Dimension Reduction & PCA"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nMachine Learning in Business",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/datetimes/test_indexing.html",
    "href": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/datetimes/test_indexing.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/period/test_indexing.html",
    "href": "mlbusiness2/Lib/site-packages/pandas/tests/indexes/period/test_indexing.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mlbusiness2/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "href": "mlbusiness2/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "title": "Machine Learning in Business",
    "section": "",
    "text": "Copyright (c) 2012-2023, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "other-materials.html",
    "href": "other-materials.html",
    "title": "Other Materials",
    "section": "",
    "text": "Other materials of the course will be available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The project files for the course are available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Readings",
    "section": "",
    "text": "The readings for the course are available here.\n\n\n\n Back to top"
  },
  {
    "objectID": "r_eda.html",
    "href": "r_eda.html",
    "title": "\nExploratory Data Analysis in R",
    "section": "",
    "text": "The objective of this document is to introduce the necessary functions from tidyverse package for data manipulation and data visualization. There are basically six functions - select(), filter(), mutate(), arrange(), group_by(), and summarize() - from dplyr package of tidyverse ecosystem that are very much necessary for data manipulation. These six functions can be used for 80% of data manipulation problems. Additionally, this handout also introduces ggplot functions from tidyverse. ggplot is considered very effective for data visualization.",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#st-first-verb---select",
    "href": "r_eda.html#st-first-verb---select",
    "title": "\nExploratory Data Analysis in R",
    "section": "7.1 1st (First) verb - select ()",
    "text": "7.1 1st (First) verb - select ()\n      The select () function is used to select some columns from your data set. For example, if you want to select all variables except SIZE_EUROPE and SIZE_UK from your data set. Then you should write the following code (We created a new data set called product2)\n\nproduct2 &lt;- product %&gt;% \n  select(\n   -SIZE_EUROPE, - SIZE_UK  \n  )  # 1st Verb\nglimpse(product2)\n\nRows: 14,967\nColumns: 12\n$ INVOICE_NO &lt;chr&gt; \"52389\", \"52390\", \"52391\", \"52392\", \"52393\", \"52394\", \"5239…\n$ DATE       &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01…\n$ COUNTRY    &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United States…\n$ PRODUCT_ID &lt;chr&gt; \"2152\", \"2230\", \"2160\", \"2234\", \"2222\", \"2173\", \"2200\", \"22…\n$ SHOP       &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5\"…\n$ GENDER     &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Female…\n$ SIZE_US    &lt;chr&gt; \"11\", \"11.5\", \"9.5\", \"9.5\", \"9\", \"10.5\", \"9\", \"10\", \"10.5\",…\n$ UNIT_PRICE &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169,…\n$ DISCOUNT   &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1,…\n$ YEAR       &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,…\n$ MONTH      &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",…\n$ SALE_PRICE &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 139…",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#nd-second-verb---filter",
    "href": "r_eda.html#nd-second-verb---filter",
    "title": "\nExploratory Data Analysis in R",
    "section": "9.1 2nd (Second) verb - filter ()",
    "text": "9.1 2nd (Second) verb - filter ()\n      If we want to subset our dataset by rows, then filter () is used. For example - we want to create a data set that will include only observations for United States, then we should write the following code. The name of the dataset is given US.\n\nUS &lt;- product %&gt;% \n  filter(\n    COUNTRY == \"United States\"\n  )   # 2nd Verb\n\nglimpse(US)\n\nRows: 5,886\nColumns: 14\n$ INVOICE_NO  &lt;chr&gt; \"52390\", \"52392\", \"52394\", \"52397\", \"52399\", \"52399\", \"523…\n$ DATE        &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-02, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United…\n$ PRODUCT_ID  &lt;chr&gt; \"2230\", \"2234\", \"2173\", \"2191\", \"2197\", \"2213\", \"2206\", \"2…\n$ SHOP        &lt;chr&gt; \"US15\", \"US6\", \"US15\", \"US13\", \"US1\", \"US11\", \"US2\", \"US15…\n$ GENDER      &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Femal…\n$ SIZE_US     &lt;chr&gt; \"11.5\", \"9.5\", \"10.5\", \"10.5\", \"10\", \"9.5\", \"9.5\", \"8\", \"1…\n$ SIZE_EUROPE &lt;chr&gt; \"44-45\", \"40\", \"43-44\", \"43-44\", \"43\", \"40\", \"40\", \"41\", \"…\n$ SIZE_UK     &lt;dbl&gt; 11.0, 7.5, 10.0, 10.0, 9.5, 7.5, 7.5, 7.5, 10.5, 7.5, 9.5,…\n$ UNIT_PRICE  &lt;dbl&gt; 199, 159, 159, 139, 129, 169, 139, 139, 149, 159, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.2, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.5, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ SALE_PRICE  &lt;dbl&gt; 159.2, 159.0, 159.0, 139.0, 129.0, 152.1, 139.0, 139.0, 74…\n\nGermany &lt;- product %&gt;% \n  filter(\n    COUNTRY == \"Germany\" & YEAR %in% c ('2014', '2015')\n  ) \nglimpse(Germany)\n\nRows: 2,229\nColumns: 14\n$ INVOICE_NO  &lt;chr&gt; \"52395\", \"52401\", \"52401\", \"52408\", \"52409\", \"52414\", \"524…\n$ DATE        &lt;date&gt; 2014-01-02, 2014-01-03, 2014-01-03, 2014-01-04, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"Germany\", \"Germany\", \"Germany\", \"Germany\", \"Germany\", \"Ge…\n$ PRODUCT_ID  &lt;chr&gt; \"2200\", \"2235\", \"2197\", \"2206\", \"2157\", \"2235\", \"2239\", \"2…\n$ SHOP        &lt;chr&gt; \"GER2\", \"GER1\", \"GER1\", \"GER2\", \"GER2\", \"GER1\", \"GER2\", \"G…\n$ GENDER      &lt;chr&gt; \"Female\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Femal…\n$ SIZE_US     &lt;chr&gt; \"9\", \"10.5\", \"8.5\", \"8.5\", \"12\", \"9.5\", \"8.5\", \"10.5\", \"10…\n$ SIZE_EUROPE &lt;chr&gt; \"39-40\", \"43-44\", \"39\", \"41-42\", \"45\", \"42-43\", \"39\", \"43-…\n$ SIZE_UK     &lt;dbl&gt; 7.0, 10.0, 6.5, 8.0, 11.5, 9.0, 6.5, 10.0, 9.5, 8.0, 10.0,…\n$ UNIT_PRICE  &lt;dbl&gt; 179, 169, 179, 149, 149, 169, 129, 169, 199, 149, 169, 189…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.5, 0.2, 0.2, 0.2, 0.3, 0.5, 0.5, 0.0, 0.2, 0.5, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\"…\n$ SALE_PRICE  &lt;dbl&gt; 179.0, 84.5, 143.2, 119.2, 119.2, 118.3, 64.5, 84.5, 199.0…\n\ncount (Germany, YEAR)\n\n# A tibble: 2 × 2\n   YEAR     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2014   710\n2  2015  1519\n\n\nQUESTIONS - 1. Filter those observations that belong to United States and Germany and that are related to Male Gender.",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#rd-third-verb---summarize",
    "href": "r_eda.html#rd-third-verb---summarize",
    "title": "\nExploratory Data Analysis in R",
    "section": "10.1 3rd (Third) verb - summarize ()",
    "text": "10.1 3rd (Third) verb - summarize ()\n      The summarize () function is used to calculate different statistics such as mean, median, standard deviation, maximum, and minimum value. For example, we want to calculate the average price of all products -\n\nproduct %&gt;% \n  summarize(AVG_PRICE = mean(SALE_PRICE)) # 3rd Verb\n\n# A tibble: 1 × 1\n  AVG_PRICE\n      &lt;dbl&gt;\n1      144.",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#th-fourth-verb---group_by",
    "href": "r_eda.html#th-fourth-verb---group_by",
    "title": "\nExploratory Data Analysis in R",
    "section": "10.2 4th (Fourth) verb - group_by ()",
    "text": "10.2 4th (Fourth) verb - group_by ()\n      The group_by () function is very useful when it is used with summarize () function. For example, we want to know the average price for each country; then, we should write the following code -",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#th-fifth-verb---arrange",
    "href": "r_eda.html#th-fifth-verb---arrange",
    "title": "\nExploratory Data Analysis in R",
    "section": "10.3 5th (Fifth) verb - arrange ()",
    "text": "10.3 5th (Fifth) verb - arrange ()\n      The arrange ()function allows you to reorder your data set by one or more variables. For example, if you want to reorder the average price in difference countries, you need to execute the following code -\n\nproduct %&gt;% \n  group_by(COUNTRY) %&gt;% # 4th Verb \n  summarise(AVG_PRICE = mean(SALE_PRICE)) %&gt;% \n  arrange(AVG_PRICE) # 5th Verb\n\n# A tibble: 4 × 2\n  COUNTRY        AVG_PRICE\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Germany             144.\n2 United States       144.\n3 Canada              144.\n4 United Kingdom      146.\n\n\nQUESTIONS - 1. Calculate the average price for both Gender. Who pays greater price? 2. Calculate the average discount for both Gender. Who gets higher discount? 3. Calculate the average discount for each month. In which month highest discount is provided?",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#tidy-data",
    "href": "r_eda.html#tidy-data",
    "title": "\nExploratory Data Analysis in R",
    "section": "11.1 Tidy Data",
    "text": "11.1 Tidy Data\n      There are three interrelated rules which make a dataset tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\n\n\n\n\n\n\n\n\n# An example of a Tidy Dataset \ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n      Manipulating tidy data is easy. For example, for dataset table1, if we want to measure the rate of, we can do it easily.\n\ntable1 %&gt;% \n  mutate(rate = cases / population * 10000)\n\n# A tibble: 6 × 5\n  country      year  cases population  rate\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071 0.373\n2 Afghanistan  2000   2666   20595360 1.29 \n3 Brazil       1999  37737  172006362 2.19 \n4 Brazil       2000  80488  174504898 4.61 \n5 China        1999 212258 1272915272 1.67 \n6 China        2000 213766 1280428583 1.67",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#untidy-data",
    "href": "r_eda.html#untidy-data",
    "title": "\nExploratory Data Analysis in R",
    "section": "11.2 Untidy Data",
    "text": "11.2 Untidy Data\n      Untidy data violate the principle of the tidy data. Therefore, we need to apply analytics to transform it into tidy data. There are two important functions from tidyr that can be used to reshape data. The first one is - pivot_wider function and the second one is pivot_longer function. pivot_wider widens a LONG data whereas pivot_longer lengthens a WIDE data.\n\n\n\n\n\n\n\n\n\n\n# Some Built-in Untidy Datasets\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n\n11.2.1 pivot_longer()\n\ntable4a %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\ntable4b %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\n# A tibble: 6 × 3\n  country     year  population\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n\ntidy4a &lt;- table4a %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\ntidy4b &lt;- table4b %&gt;% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\nleft_join(tidy4a, tidy4b,\n          by = c ('country', 'year'))\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\n11.2.2 pivot_wider()\n\ntable2 %&gt;%\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#identify-total-sales-by-gender-of-each-product-product_id",
    "href": "r_eda.html#identify-total-sales-by-gender-of-each-product-product_id",
    "title": "\nExploratory Data Analysis in R",
    "section": "11.3 Identify Total Sales by Gender of Each Product (PRODUCT_ID)",
    "text": "11.3 Identify Total Sales by Gender of Each Product (PRODUCT_ID)\n\n11.3.1 6th (Sixth) verb - mutate ()\n      The function mutate () is used to create new variables (columns). For example, we want to know the TOTAL_SALE, which is the sum of the sum of sale by gender; then, we should write the following code -\n\nproduct %&gt;%\n  count(PRODUCT_ID, GENDER) %&gt;%\n  arrange(-n) %&gt;% \n  pivot_wider(\n    names_from = GENDER,\n    values_from = n\n  ) %&gt;% \n  rename_all(toupper) %&gt;% \n  rename_at(vars(c(\"MALE\",\"FEMALE\")), ~paste0(.x,\"_SALE\")) %&gt;% \n  mutate(\n    TOTAL_SALE = MALE_SALE + FEMALE_SALE\n  ) %&gt;% # 6th Verb\n  arrange(-TOTAL_SALE)\n\n# A tibble: 96 × 4\n   PRODUCT_ID MALE_SALE FEMALE_SALE TOTAL_SALE\n   &lt;chr&gt;          &lt;int&gt;       &lt;int&gt;      &lt;int&gt;\n 1 2190             132          75        207\n 2 2226             123          81        204\n 3 2213             114          90        204\n 4 2192             135          66        201\n 5 2158             120          78        198\n 6 2172             114          75        189\n 7 2179             102          87        189\n 8 2239              87         102        189\n 9 2225             117          69        186\n10 2183             123          60        183\n# ℹ 86 more rows\n\n\nQUESTIONS - 1. Identify Total Sales by Gender of Each Shoe Size (SIZE_US)",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#create-a-bar-chart-of-sale-of-shoes-by-gender-of-different-sizes",
    "href": "r_eda.html#create-a-bar-chart-of-sale-of-shoes-by-gender-of-different-sizes",
    "title": "\nExploratory Data Analysis in R",
    "section": "12.1 Create a Bar Chart of Sale of Shoes by Gender of Different Sizes",
    "text": "12.1 Create a Bar Chart of Sale of Shoes by Gender of Different Sizes\n\nproduct %&gt;%\n  count(SIZE_US, GENDER) %&gt;%\n  pivot_wider(\n    names_from = \"GENDER\",\n    values_from = \"n\"\n  ) %&gt;%\n  rename_all(toupper) %&gt;%\n  replace(is.na(.),0) %&gt;%\n  mutate(\n    TOTAL_SALES = FEMALE + MALE\n  ) %&gt;%\n  pivot_longer(\n    cols = c(\"FEMALE\", \"MALE\"),\n    names_to = \"GENDER\",\n    values_to = \"GENDERSALES\"\n  )%&gt;%\n  ggplot(aes(x=reorder(SIZE_US,as.numeric(SIZE_US)), y= GENDERSALES, fill = GENDER))+\n  geom_bar(stat = \"identity\")+\n  labs(x = \"SHOE SIZE\",\n       y = \"TOTAL SALES\",\n       title = \"SALES OF DIFFERENT SIZES OF SHOE\")+\n  geom_text(aes(label = GENDERSALES),\n            position = position_stack(vjust = 0.5),\n            color = \"black\",\n            size = 2\n  )+\n  theme(legend.title = element_blank())",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#create-a-bar-chart-of-sale-of-shoes-by-gender-of-different-sizes-in-different-countries",
    "href": "r_eda.html#create-a-bar-chart-of-sale-of-shoes-by-gender-of-different-sizes-in-different-countries",
    "title": "\nExploratory Data Analysis in R",
    "section": "12.2 Create a Bar Chart of Sale of Shoes by Gender of Different Sizes in different countries",
    "text": "12.2 Create a Bar Chart of Sale of Shoes by Gender of Different Sizes in different countries\n\nproduct %&gt;%\n  count(SIZE_US, GENDER, COUNTRY) %&gt;%\n  ggplot(aes(x=reorder(SIZE_US,as.numeric(SIZE_US)), y= n, fill = GENDER))+\n  geom_bar(stat = \"identity\")+\n  labs(x = \"SHOE SIZE\",\n       y = \"TOTAL SALES\",\n       title = \"SALES OF DIFFERENT SIZES OF SHOE BY GENDER IN DIFFERENT COUNTRIES\"\n  )+\n  geom_text(\n    aes(label = n),\n    position = position_stack(vjust = 0.5),\n    color = \"black\",\n    size = 2\n  )+\n  facet_wrap(~ COUNTRY, nrow = 2, ncol = 2\n  )+\n  theme(legend.position=\"top\",\n        legend.title = element_blank())+\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#create-a-bar-chart-for-product-id-product_id-2190-sales-by-shoes-sizes",
    "href": "r_eda.html#create-a-bar-chart-for-product-id-product_id-2190-sales-by-shoes-sizes",
    "title": "\nExploratory Data Analysis in R",
    "section": "12.3 Create a Bar Chart for Product ID (PRODUCT_ID) 2190 Sales by shoes sizes",
    "text": "12.3 Create a Bar Chart for Product ID (PRODUCT_ID) 2190 Sales by shoes sizes\n\nproduct %&gt;% \n  filter(\n    PRODUCT_ID == \"2190\"\n  ) %&gt;% \n  count(SIZE_US) %&gt;% \n  mutate(SIZE_US = (str_c (\"SIZE_\", SIZE_US))) %&gt;% \n  ggplot(aes(x = reorder(SIZE_US,n), y = n))+\n  geom_bar(stat=\"identity\", color = \"blue\", fill = \"orange\")+\n  coord_flip()+\n  geom_text(aes(label = n), stat = \"identity\", hjust = -0.2)+ # Here also try to use vjust and take out coord_flip()\n  xlab(\"SHOE SIZE\")+\n  ylab(\"SALES (UNIT)\")+\n  ggtitle(\"DISTRIBUTION of SALES for PRODUCT ID 2190\")\n\n\n\n\n\n\n\n\nQUESTIONS - 1. Create a Bar Chart for Product ID (PRODUCT_ID) 2190 Sales by Gender of Different Shoes Sizes",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#relationship-between-shoe-size-and-price-in-different-gender-and-in-different-countries",
    "href": "r_eda.html#relationship-between-shoe-size-and-price-in-different-gender-and-in-different-countries",
    "title": "\nExploratory Data Analysis in R",
    "section": "13.1 Relationship between Shoe Size and Price in Different Gender and in Different Countries",
    "text": "13.1 Relationship between Shoe Size and Price in Different Gender and in Different Countries\n\nproduct %&gt;%\n  ggplot(\n    aes(x = as.numeric(SIZE_US), y = UNIT_PRICE, color = GENDER)\n  )+\n  geom_smooth(se = FALSE)+\n  xlab(\"SHOE SIZE (US)\")+\n  ylab(\"PRICE\")+\n  facet_wrap(~COUNTRY)+\n  theme(legend.title = element_blank())\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nQUESTIONS - 1. Do the Above Analyses for the Relationship between Shoe Size and Discount",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  },
  {
    "objectID": "r_eda.html#footnotes",
    "href": "r_eda.html#footnotes",
    "title": "\nExploratory Data Analysis in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.techrepublic.com/article/why-data-scientist-is-the-most-promising-job-of-2019/↩︎",
    "crumbs": [
      "Modules & Slides",
      "Appendix",
      "<center> Exploratory Data Analysis in R"
    ]
  }
]