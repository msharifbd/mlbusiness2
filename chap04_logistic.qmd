---
title: "<center> Chapter # 04 <br> Logistic Regression"
format: 
  html: 
    toc: true
    #toc-title: Table of Contents 
    toc-depth: 5
    number-sections: true
    mainfont: emoji
---

## Introduction 


## Loading Python Package 

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
# For Visualization
sns.set(style = "white")
sns.set(style = "whitegrid", color_codes = True)

import sklearn # For Machine Learning 

import warnings 
warnings.filterwarnings('ignore')

import sys
sys.version

print ('The Scikit-learn version that is used for this code file is {}'.format(sklearn.__version__))
```

## Loading Dataset 

```{python}
# Importing Training Dataset 
train_df = pd.read_csv("DATA/train.csv")
# Importing Testing Dataset
test_df = pd.read_csv("DATA/test.csv")
```

### Metadata of the Dataset 

```{python}

print("The total number of rows and columns in the dataset is {} and {} respectively.".format(train_df.shape[0],train_df.shape[1]))
print ("\nThe names and the types of the variables of the dataset:")
train_df.info()
train_df.head()
print("\nThe types of the variables in the dataset are:")
train_df.dtypes
```

```{python}
for x in train_df.columns:
  print  (x)
train_df['Pclass'].value_counts(sort = True, ascending = True)
```

```{python}
train_df['Sex'].value_counts()
```

## Data Quality & Missing Value Assessment

```{python}
# Missing value in Training Dataset 
train_df.isna().sum()
```

```{python}
# Missing value in Testing Dataset 
test_df.isnull().sum()
```

### Age - Missing Value

```{python}
print ('The percent of missing "Age" record in the training dataset is %0.2f%%' %(train_df['Age'].isnull().sum()/train_df.shape[0]*100))
```

```{python}
print ('The percent of missing "Age" record in testing dataset is %0.3f%%' %(test_df['Age'].isnull().sum()/test_df.shape[0]*100))
```

```{python}
plt.figure(figsize = (10,8))
train_df['Age'].hist(bins = 15, density = True, color = 'teal', alpha = 0.60)
train_df['Age'].plot(kind = 'density', color = 'teal', alpha = 0.60)
plt.show()
```

Since the variable `Age` is a little bit right skewed, using the mean to replace the missing observations might bias our results. Therefore, it is recommended that median be used to replace the missing observations.

```{python}
print ('The mean of "Age" variable is %0.3f.' %(train_df['Age'].mean(skipna = True)))
```

```{python}
print ('The median of "Age" variable is %0.2f.' %(train_df['Age'].median(skipna = True)))
```

### Cabin - Missing Value

```{python}
train_df['Cabin'].value_counts()
```

```{python}
print ('The percent of cabin variable missing value is %0.2f%%.' %(train_df['Cabin'].isnull().sum()/train_df.shape[0]*100))
```

77% observations of the `Cabin` variable is missing. Therefore, it is better to prune the variable from the dataset. Moreover, the drop of the `Cabin` variable is justified because it has correlation with two other variables - `Fare` and `Pclass`.

### Embarked - Missing Value

```{python}
train_df['Embarked'].value_counts()
```

```{python}
# Percent of missing 'Embarked' Variable
print(
    "The percent of missing 'Embarked' records is %.2f%%" %
    (train_df['Embarked'].isnull().sum()/train_df.shape[0]*100)
)
```

Since there are only 0.22% missing observation for `Embarked`, we can impute the missing values with the port where most people embarked.

```{python}
print('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')
print(train_df['Embarked'].value_counts())
sns.countplot(x='Embarked', data=train_df, palette='Set2')
plt.show()
```

```{python}
print('The most common boarding port of embarkation is %s.' %train_df['Embarked'].value_counts().idxmax())
```

## Final Adjustment to the Datasets (Training & Testing)

Based on the assessment of the missing values in the dataset, We will make the following changes to the data:

-   The missing value of `Age` variable will be imputed with 28 (median value `Age`)
-   The missing value of `Embarked` variable will be imputed with `S` (the most common boarding point)
-   There are many missing values for the variable `Cabin`; therefore, the variable will be dropped. Moreover, the drop will not affect the model as the variable is associated with two other variables - `Pclass` and `Fare`.

```{python}
train_data = train_df.copy()
train_data['Age'].fillna(train_data['Age'].median(skipna = True), inplace = True)
train_data['Embarked'].fillna(train_data['Embarked'].value_counts().idxmax(), inplace = True)
train_data.drop(['Cabin'], axis = 1, inplace = True)
train_data.isnull().sum()
```

```{python}
train_data.tail()
```

```{python}
plt.figure(figsize=(15,8))
# Data with missing observations
ax = train_df['Age'].hist(bins = 15, density = True, stacked = True, color = 'teal', alpha = 0.6)
train_df['Age'].plot(kind = 'density', color = 'teal')
# Data without missing observations
ax = train_data['Age'].hist(bins = 15, density = True, stacked = True, color = 'orange', alpha = 0.6)
train_data['Age'].plot(kind = 'density', color = 'orange')
plt.xlim(-10,85)
ax.legend(["Raw Age", "Adjusted Age"])
ax.set(xlabel = 'Age')
plt.show()
```

### Additional Variables

The variable `SibSp` means whether the passenger has sibling or spouse aboard and the variable `Parch` means whether the passenger has parents or children aboard. For the sake of simplicity and to account for **multicollinearity**, these two variables will be combined into a categorical variable: whether or not the individual was traveling alone.

```{python}
# Creating categorical variable for Traveling alone
train_data['TravelAlone'] = np.where((train_data['SibSp']+train_data['Parch']) > 0, 0, 1)
train_data.drop('SibSp', axis = 1, inplace = True)
train_data.drop('Parch', axis = 1, inplace = True)
```

For variables `Pclass`, `Sex`, and `Embarked`, categorical variables will be created

```{python}
training = pd.get_dummies(train_data, columns= ["Pclass", "Embarked", "Sex"])
training.info()
```

```{python}
training[['Pclass_1','Pclass_2','Pclass_3', 'Embarked_C','Embarked_Q','Embarked_S', 'Sex_female', 'Sex_male']].head()
```

```{python}
training.drop(['Sex_female', 'PassengerId', 'Name', 'Ticket'], axis=1, inplace = True)
training.tail()
```

```{python}
training.info()
```

```{python}
final_train = training
final_train.tail()
```

```{python}
test_df.isna().sum()
```

Now we apply the same changes in the testing dataset.

-   We will apply to same imputation for `Age` in the Test data as we did for my Training data (if missing, `Age` = 28).
-   We will also remove the `Cabin` variable from the test data, as we've decided not to include it in my analysis.
-   There were no missing values in the `Embarked` port variable.
-   We will add the dummy variables to finalize the test set.
-   Finally, We will impute the 1 missing value for `Fare` with the median, 14.45.

```{python}
print('The median value of "Fare" variable in testing dataset is %0.3f.' %(train_df['Fare'].median(skipna = True)))
```

```{python}
test_data = test_df.copy()
test_data['Age'].fillna(test_data['Age'].median(skipna = True), inplace = True)
test_data['Fare'].fillna(test_data['Fare'].median(skipna = True), inplace = True)
test_data.drop(['Cabin'], axis  = 1, inplace = True)
```

### Creating New Variables

```{python}
# Creating new variable - TravelAlone
test_data['TravelAlone']= np.where(test_data['SibSp']+test_data['Parch']>0,0,1)
test_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)
test_data.sample(5)
```

#### Creating the Dummies for Categorical Variables

```{python}
testing = pd.get_dummies(test_data, columns = ['Pclass', 'Sex', 'Embarked'])
testing.drop(['Sex_female','PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)
testing.tail()
```

```{python}
final_test = testing
final_test.head()
```

## Exploratory Data Analysis (EDA)

### Exploration of `Age` Variable

```{python}
final_train.info()
```

```{python}
plt.figure(figsize=(15,8))
ax = sns.kdeplot(final_train['Age'][final_train.Survived == 1], shade=True, color = 'darkturquoise')
sns.kdeplot(final_train['Age'][final_train.Survived == 0], shade=True, color = 'lightcoral')
ax.legend(['Survived', 'Died']) # or you can use plt.legend(['Survived', 'Died])
plt.title('Density Plot for Surviving Population and Deceased Population')
plt.xlabel('Age') # or you can use ax.set(xlabel = 'Age')
plt.xlim(-10,85)
plt.show()
```

The age distribution for survivors and deceased is actually very similar. One notable difference is that, of the survivors, a larger proportion were children. The passengers evidently made an attempt to save children by giving them a place on the life rafts.

```{python}
avg_age = final_train.groupby(['Survived']) ['Age'].mean()
avg_age.to_frame().reset_index()
```

```{python}
sns.boxplot(data = final_train, x  = 'Survived', y = 'Age', palette='Set2')
plt.title("Comparison of Age of Passengers Conditioned on Survived")
plt.show()
```

```{python}
# Creating a Dummy Variable IsMinor
final_train['IsMinor'] = np.where(final_train['Age'] <= 16, 1, 0)
final_test['IsMinor'] = np.where(final_test['Age'] <= 16, 1, 0)
```

### Exploration of `Fare` Variable

```{python}
train_df.groupby(['Survived']) ['Fare'].mean().to_frame().reset_index()
```

```{python}
sns.boxplot(data = final_train, x = 'Survived', y = 'Fare', palette='Set2')
plt.ylim(0, 100)
plt.title('Comparison of Fare of Passengers Conditioned on Survived')
plt.show()
```

```{python}
plt.figure(figsize=(15,8))
ax = sns.kdeplot(final_train['Fare'][final_train.Survived == 1],shade=True, color='darkturquoise')
sns.kdeplot(final_train['Fare'][final_train.Survived==0], shade=True, color='lightcoral')
ax.legend(['Survived', 'Died'])
ax.set(xlabel= 'Fare')
plt.xlim(-20,200)
plt.title('Density Plot of Fare for Surviving Population and Deceased Population')
plt.show()
```

As the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class, which we'll look at next.

```{python}
#| warning: false
# Pair Plot of two continuous variables (Age and Fare)
plt.figure(figsize=(15,8))
sns.pairplot(data=train_data, hue='Survived', vars= ['Age', 'Fare'])
plt.show()
```

### Exploration of `PClass` Variable

```{python}
sns.barplot(data = train_df, x = 'Pclass', y = 'Survived', color='darkturquoise')
plt.show()
```

As expected, first class passengers were more likely to survive.

### Exploration of `Embarked` Variable

```{python}
sns.barplot(x = 'Embarked', y = 'Survived', data=train_df, color="teal")
plt.show()
```

Passengers who boarded in Cherbourg, France, appear to have the highest survival rate. Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown. This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck). It's also worth noting the size of the whiskers in these plots. Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest. The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker. It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts.

### Exploration of `TravelAlone` Variable

```{python}
sns.barplot(x = 'TravelAlone', y = 'Survived', data=final_train, color="mediumturquoise")
plt.xlabel('Travel Alone')
plt.show()
```

Individuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male.

### Exploration of `Gender` Variable

```{python}
sns.barplot(x = 'Sex', y = 'Survived', data=train_df, color="aquamarine")
plt.show()
```

### Chi-square Test of Independence

#### Chi-square Test of Independence betweeen `Survived` and `Sex`

```{python}
pd.crosstab(train_df['Survived'], train_df['Sex'])
```

```{python}
# Importing scipy package 
from scipy import stats
```

```{python}
stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))
```


```{python}
chi2_stat, p, dof, expected = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Sex']))
print(f"chi2 statistic:     {chi2_stat:.5g}")
print(f"p-value:            {p:.5g}")
print(f"degrees of freedom: {dof}")
print("expected frequencies:\n",expected)

```
#### Chi-square Test of Independence betweeen `Survived` and `Pclass`

```{python}
chi2_stat_2, p_2, dof_2, expected_2 = stats.chi2_contingency(pd.crosstab(train_df['Survived'], train_df['Pclass']))
print(f"chi2 statistic:     {chi2_stat_2:.5g}")
print(f"p-value:            {p_2:.5g}")
print(f"degrees of freedom: {dof_2}")
print("expected frequencies:\n",expected_2)
```

### Post Hoc Analysis of `Pclass`

The explanation of  `Post Hoc` analysis is given in this [link](!https://www.youtube.com/watch?v=-S8EJEYNFIc).


```{python}
pclass_cross = pd.crosstab(train_df['Survived'], train_df['Pclass'])
pclass_cross
```

```{python}
import gc
from itertools import combinations
import scipy.stats
import statsmodels.stats.multicomp as multi
from statsmodels.stats.multitest import multipletests
```


```{python}
p_vals_chi = []
pairs_of_class = list(combinations(train_df['Pclass'].unique(),2))

for each_pair in pairs_of_class:
    each_df = train_df[(train_df['Pclass']==each_pair[0]) | (train_df['Pclass']==each_pair[1])]
    p_vals_chi.append(\
          scipy.stats.chi2_contingency(
            pd.crosstab(each_df['Survived'], each_df['Pclass']))[1]
         )
```


```{python}
#Results of Bonferroni Adjustment
bonferroni_results = pd.DataFrame(columns=['pair of class',\
                                           'original p value',\
                                           'corrected p value',\
                                           'Reject Null?'])

bonferroni_results['pair of class'] = pairs_of_class
bonferroni_results['original p value'] = p_vals_chi

#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.
multi_test_results_bonferroni = multipletests(p_vals_chi, method='bonferroni')

bonferroni_results['corrected p value'] = multi_test_results_bonferroni[1]
bonferroni_results['Reject Null?'] = multi_test_results_bonferroni[0]
bonferroni_results.head()
```

### Post Hoc Analysis of `Embarked`

```{python}
# Write code Here 
p_vals_chi_embark = []
pairs_of_embark = list(combinations(train_data['Embarked'].unique(),2))

for each_pair in pairs_of_embark:
    each_df = train_data[(train_data['Embarked']==each_pair[0]) | (train_data['Embarked']==each_pair[1])]
    p_vals_chi_embark.append(\
          scipy.stats.chi2_contingency(
            pd.crosstab(each_df['Survived'], each_df['Embarked']))[1]
         )
```

```{python}
#Write code Here
#Results of Bonferroni Adjustment
bonferroni_results = pd.DataFrame(columns=['pair of embark',\
                                           'original p value',\
                                           'corrected p value',\
                                           'Reject Null?'])

bonferroni_results['pair of embark'] = pairs_of_embark
bonferroni_results['original p value'] = p_vals_chi_embark

#Perform Bonferroni on the p-values and get the reject/fail to reject Null Hypothesis result.
multi_test_results_bonferroni_embark = multipletests(p_vals_chi_embark, method='bonferroni')

bonferroni_results['corrected p value'] = multi_test_results_bonferroni_embark[1]
bonferroni_results['Reject Null?'] = multi_test_results_bonferroni_embark[0]
bonferroni_results.head()
```

## Logistic Regression & Results

### Feature Selection

#### Recursive Feature Selection (RFE)

Given an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a `coef_ attribute` or through a `feature_importances_` attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.

```{python}
# Our all training dataset 
train_df
train_data # impute missing values 
training # create the dummies 
final_train
```

```{python}
cols = ['Age', 'Fare', 'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']
X = final_train[cols] # features vector
y = final_train['Survived'] # Target vector 
```

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.feature_selection import RFECV
```

```{python}
# Build the model 
model = LogisticRegression()
# Create the RFE model 
rfe = RFE(estimator=model, n_features_to_select=8)
rfe = rfe.fit(X ,y)
dir(rfe)

```

```{python}
# summarize the selection of the attributes
print('Selected features: %s' % list(X.columns[rfe.support_]))
```

```{python}
# Getting the Regression Results 
import statsmodels.api as sm
logit_model = sm.Logit(y,X.astype(float))
result = logit_model.fit()
print(result.summary2())
```

```{python}
cols2 = ['Age',  'TravelAlone','Pclass_1','Pclass_2','Embarked_C','Embarked_Q','Sex_male','IsMinor']
X_alt = final_train[cols2] # features vector
logit_model2 = sm.Logit(y,sm.add_constant(X_alt.astype(float)))
result2 = logit_model2.fit()
print(result2.summary2())
```

```{python}
from stargazer.stargazer import Stargazer
titanic_logit = Stargazer([result, result2])
titanic_logit
```

```{python}
# Interpreting the coefficients, which are the log odds; therefore, we need to convert them into odds ratio. 
np.exp(-2.5527)
```

```{python}
np.exp(result.params) # Getting the Odds Ratio of all Features 
```

#### Feature Ranking with Recursive Feature Elimination and Cross-validation (`RFECV`)

`RFECV` performs `RFE` in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation

```{python}
# Create the RFE object and compute a cross-validated score.
# The "accuracy" scoring is proportional to the number of correct classifications
rfecv = RFECV(estimator=LogisticRegression(max_iter = 5000), step=1, cv=10, scoring='accuracy')
rfecv.fit(X, y)
for x in dir(rfecv):
  print(x)
```

```{python}
# To get the accuracy
rfecv.cv_results_
```

```{python}
type(rfecv.cv_results_)
```

```{python}
rfecv.cv_results_

```

```{python}
rfecv.n_features_
```

```{python}
list(X.columns[rfecv.support_])
```

```{python}
# To check whether the RFE and RFECV generate the same features
set(list(X.columns[rfecv.support_])) == set(list((X.columns[rfe.support_])))  
```

```{python}
rfecv.cv_results_.keys()
```

```{python}
crossVal_results = rfecv.cv_results_
crossVal_results
del crossVal_results['mean_test_score']
del crossVal_results['std_test_score']
crossVal_results
```

## Correlation Matrix

```{python}
Selected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 
                     'Embarked_S', 'Sex_male', 'IsMinor']
X = final_train[Selected_features] # Recreated features vector 

plt.subplots(figsize=(8, 5))
sns.heatmap(X.corr(), annot=True, cmap="RdYlGn") # for cmap = 'viridis' can also be used.
plt.show()
```

```{python}
plt.subplots(figsize=(8, 5))
sns.heatmap(final_train[['Survived', 'Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', 'Embarked_S', 'Sex_male', 'IsMinor']].corr(), annot = True, cmap = 'viridis')
plt.show()
```

## Model Evaluation Procedures

### Model Evaluation Based on Train/Test Split

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score
from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss
```

```{python}
X = final_train[Selected_features]
y = final_train['Survived']
```

```{python}
# use train/test split with different random_state values
# we can change the random_state values that changes the accuracy scores
# the scores change a lot, this is why testing scores is a high-variance estimate
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)
```

```{python}
# Check classification scores of Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test) # Prediction actual class
y_pred_proba = logreg.predict_proba(X_test) [:,1]
[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)
print('Train/Test split results:')
print(logreg.__class__.__name__+" accuracy is %2.3f" % accuracy_score(y_test, y_pred))
print(logreg.__class__.__name__+" log_loss is %2.3f" % log_loss(y_test, y_pred_proba)) 
print(logreg.__class__.__name__+" auc is %2.3f" % auc(fpr, tpr)) 

```

### Confusion Matrix

```{python}
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print('Confusion matrix\n\n', cm)
print('\nTrue Positives(TP) = ', cm[0,0])
print('\nTrue Negatives(TN) = ', cm[1,1])
print('\nFalse Positives(FP) = ', cm[0,1])
print('\nFalse Negatives(FN) = ', cm[1,0])
```

```{python}
# Visualizing Confusion Matrix
plt.figure(figsize=(6,4))
cm_matrix = pd.DataFrame(data= cm, columns=['Actual Positive:1', 'Actual Negative:0'], 
                                 index=['Predict Positive:1', 'Predict Negative:0'])

sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')
plt.show()
```

### Classification Report

```{python}
print (classification_report(y_test, y_pred))
```

### ROC-AUC Curve

```{python}
idx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility (true positive rate (tpr)) > 0.95

plt.figure()
plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')
plt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)
plt.ylabel('True Positive Rate (recall/sensitivity)', fontsize=14)
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

print("Using a threshold of %.3f " % thr[idx] + "guarantees a sensitivity of %.3f " % tpr[idx] +  
      "and a specificity of %.3f" % (1-fpr[idx]) + 
      ", i.e. a false positive rate of %.2f%%." % (np.array(fpr[idx])*100))
```

## Model Evaluation Based on K-fold Cross-validation `cross_val_score()` Function

```{python}
# 10-fold cross-validation logistic regression
logreg = LogisticRegression(max_iter=5000)
# Use cross_val_score function
# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data
# cv=10 for 10 folds
# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many
scores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')
scores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')
scores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')
print('K-fold cross-validation results:')
print(logreg.__class__.__name__+" average accuracy is %2.3f" % scores_accuracy.mean())
print(logreg.__class__.__name__+" average log_loss is %2.3f" % -scores_log_loss.mean())
print(logreg.__class__.__name__+" average auc is %2.3f" % scores_auc.mean())

```

### Model Evaluation Based on K-fold Cross-validation Using `cross_validate()` Function

```{python}
scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}

modelCV = LogisticRegression(max_iter=5000)

results = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()), 
                         return_train_score=False)

print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(modelCV.__class__.__name__+" average %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()
                               if list(scoring.values())[sc]=='neg_log_loss' 
                               else results['test_%s' % list(scoring.values())[sc]].mean(), 
                               results['test_%s' % list(scoring.values())[sc]].std())) 
```

What happens when we add the feature `Fare` - 

```{python}
cols = ["Age","Fare","TravelAlone","Pclass_1","Pclass_2","Embarked_C","Embarked_S","Sex_male","IsMinor"]
X = final_train[cols]

scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}

modelCV = LogisticRegression(max_iter=5000)

results = cross_validate(modelCV, final_train[cols], y, cv=10, scoring=list(scoring.values()), 
                         return_train_score=False)

print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(modelCV.__class__.__name__+" average %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()
                               if list(scoring.values())[sc]=='neg_log_loss' 
                               else results['test_%s' % list(scoring.values())[sc]].mean(), 
                               results['test_%s' % list(scoring.values())[sc]].std()))
```
We notice that the model is slightly deteriorated. The `Fare` variable does not carry any useful information. Its presence is just a noise for the logistic regression model.


## `GridSearchCV` Evaluating Using Multiple Scorers Simultaneously





```{python}
from sklearn.model_selection import GridSearchCV

X = final_train[Selected_features]

param_grid = {'C': np.arange(1e-05, 3, 0.1)}
scoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}

gs = GridSearchCV(LogisticRegression(max_iter=5000), return_train_score=True,
                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')

gs.fit(X, y)
results = gs.cv_results_

print('='*20)
print("best params: " + str(gs.best_estimator_))
print("best params: " + str(gs.best_params_))
print('best score:', gs.best_score_)
print('='*20)

plt.figure(figsize=(10, 10))
plt.title("GridSearchCV evaluating using multiple scorers simultaneously",fontsize=16)

plt.xlabel("Inverse of regularization strength: C")
plt.ylabel("Score")
plt.grid()

ax = plt.axes()
ax.set_xlim(0, param_grid['C'].max()) 
ax.set_ylim(0.35, 0.95)

# Get the regular numpy array from the MaskedArray
X_axis = np.array(results['param_C'].data, dtype=float)

for scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): 
    for sample, style in (('train', '--'), ('test', '-')):
        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]
        sample_score_std = results['std_%s_%s' % (sample, scorer)]
        ax.fill_between(X_axis, sample_score_mean - sample_score_std,
                        sample_score_mean + sample_score_std,
                        alpha=0.1 if sample == 'test' else 0, color=color)
        ax.plot(X_axis, sample_score_mean, style, color=color,
                alpha=1 if sample == 'test' else 0.7,
                label="%s (%s)" % (scorer, sample))

    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]
    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]
        
    # Plot a dotted vertical line at the best score for that scorer marked by x
    ax.plot([X_axis[best_index], ] * 2, [0, best_score],
            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)

    # Annotate the best score for that scorer
    ax.annotate("%0.2f" % best_score,
                (X_axis[best_index], best_score + 0.005))

plt.legend(loc="best")
plt.grid('off')
plt.show() 
```

### `GridSearchCV` Evaluating Using Multiple scorers, `RepeatedStratifiedKFold` and `pipeline` for Preprocessing Simultaneously

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.pipeline import Pipeline

#Define simple model
###############################################################################
C = np.arange(1e-05, 5.5, 0.1)
scoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}
log_reg = LogisticRegression(max_iter=5000)

#Simple pre-processing estimators
###############################################################################
std_scale = StandardScaler(with_mean=False, with_std=False)
#std_scale = StandardScaler()

#Defining the CV method: Using the Repeated Stratified K Fold
###############################################################################

n_folds=5
n_repeats=5

rskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)

#Creating simple pipeline and defining the gridsearch
###############################################################################

log_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])

log_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,
              scoring=scoring, return_train_score=True,
              param_grid=dict(clf__C=C), refit='Accuracy')

log_clf.fit(X, y)
results = log_clf.cv_results_

print('='*20)
print("best params: " + str(log_clf.best_estimator_))
print("best params: " + str(log_clf.best_params_))
print('best score:', log_clf.best_score_)
print('='*20)

plt.figure(figsize=(10, 10))
plt.title("GridSearchCV evaluating using multiple scorers simultaneously",fontsize=16)

plt.xlabel("Inverse of regularization strength: C")
plt.ylabel("Score")
plt.grid()

ax = plt.axes()
ax.set_xlim(0, C.max()) 
ax.set_ylim(0.35, 0.95)

# Get the regular numpy array from the MaskedArray
X_axis = np.array(results['param_clf__C'].data, dtype=float)

for scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): 
    for sample, style in (('train', '--'), ('test', '-')):
        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]
        sample_score_std = results['std_%s_%s' % (sample, scorer)]
        ax.fill_between(X_axis, sample_score_mean - sample_score_std,
                        sample_score_mean + sample_score_std,
                        alpha=0.1 if sample == 'test' else 0, color=color)
        ax.plot(X_axis, sample_score_mean, style, color=color,
                alpha=1 if sample == 'test' else 0.7,
                label="%s (%s)" % (scorer, sample))

    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]
    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]
        
    # Plot a dotted vertical line at the best score for that scorer marked by x
    ax.plot([X_axis[best_index], ] * 2, [0, best_score],
            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)

    # Annotate the best score for that scorer
    ax.annotate("%0.2f" % best_score,
                (X_axis[best_index], best_score + 0.005))

plt.legend(loc="best")
plt.grid('off')
plt.show() 
```
## Regularization 

`Regularization` is a method of preventing overfitting, which is a common problem in machine learning. Overfitting means that your model learns too much from the specific data you have, and fails to generalize well to new or unseen data. This can lead to poor predictions and low performance. Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables.

Since the size of each coefficient depends on the scale of its corresponding variable, scaling the data is required so that the regularization penalizes each variable equally. The regularization strength is determined by `C` and as `C` increases, the regularization term becomes smaller (and for extremely large `C` values, it's as if there is no regularization at all).

If the initial model is overfit (as in, it fits the training data too well), then adding a strong regularization term (with small `C` value) makes the model perform worse for the training data, but introducing such "noise" improves the model's performance on unseen (or test) data.

An example with 1000 samples and 200 features shown below. As can be seen from the plot of accuracy over different values of C, if C is large (with very little regularization), there is a big gap between how the model performs on training data and test data. However, as C decreases, the model performs worse on training data but performs better on test data (test accuracy increases). However, when C becomes too small (or the regularization becomes too strong), the model begins performing worse again because now the regularization term completely dominates the objective function.



```{python}
# Necessary Python Packages 
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
# make sample data
X, y = make_classification(1000, 200, n_informative=195, random_state=2023)
# split into train-test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2023)

# normalize the data
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# train Logistic Regression models for different values of C
# and collect train and test accuracies
scores = {}
for C in (10**k for k in range(-6, 6)):
    lr = LogisticRegression(C=C)
    lr.fit(X_train, y_train)
    scores[C] = {'train accuracy': lr.score(X_train, y_train), 
                 'test accuracy': lr.score(X_test, y_test)}

# plot the accuracy scores for different values of C
pd.DataFrame.from_dict(scores, 'index').plot(logx=True, xlabel='C', ylabel='accuracy')
```


### Types of Regularization 

#### `L1` regularization

`L1` regularization, also known as `Lasso regularization`, adds the sum of the absolute values of the model’s coefficients to the loss function. It encourages sparsity in the model by shrinking some coefficients to precisely zero. This has the effect of performing feature selection, as the model can effectively ignore irrelevant or less important features. L1 regularization is particularly useful when dealing with high-dimensional datasets with desired feature selection.

Mathematically, the `L1` regularization term can be written as:

**`L1 regularization = λ * Σ|wi|`**

Here, `λ` is the regularization parameter that controls the strength of regularization, `wi` represents the individual model coefficients and the sum is taken over all coefficients.


#### `L2` regularization

`L2` regularization, also known as `Ridge regularization`, adds the sum of the squared values of the model’s coefficients to the loss function. Unlike L1 regularization, L2 regularization does not force the coefficients to be exactly zero but instead encourages them to be small. L2 regularization can prevent overfitting by spreading the influence of a single feature across multiple features. It is advantageous when there are correlations between the input features.

Mathematically, the `L2` regularization term can be written as:

**`L2 regularization = λ * Σ(wi^2)`**

Similar to `L1` regularization, `λ` is the regularization parameter, and `wi` represents the model coefficients. The sum is taken over all coefficients, and the squares of the coefficients are summed.


The choice between `L1` and `L2` regularization depends on the specific problem and the characteristics of the data. For example, `L1` regularization produces sparse models, which can be advantageous when feature selection is desired. `L2` regularization, on the other hand, encourages small but non-zero coefficients and can be more suitable when there are strong correlations between features.

## Conclusion




