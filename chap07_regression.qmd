---
title: "<center> Chapter # 07 <br> Regression Analysis"
format: html
---



# Introduction 

Multiple linear regression is used to model the relationship between a continuous response variable and continuous or categorical explanatory variables. Multiple linear regression allows to evaluate the relationship between two variables, while controlling for the effect (i.e., removing the effect) of other variables.

## Working Directory 

```{python}
#| eval: false
import os
# Current Working Directory 
os.getcwd()
```


```{python}
#| eval: false
# Files in Working Directory
for file in os.listdir():
  print(file)
```



# Importing Necessary `Python` Packages 

```{python}
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import sklearn
import warnings 
warnings.filterwarnings('ignore')
```



# Importing Dataset

```{python}
df = pd.read_csv('DATA/CAR DETAILS FROM CAR DEKHO.csv')
```

```{python}
df.head()
```

## Metadata of the Dataset

```{python}
df.info()
```
```{python}
df.shape
```
```{python}
df.isna().sum()
```
# Exploratory Data Analysis

We have 1491 unique car names in our dataset. Clearly, it does not add any meaning to our dataset, since there are so many categories. Let's drop that column.

```{python}
df['name'].nunique()
```

```{python}
df.drop(columns = ['name'], inplace = True)
df.info()
```


The dataset has the column named "Year". Ideally, we need the age of car over the year it was bought / sold. So, let's convert that to "Age" and remove the "Year" column.

```{python}
df.insert(0, "Age", df["year"].max()+1-df["year"] )
df.drop('year', axis=1, inplace=True)
df.info()
```

"Age" is calculated by finding the difference between the maximum year available in our dataset and the year of that particular car. This is because, our calculations will be specific to that particular time period and to this dataset.

## Finding Outliers 

An outlier is a data point that differs significantly from other observations. They can cause the performance of the model to drop. Please look at @fig-outliers.

::: {#fig-outliers}

![](images/outliers.png)

How to Detect Outliers 
:::

### Outliers for Numeric Variables 

```{python}
for col in df.select_dtypes(exclude = 'object'):
  sns.boxplot(data = df, x = col)
  plt.show()
```
### Outliers for Categorical Variables 

```{python}
for col in df.select_dtypes(include = 'object'):
  sns.boxplot(data = df, x = col, y = 'selling_price')
  plt.show()
```

### Finding the Outliers in the Dataset

This is based on the concept of quartiles, which divide a dataset into four equal parts. The IQR (InterQuartile Range rule) rule specifically focuses on the range of values within the middle 50% of the data and uses this range to identify potential outliers.

We have to find the minimum and maximum quantile values for each unique value in the categorical columns and filter the outlier samples which do not fit into the 25th and 75th percentile of our target column (Selling Price).

On the other hand, the outliers in numerical columns can be filtered by the 25th and 75th percentiles of the same column. We don't need to filter out with respect to the target column.

```{python}
outliers_indexes = []
target = 'selling_price'

for col in df.select_dtypes(include='object').columns:
    for cat in df[col].unique():
        df1 = df[df[col] == cat]
        q1 = df1[target].quantile(0.25)
        q3 = df1[target].quantile(0.75)
        iqr = q3-q1
        maximum = q3 + (1.5 * iqr)
        minimum = q1 - (1.5 * iqr)
        outlier_samples = df1[(df1[target] < minimum) | (df1[target] > maximum)]
        outliers_indexes.extend(outlier_samples.index.tolist())
        
        
for col in df.select_dtypes(exclude='object').columns:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3-q1
    maximum = q3 + (1.5 * iqr)
    minimum = q1 - (1.5 * iqr)
    outlier_samples = df[(df[col] < minimum) | (df[col] > maximum)]
    outliers_indexes.extend(outlier_samples.index.tolist())
    
outliers_indexes = list(set(outliers_indexes))
print('{} outliers were identified, whose indices are:\n\n{}'.format(len(outliers_indexes), outliers_indexes))
```

## Bivariate Analysis

```{python}
sns.scatterplot(data = df, x = 'km_driven', y = 'selling_price')
plt.show()
```

```{python}
sns.scatterplot(data = df, x = 'Age', y = 'selling_price')
plt.show()
```

```{python}
sns.scatterplot(data = df, x = 'fuel', y = 'selling_price')
plt.show()
```

## Encoding Categorical Variables 

### Label Encoding vs One Hot Encoding vs Ordinal Encoding

  * **Label Encoding**: Label encoding assigns a unique numerical label to each category in a categorical variable. It preserves the ordinal relationship between categories if present. For example, “Red” may be encoded as 1, “Green” as 2, and “Blue” as 3.
  * **One-Hot Encoding**: One-hot encoding converts each category in a categorical variable into a binary vector. It creates new binary columns for each category, representing the presence or absence of the category. Each category is mutually exclusive. For example, “Red” may be encoded as [1, 0, 0], “Green” as [0, 1, 0], and “Blue” as [0, 0, 1].
  
  * **Ordinal Encoding**: Ordinal encoding is similar to label encoding but considers the order or rank of categories. It assigns unique numerical labels to each category, preserving the ordinal relationship between categories. For example, “Cold” may be encoded as 1, “Warm” as 2, and “Hot” as 3.

### Dummy Variable Trap

The Dummy variable trap is a scenario where there are attributes that are highly correlated (Multicollinear) and one variable predicts the value of others. When we use *one-hot encoding* for handling the categorical data, then one dummy variable (attribute) can be predicted with the help of other dummy variables. Hence, one dummy variable is highly correlated with other dummy variables. Using all dummy variables for regression models leads to a dummy variable trap. So, the **regression models should be designed to exclude one dummy variable**. 


```{python}
cat_col = df.select_dtypes(include = 'object').columns.values
cat_col
df = pd.get_dummies(df, cat_col, drop_first = True)
df.info()
```

## Correlation Analysis

```{python}
plt.figure(figsize = (12,6))
cmap = sns.diverging_palette(125, 28, s=100, l=65, sep=50, as_cmap=True)
sns.heatmap(df.corr(), annot = True, cmap = cmap)
plt.show()
```

# Multiple Linear Regression Model

```{python}
import statsmodels.api as sm
# Installing stargazer package
#!pip install stargazer
from stargazer.stargazer import Stargazer
```


```{python}
#| include: false
col_ord = [var for var in df.columns.tolist() if var != 'selling_price']
col_ord.insert(0,'selling_price') 
col_ord
```
```{python}
sel_price = ['selling_price']
next_col = df.columns.difference(sel_price, sort = False).tolist()
df = df[sel_price + next_col]
df.head()
```

```{python}
df['selling_price'] = pd.to_numeric(df['selling_price'], errors = 'coerce')

bool_cols = df.select_dtypes(include = "bool").columns
df[bool_cols] = df[bool_cols].astype(int)

model_1 = sm.OLS(endog=df['selling_price'], exog = sm.add_constant(df[df.columns[1:]])).fit()

model_2 = sm.OLS(endog=df['selling_price'], exog = sm.add_constant(df[df.columns[1:10]])).fit()

model_3 = sm.OLS(endog=df['selling_price'], exog = sm.add_constant(df[['Age','km_driven', 'fuel_Diesel','seller_type_Individual','seller_type_Trustmark Dealer','transmission_Manual','owner_Second Owner']])).fit()
```

```{python}
print(model_1.summary())
```


```{python}
models = Stargazer([model_1, model_2, model_3])
models.significant_digits(3)

```

```{python}
models.render_html()
```

# Build the Model in Machine Learning 

```{python}
X = df.drop(columns = ['selling_price'], axis = 1)
y = df['selling_price']
```

```{python}
from sklearn.model_selection import train_test_split
```

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)
```

## Standardize (Normalize) the Data 

```{python}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### Train the Model 

```{python}
from sklearn.linear_model import LinearRegression
linear_reg = LinearRegression()
linear_reg.fit(X_train_scaled, y_train)
```

```{python}
pd.DataFrame(data = np.append(linear_reg.intercept_ , linear_reg.coef_), index = ['Intercept']+[col+" Coef." for col in X.columns], columns=['Value']).sort_values('Value', ascending=False)
```
# Evaluation of the Model 

`Scikit` Learn provides a metrics feature which helps us to measure the metrics of our model. We can use that to determine metrics include Mean Squared Error, Mean Absolute Error, Root Mean Squared Error, and R2-Score.

**Mean Absolute Error(MAE)** calculates the absolute difference between the actual and predicted values. We get the sum of all the prediction errors and divide them by the total number of data points. @fig-mae shows the formula for MAE.

::: {#fig-mae}

![](images/mae.png)

Mean Absolute Error (MAE) 
:::


**Mean Squared Error(MSE)** This is the most used metric. It finds the squared difference between actual and predicted values. We get the sum of the square of all prediction errors and divide it by the number of data points. @fig-mse shows the formula for MSE.

::: {#fig-mse}

![](images/mse.png)

Mean Squared Error (MSE) 
:::


**Root Mean Squared Error(RMSE)** is the square root of MSE. Since MSE is calculated by the square of error, the square root brings it back to the same level of prediction error. @fig-rmse shows the formula for RMSE.

::: {#fig-rmse}

![](images/rmse.png)

Root Mean Squared Error (RMSE) 
:::

**R Squared (R^2^)**: R^2^ is also called the coefficient of determination or goodness of fit score regression function. It measures how much irregularity in the dependent variable the model can explain. The R2 value is between 0 to 1, and a bigger value shows a better fit between prediction and actual value.


```{python}
from sklearn import metrics
```

```{python}
def model_evaluation(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    
    MAE = metrics.mean_absolute_error(y_test, y_pred)
    MSE = metrics.mean_squared_error(y_test, y_pred)
    RMSE = np.sqrt(MSE)
    R2_Score = metrics.r2_score(y_test, y_pred)
    
    return pd.DataFrame([MAE, MSE, RMSE, R2_Score], index=['MAE', 'MSE', 'RMSE' ,'R2-Score'], columns=[model_name])

model_evaluation(linear_reg, X_test_scaled, y_test, 'Linear Reg.')
```
# Assumptions of Linear Regression 

## Check for Homoscedasticity: 

Assumption  03 -  Residual Errors have a Mean Value of Zero

Assumption  04 -  Residual Errors have Constant Variance

Homoscedasticity means that the residuals have equal or almost equal variance across the regression line. By plotting the error terms with predicted terms we can check that there should not be any pattern in the error terms.

@fig-assm-one-three-four and @fig-assm-one-three-four2 shows these assumptions. 


::: {#fig-assm-one-three-four}
![](images/wanttosee.webp)

This is What We Want to See 
:::



::: {#fig-assm-one-three-four2}

![](images/dontwanttosee.webp)

This is What We Do Not Want to See 
:::




```{python}
regr = LinearRegression()
regr.fit(X_train,y_train)
y_pred = regr.predict(X_train)
residuals = y_train.values-y_pred
```

```{python}
assum_df = pd.DataFrame({'y_pred':y_pred, 'residuals': residuals})
plt.figure(figsize = (15,8))

sns.set(context="notebook", palette="Spectral", style = 'darkgrid' ,font_scale = 1.0, color_codes=True)

p = sns.scatterplot(data = assum_df, x = y_pred, y = residuals)
plt.xlabel('y_pred/predicted values')
plt.ylabel('Residuals')
#plt.ylim(-10,20)
#plt.xlim(0,25)
p = sns.lineplot(x = [0,26],y = [0,0],color='blue')
p = plt.title('Residuals vs fitted values plot for homoscedasticity check')
plt.show()
```

### Goldfeld Quandt Test

Checking heteroscedasticity : Using Goldfeld Quandt we test for heteroscedasticity.

**Null Hypothesis**: Error terms are homoscedastic

**Alternative Hypothesis**: Error terms are heteroscedastic.


```{python}
import statsmodels.stats.api as sms
from statsmodels.compat import lzip
name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(residuals, X_train)
lzip(name, test)
```
Since p value is more than 0.05 in Goldfeld Quandt Test, we can't reject null hypothesis that error terms are homoscedastic.

## Check for Normality of Error Terms/Residuals

Assumptions 01 - Linearity of the Data


```{python}
p = sns.distplot(residuals,kde=True)
p = plt.title('Normality of error terms/residuals')
plt.show()
```
The residual terms are pretty much normally distributed for the number of test points we took. Remember the central limit theorem which says that as the sample size increases the distribution tends to be normal. A skew is also visible from the plot. It's very difficult to get perfect curves, distributions in real life data.

## No Autocorrelation of Residuals

Assumptions 02 - Predictors (x) are Independent & Observed with Negligible Error

When the residuals are autocorrelated, it means that the current value is dependent of the previous (historic) values and that there is a definite unexplained pattern in the `Y` variable that shows up in the error terms. Though it is more evident in time series data.

In plain terms autocorrelation takes place when there's a pattern in the rows of the data. This is usual in time series data as there is a pattern of time for eg. Week of the day effect which is a very famous pattern seen in stock markets where people tend to buy stocks more towards the beginning of weekends and tend to sell more on Mondays. There's been great study about this phenomenon and it is still a matter of research as to what actual factors cause this trend.

```{python}
plt.figure(figsize=(10,5))
p = sns.lineplot(x = y_pred,y = residuals,marker='o',color='blue')
plt.xlabel('y_pred/predicted values')
plt.ylabel('Residuals')
#plt.ylim(-10,10)
#plt.xlim(0,26)
p = sns.lineplot(x = [0,26], y =[0,0],color='red')
p = plt.title('Residuals vs fitted values plot for autocorrelation check')
plt.show()
```

```{python}
# autocorrelation
sm.graphics.tsa.plot_acf(residuals, lags=40)
plt.show()
```


```{python}
#| eval: false
#| include: false
#| echo: false
from statsmodels.formula.api import ols
from statsmodels.stats.stattools import durbin_watson
df2 = df.rename(columns = {'seller_type_Trustmark Dealer':'seller_type_Trustmark_Dealer',
'owner_Second Owner':'owner_Second_Owner'
})
df2.info()
#fit multiple linear regression model
model = ols('selling_price ~ Age + km_driven + fuel_Diesel + seller_type_Individual+seller_type_Trustmark_Dealer+transmission_Manual+owner_Second_Owner', data=df2).fit()


durbin_watson(model)
```



## Multicollinearity 

In regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor 

# VIF dataframe 
vif_data = pd.DataFrame() 
vif_data["feature"] = X.columns 
  
# calculating VIF for each feature 
vif_data["VIF"] = [variance_inflation_factor(X.values, i) 
                          for i in range(len(X.columns))] 
  
print(vif_data)
```
# Feature Selection in Linear Regression 

For a dataset with `d` features, if we apply the hit and trial method with all possible combinations of features then total (2^d^ – 1) models need to be evaluated for a significant set of features. It is a time-consuming approach; therefore, we use feature selection techniques to find out the smallest set of features more efficiently.

There are three types of feature selection techniques :

  * Filter methods

  * Wrapper methods

  * Embedded methods

@fig-feature_selection_methods compares the three methods. 


::: {#fig-feature_selection_methods}

![](images/feature_selection_methods.png)

Feature Selection Methods  
:::

Here, we will only discuss feature selection using Wrapper methods in Python

## Wrapper Methods

In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset.

It follows a *greedy search* approach by evaluating all the possible combinations of features against the evaluation criterion. The *evaluation criterion* is simply the performance measure that depends on the type of problem, e.g., in *regression*, evaluation criterion can be p-values, R-squared, Adjusted R-squared, similarly for *classification* the evaluation criterion can be accuracy, precision, recall, f1-score, etc. Finally, it selects the combination of features that gives the optimal results for the specified machine learning algorithm.

Most commonly used techniques under wrapper methods are:

  * Forward selection

  * Backward elimination

  * Bi-directional elimination(Stepwise Selection)

## Forward Selection 

In forward selection, we start with a null model and then start fitting the model with each individual feature one at a time and select the feature with the minimum p-value. Now fit a model with two features by trying combinations of the earlier selected feature with all other remaining features. Again select the feature with the minimum p-value. Now fit a model with three features by trying combinations of two previously selected features with other remaining features. Repeat this process until we have a set of selected features with a p-value of individual features less than the significance level.

In short, the steps for the forward selection technique are as follows :

  i) Choose a significance level (e.g. SL = 0.05 with a 95% confidence).

  ii) Fit all possible simple regression models by considering one feature at a time. Total ’n’ models are possible. Select the feature with the lowest p-value.

  iii) Fit all possible models with one extra feature added to the previously selected feature(s).

  iv) Again, select the feature with a minimum p-value. if p_value < significance level then go to Step 3, otherwise terminate the process.

### Forward Selection Using User Defined Function 

```{python}
def forward_selection(data, target, significance_level=0.05):
    initial_features = data.columns.tolist()
    best_features = []
    while (len(initial_features)>0):
        remaining_features = list(set(initial_features)-set(best_features))
        new_pval = pd.Series(index=remaining_features)
        for new_column in remaining_features:
            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()
            new_pval[new_column] = model.pvalues[new_column]
        min_p_value = new_pval.min()
        if(min_p_value<significance_level):
            best_features.append(new_pval.idxmin())
        else:
            break
    return best_features
```


```{python}
for i in forward_selection(X,y):
  print (i)
```
### Forward Selection Using Built-in Functions in `Python`

```{python}
## Installing mlxtend package 
#!pip install mlxtend

```

```{python}
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.linear_model import LinearRegression
# Sequential Forward Selection(sfs)
sfs = SFS(LinearRegression(),
          k_features=7,
          forward=True,
          floating=False,
          scoring = 'r2',
          cv = 0)
```

`SequentialFeatureSelector()` function accepts the following major arguments :

  * `LinearRegression()` is an estimator for the entire process. Similarly, it can be any classification based algorithm.

  * `k_features` indicates the number of features to be selected. It can be any random value, but the optimal value can be found by analyzing and visualizing the scores for different numbers of features.

  * `forward` and `floating` arguments for different flavors of wrapper methods, here, forward = True and floating = False are for forward selection technique.

  * The `scoring` argument specifies the evaluation criterion to be used. For regression problems, there is only r2 score in default implementation. Similarly for classification, it can be accuracy, precision, recall, f1-score, etc.

  * `cv` argument is for k-fold cross-validation.


```{python}
sfs.fit(X,y)
for x in sfs.k_feature_names_:
  print (x)
```

## Backward Selection

In *backward elimination*, we start with the full model (including all the independent variables) and then remove the insignificant feature with the highest *p-value(> significance level)*. This process repeats again and again until we have the final set of significant features.

In short, the steps involved in *backward elimination* are as follows:

  * Choose a significance level (e.g. SL = 0.05 with a 95% confidence).

  * Fit a full model including all the features.

  * Consider the feature with the highest p-value. If the p-value > significance level then go to Step 4, otherwise terminate the process.

  * Remove the feature which is under consideration.

  * Fit a model without this feature. Repeat the entire process from Step 3.


### Backward Selection Using User Defined Functions 

```{python}
def backward_elimination(data, target,significance_level = 0.05):
    features = data.columns.tolist()
    while(len(features)>0):
        features_with_constant = sm.add_constant(data[features])
        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]
        max_p_value = p_values.max()
        if(max_p_value >= significance_level):
            excluded_feature = p_values.idxmax()
            features.remove(excluded_feature)
        else:
            break 
    return features
```

```{python}
for x in backward_elimination(X,y):
  print (x)
```
### Backward Selection Using Built-in Functions in `Python`

```{python}
#Sequential backward selection(sbs)
sbs = SFS(LinearRegression(),
         k_features=7,
         forward=False,
         floating=False,
         cv=0)
sbs.fit(X, y)
```
```{python}
for x in sbs.k_feature_names_:
  print (x)
```

## Step-wise Selection 

It is similar to *forward selection* but the difference is while adding a new feature it also checks the *significance* of already added features and if it finds any of the already selected features insignificant then it simply removes that particular feature through *backward elimination*.

Hence, It is a combination of *forward selection* and *backward elimination*.

In short, the steps involved in bi-directional elimination (Stepwise Selection) are as follows:

  * Choose a significance level to enter and exit the model (e.g. SL_in = 0.05 and SL_out = 0.05 with 95% confidence).

  * Perform the next step of forward selection (newly added feature must have p-value < SL_in to enter).

  * Perform all steps of backward elimination (any previously added feature with p-value>SL_out is ready to exit the model).

  * Repeat steps 2 and 3 until we get a final optimal set of features.

### Stepwise Selection Using User Defined Functions 

```{python}
def stepwise_selection(data, target,SL_in=0.05,SL_out = 0.05):
    initial_features = data.columns.tolist()
    best_features = []
    while (len(initial_features)>0):
        remaining_features = list(set(initial_features)-set(best_features))
        new_pval = pd.Series(index=remaining_features)
        for new_column in remaining_features:
            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()
            new_pval[new_column] = model.pvalues[new_column]
        min_p_value = new_pval.min()
        if(min_p_value<SL_in):
            best_features.append(new_pval.idxmin())
            while(len(best_features)>0):
                best_features_with_constant = sm.add_constant(data[best_features])
                p_values = sm.OLS(target, best_features_with_constant).fit().pvalues[1:]
                max_p_value = p_values.max()
                if(max_p_value >= SL_out):
                    excluded_feature = p_values.idxmax()
                    best_features.remove(excluded_feature)
                else:
                    break 
        else:
            break
    return best_features
```

```{python}
for x in stepwise_selection(X,y):
  print (x)
```

### Stepwise Selection Using Built-in Functions in `Python`

```{python}
# Sequential Forward Floating Selection(sffs)
sffs = SFS(LinearRegression(),
         k_features=(3,7),
         forward=True,
         floating=True,
         cv=0)
sffs.fit(X, y)
```

```{python}
for x in sffs.k_feature_names_:
  print(x)
```


### Visualization of Feature Selection 

```{python}
sfs1 = SFS(LinearRegression(),
         k_features=(3,11),
         forward=True,
         floating=False,
         cv=0)
sfs1.fit(X, y)
```


```{python}
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
import matplotlib.pyplot as plt
fig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')
plt.title('Sequential Forward Selection')
plt.show()
```
Here, on the y-axis, the performance label indicates the R-squared values for the different number of features.


# Conclusion 

The following links - [here](https://www.freecodecamp.org/news/build-a-linear-regression-model-with-an-example/) and [here](https://www.kaggle.com/code/shrutimechlearn/step-by-step-assumptions-linear-regression) -  are used for the above code and discussions. 

